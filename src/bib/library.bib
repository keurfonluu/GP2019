Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Luo1991,
abstract = {This paper presents a new traveltime inversion method based on the wave equation. In this new method, designated as wave-equation traveltime inversion (WT), seismograms are computed by any full-wave forward modeling method (we use a finite-difference method). The velocity model is perturbed until the traveltimes from the synthetic seismograms are best fitted to the observed traveltimes in a least squares sense. A gradient optimization method is used and the formula for the Frech{\'{e}}t derivative (perturbation of traveltimes with respect to velocity) is derived directly from the wave equation. No traveltime picking or ray tracing is necessary, and there are no high frequency assumptions about the data. Body wave, diffraction, reflection and head wave traveltimes can be incorporated into the inversion. In the high-frequency limit, WT inversion reduces to ray-based traveltime tomography. It can also be shown that WT inversion is approximately equivalent to full-wave inversion when the starting velocity model is “close” to the actual model. Numerical simulations show that WT inversion succeeds for models with up to 80 percent velocity contrasts compared to the failure of full-wave inversion for some models with no more than 10 percent velocity contrast. We also show that the WT method succeeds in inverting a layered velocity model where a shooting ray-tracing method fails to compute the correct first arrival times. The disadvantage of the WT method is that it appears to provide less model resolution compared to full-wave inversion, but this problem can be remedied by a hybrid traveltime + full-wave inversion method (Luo and Schuster, 1989).},
author = {Luo, Y. and Schuster, G. T.},
doi = {10.1190/1.1443081},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Luo, Schuster{\_}1991.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {conjugate gradient},
mendeley-tags = {conjugate gradient},
month = {may},
number = {5},
pages = {645--653},
title = {{Wave‐equation traveltime inversion}},
url = {http://library.seg.org/doi/abs/10.1190/1.1443081 http://library.seg.org/doi/10.1190/1.1443081},
volume = {56},
year = {1991}
}
@article{Deichmann2009,
abstract = {To stimulate the reservoir for a "hot dry rock" geothermal project initiated by a private/public consortium in the city of Basel, Switzerland, approximately 11,500 m3 of water were injected at high pressures between 2 December and 8 December 2006 into a 5-km-deep well below Kleinh{\"{u}}ningen (H{\"{a}}ring et al. 2008). A six-sensor borehole array, installed by the operators of the project at depths between 317 and 2,740 meters around the well to monitor the induced seismicity recorded more than 10,500 seismic events during the injection phase. Hypocentral locations could be calculated for more than 3,000 of these events. The gradual increase in flow rate and wellhead pressure was accompanied by a steady increase in seismicity, both in terms of event rates and magnitudes. In the early morning hours of 8 December, after water had been injected at maximum rates in excess of 50 l/s and at wellhead pressures of up to 29.6 MPa for about 16 hours (H{\"{a}}ring et al. 2008), a magnitude ML 2.6 event occurred within the reservoir. This exceeded the safety threshold for continued stimulation, so that injection was stopped prematurely. In the afternoon and evening of the same day, two additional events of magnitude ML 2.7 and 3.4 occurred within the same source volume. As a consequence, the well was opened and the water allowed to flow back. In the following days about one third of the injected water volume flowed back out of the well (H{\"{a}}ring et al. 2008). Though the seismic activity declined rapidly thereafter, even more than two years later sporadic microseismicity was being detected in the stimulated rock volume by the downhole-instruments.},
author = {Deichmann, N. and Giardini, D.},
doi = {10.1785/gssrl.80.5.784},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Seismological Research Letters/Deichmann, Giardini{\_}2009.pdf:pdf},
isbn = {0895-0695},
issn = {0895-0695},
journal = {Seismological Research Letters},
month = {sep},
number = {5},
pages = {784--798},
pmid = {16183246},
title = {{Earthquakes Induced by the Stimulation of an Enhanced Geothermal System below Basel (Switzerland)}},
url = {http://srl.geoscienceworld.org/cgi/doi/10.1785/gssrl.80.5.784 https://pubs.geoscienceworld.org/srl/article/80/5/784-798/143596},
volume = {80},
year = {2009}
}
@article{Audigane2011,
abstract = {This paper is addressed to the TOUGH2 user community. It presents a new tool for handling simulations run with the TOUGH2 code with specific application to CO2geological storage. This tool is composed of separate FORTRAN subroutines (or modules) that can be run independently, using input and output files in ASCII format for TOUGH2. These modules have been developed specifically for modeling of carbon dioxide geological storage and their use with TOUGH2 and the Equation of State module ECO2N, dedicated to CO2-water-salt mixture systems, with TOUGHREACT, which is an adaptation of TOUGH2 with ECO2N and geochemical fluid-rock interactions, and with TOUGH2 and the EOS7C module dedicated to CO2-CH4gas mixture is described. The objective is to save time for the pre-processing, execution and visualization of complex geometry for geological system representation. The workflow is rapid and user-friendly and future implementation to other TOUGH2 EOS modules for other contexts (e.g. nuclear waste disposal, geothermal production) is straightforward. Three examples are shown for validation: (i) leakage of CO2up through an abandoned well; (ii) 3D reactive transport modeling of CO2in a sandy aquifer formation in the Sleipner gas Field, (North Sea, Norway); and (iii) an estimation of enhanced gas recovery technology using CO2as the injected and stored gas to produce methane in the K12B Gas Field (North Sea, Denmark). {\textcopyright} 2011 Elsevier Ltd.},
author = {Audigane, Pascal and Chiaberge, Christophe and Mathurin, Fr{\'{e}}d{\'{e}}ric and Lions, Julie and Picot-Colbeaux, G{\'{e}}raldine},
doi = {10.1016/j.cageo.2010.11.020},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Audigane et al.{\_}2011.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {3D visualization,CO2geological storage,Multiphase flow,Pre and post processing,Reactive transport modeling,TOUGH2,tough2},
mendeley-tags = {tough2},
month = {apr},
number = {4},
pages = {610--620},
title = {{A workflow for handling heterogeneous 3D models with the TOUGH2 family of codes: Applications to numerical modeling of CO2 geological storage}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300411000355},
volume = {37},
year = {2011}
}
@article{Yin2004,
abstract = {Polygonal approximation of digital curves is one of the crucial steps prior to many image analysis tasks. This paper presents a new polygonal approximation approach based on the particle swarm optimization (PSO) algorithm. Each particle represented as a binary vector corresponds to a candidate solution to the polygonal approximation problem. A swarm of particles are initiated and fly through the solution space for targeting the optimal solution. We also propose to use a hybrid version of PSO embedding a local optimizer to enhance the performance. The experimental results manifest that the proposed discrete PSO is comparable to the genetic algorithm, and it outperforms another discrete implementation of PSO in the literature. The proposed hybrid version of PSO can significantly improve the approximation results in terms of the compression ratio, and the results obtained in different runs are more consistent. ?? 2003 Elsevier Inc. All rights reserved.},
author = {Yin, Peng Yeng},
doi = {10.1016/j.jvcir.2003.12.001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Visual Communication and Image Representation/Yin{\_}2004.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Genetic algorithm,Global optimal solution,Local optimal solution,Particle swarm optimization,Polygonal approximation,pso},
mendeley-tags = {pso},
number = {2},
pages = {241--260},
title = {{A discrete particle swarm algorithm for optimal polygonal approximation of digital curves}},
volume = {15},
year = {2004}
}
@inproceedings{Kassahun2005,
author = {Kassahun, Yohannes and Sommer, Gerald},
booktitle = {ESANN},
pages = {259--266},
title = {{Efficient reinforcement learning through Evolutionary Acquisition of Neural Topologies.}},
year = {2005}
}
@article{Versteeg1994a,
abstract = {The motivation behind seismic data acquisition and processing is simple—to obtain a depth image of the earth—but performing this process correctly is extremely difficult. A number of well known processing tasks (such as static corrections, deconvolution, multiple elimination, velocity estimation, and migration) have to be executed, and each directly influences the final result. However, the success of the velocity estimation is paramount and critical, and large amounts of man/CPU hours are devoted to it.},
author = {Versteeg, Roelof},
doi = {10.1190/1.1437051},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The Leading Edge/Versteeg{\_}1994.pdf:pdf},
issn = {1070-485X},
journal = {The Leading Edge},
month = {sep},
number = {9},
pages = {927--936},
title = {{The Marmousi experience: Velocity model determination on a synthetic complex data set}},
url = {http://library.seg.org/doi/10.1190/1.1437051},
volume = {13},
year = {1994}
}
@article{Cappa2011a,
abstract = {The interaction between mechanical deformation and fluid flow in fault zones gives rise to a host of coupled hydromechanical processes fundamental to fault instability, induced seismicity, and associated fluid migration. In this paper, we discuss these coupled processes in general and describe three modeling approaches that have been considered to analyze fluid flow and stress coupling in fault-instability processes. First, fault hydromechanical models were tested to investigate fault behavior using different mechanical modeling approaches, including slip interface and finite-thickness elements with isotropic or anisotropic elasto-plastic constitutive models. The results of this investigation showed that fault hydromechanical behavior can be appropriately represented with the least complex alternative, using a finite-thickness element and isotropic plasticity. We utilized this pragmatic approach coupled with a strain-permeability model to study hydromechanical effects on fault instability during deep underground injection of CO2. We demonstrated how such a modeling approach can be applied to determine the likelihood of fault reactivation and to estimate the associated loss of CO2 from the injection zone. It is shown that shear-enhanced permeability initiated where the fault intersects the injection zone plays an important role in propagating fault instability and permeability enhancement through the overlying caprock. {\textcopyright} 2010 Elsevier Ltd.},
author = {Cappa, Fr{\'{e}}d{\'{e}}ric and Rutqvist, Jonny},
doi = {10.1016/j.ijggc.2010.08.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Cappa, Rutqvist{\_}2011.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Carbon dioxide (CO2),Fault zone,Hydromechanical couplings,Numerical simulation,Permeability,Rupture,flac,tough2},
mendeley-tags = {flac,tough2},
month = {mar},
number = {2},
pages = {336--346},
title = {{Modeling of coupled deformation and permeability evolution during fault reactivation induced by deep underground injection of CO2}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1750583610001337},
volume = {5},
year = {2011}
}
@article{Carlisle2001,
abstract = {What attributes and settings of the Particle Swarm Optimizer constants result in a good, off-the-shelf, PSO implementation? There are many parameters, both explicit and implicit, associated with the Particle Swarm Optimizer that may affect its performance. There are the social and cognitive learning rates and magnitudes, the population size, the neighborhood size (including global neighborhoods), synchronous or asynchronous updates, and various additional controls, such as inertia and constriction factors. For any given problem, the values and choices for some of these parameters may have significant impact on the efficiency and reliability of the PSO, and yet varying other parameters may have little or no effect. What set of values, then, constitutes a good, general purpose PSO? While some of these factors have been investigated in the literature, others have not. In this paper we use existing literature and a selection of benchmark problems to determine a set of starting values suitable for an off the shelf PSO.},
author = {Carlisle, Anthony and Dozier, Gerry},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Population English Edition/Carlisle, Dozier{\_}2001.pdf:pdf},
journal = {Population English Edition},
keywords = {pso},
mendeley-tags = {pso},
pages = {1--6},
title = {{An Off-The-Shelf PSO}},
url = {http://antho.huntingdon.edu/publications/Off-The-Shelf{\_}PSO.pdf},
volume = {1},
year = {2001}
}
@article{Storn1997,
abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
author = {Storn, Rainer and Price, Kenneth},
doi = {10.1023/A:1008202821328},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Global Optimization/Storn, Price{\_}1997.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Stochastic optimization,de,evolution strategy,genetic algorithm,global optimization,nonlinear optimization},
mendeley-tags = {de},
number = {4},
pages = {341--359},
pmid = {2015},
title = {{Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces}},
url = {http://dx.doi.org/10.1023/A:1008202821328 http://link.springer.com/10.1023/A:1008202821328},
volume = {11},
year = {1997}
}
@article{Zelt1998,
author = {Zelt, Colin A. and Barton, Penny J.},
doi = {10.1029/97JB03536},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Zelt, Barton{\_}1998.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
month = {apr},
number = {B4},
pages = {7187--7210},
title = {{Three-dimensional seismic refraction tomography: A comparison of two methods applied to data from the Faeroe Basin}},
url = {http://doi.wiley.com/10.1029/97JB03536},
volume = {103},
year = {1998}
}
@article{DeMeersman2009,
author = {{De Meersman}, K. and Kendall, J.-M. and van der Baan, M.},
doi = {10.1190/1.3205028},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {B183--B195},
title = {{The 1998 Valhall microseismic data set: An integrated study of relocated sources, seismic multiplets, and S-wave splitting}},
url = {http://library.seg.org/doi/10.1190/1.3205028},
volume = {74},
year = {2009}
}
@article{DAgostino1990,
abstract = {For testing that an underlying population is normally distributed the skewness and kurtosis statistics, {\$}\backslashsqrt{\{}b{\_}1{\}}{\$} and b2, and the D'Agostino-Pearson K2 statistic that combines these two statistics have been shown to be powerful and informative tests. Their use, however, has not been as prevalent as their usefulness. We review these tests and show how readily available and popular statistical software can be used to implement them. Their relationship to deviations from linearity in normal probability plotting is also presented.},
author = {D'Agostino, Ralph B. and Belanger, Albert and D'Agostino, Ralph B.},
doi = {10.2307/2684359},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The American Statistician/D'Agostino, Belanger, D'Agostino{\_}1990.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
keywords = {statistics},
mendeley-tags = {statistics},
month = {nov},
number = {4},
pages = {316},
title = {{A Suggestion for Using Powerful and Informative Tests of Normality}},
url = {http://www.jstor.org/stable/2684359?origin=crossref},
volume = {44},
year = {1990}
}
@inproceedings{Kennedy1999,
abstract = {The study manipulated the neighborhood topologies of particle swarms optimizing four test functions. Several social network structures were tested, with "small-world" randomization of a specified number of links. Sociometric structure and the small-world manipulation interacted with function to produce a significant effect on performance.},
author = {Kennedy, James},
booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
doi = {10.1109/CEC.1999.785509},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)/Kennedy{\_}1999.pdf:pdf},
isbn = {0-7803-5536-9},
keywords = {pso},
mendeley-tags = {pso},
pages = {1931--1938},
publisher = {IEEE},
title = {{Small worlds and mega-minds: effects of neighborhood topology on particle swarm performance}},
url = {http://ieeexplore.ieee.org/document/785509/},
volume = {3},
year = {1999}
}
@article{Senel2014,
abstract = {Companies, scientists, academics, and environmental non-governmental organizations (NGO) that together make up ZEP undertook a study based on new data provided by member organizations on existing pilot and planned demonstration projects, to establish a reference point for the costs of CO 2 capture and storage (CCS). The study aimed to estimate the costs of complete CCS value chains for new-build coal- and gas-fired power plants located at a generic site in northern Europe, from the early 2020s. ZEP's study indicates that the EU CCS demonstration program will not only prove the costs of CCS. Using agreed assumptions and the levelized cost of electricity as the main quantitative value, there is currently no clear difference between any of these and all could be competitive in the future, once successfully demonstrated. The current main incentive for the EU-wide deployment of CCS is the price of Emission Unit Allowances (EUA) under the EU Emissions Trading System (ETS).},
author = {Senel, Ozgur and Will, Robert and Butsch, Robert J.},
doi = {10.1002/ghg.1451},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Greenhouse Gases Science and Technology/Senel, Will, Butsch{\_}2014.pdf:pdf},
issn = {21523878},
journal = {Greenhouse Gases: Science and Technology},
keywords = {co2 sequestration,decatur,history matching,ibdp,integrated modeling,monitoring,simulation,uncertainty,verification},
mendeley-tags = {decatur},
month = {oct},
number = {5},
pages = {662--684},
title = {{Integrated reservoir modeling at the Illinois Basin - Decatur Project}},
url = {http://doi.wiley.com/10.1002/ghg.1451},
volume = {4},
year = {2014}
}
@article{Pellerin2017,
abstract = {RINGMesh is a C++ open-source programming library for manipulating discretized geological models. It is designed to ease the development of applications and workflows that use discretized 3D models. It is neither a geomodeler, nor a meshing software. RINGMesh implements functionalities to read discretized surface-based or volumetric structural models and to check their validity. The models can be then exported in various file formats. RINGMesh provides data structures to represent geological structural models, either defined by their discretized boundary surfaces, and/or by discretized volumes. A programming interface allows to develop of new geomodeling methods, and to plug in external software. The goal of RINGMesh is to help researchers to focus on the implementation of their specific method rather than on tedious tasks common to many applications. The documented code is open-source and distributed under the modified BSD license. It is available at https://www.ring-team.org/index.php/software/ringmesh.},
author = {Pellerin, Jeanne and Botella, Arnaud and Bonneau, Fran{\c{c}}ois and Mazuyer, Antoine and Chauvin, Benjamin and L{\'{e}}vy, Bruno and Caumon, Guillaume},
doi = {10.1016/j.cageo.2017.03.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Pellerin et al.{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {BRep,C++,Geology,Open-source,Structural model,Unstructured meshes,mesh},
mendeley-tags = {mesh},
month = {jul},
number = {March},
pages = {93--100},
publisher = {Elsevier Ltd},
title = {{RINGMesh: A programming library for developing mesh-based geomodeling applications}},
url = {http://dx.doi.org/10.1016/j.cageo.2017.03.005 https://linkinghub.elsevier.com/retrieve/pii/S0098300417302637},
volume = {104},
year = {2017}
}
@article{Zitzler1999,
abstract = {Ether some techniques are in general superior to others, which algorithms are suited to which kind of problem, and what the specific advantages and drawbacks of certain methods are. The subject of this work is the comparison and the improvement of existing multiobjective evolutionary algorithms and their application to system design problems in computer engineering. In detail, the major contributions are: An experimental methodology to compare multiobjective optimizers is devel- oped. In particular, quantitative measures to assess the quality of trade-off fronts are introduced and a set of general test problems is defined, which are i) easy to formulate, ii) represent essential aspects of real-world problems, and iii) test for different types of problem difficulty. On the basis of this methodology, an extensive comparison of numerous evolu- tionary techniques is performed in which further aspects such as the influence of elitism and the population size are also investigated. A novel approach to multiobjective optimization, the strength Pareto evolution- ary algorithm, is proposed. It combines both established and new techniques in a unique manner. Two complex multicriteria applications are addressed using evolutionary algo- rithms: i) the automatic synthesis of heterogeneous hardware/systems and ii) the multidimensional exploration of software implementations for digital signal processors.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Zitzler, Eckart},
doi = {citeulike-article-id:4597043},
eprint = {9605103},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/TIK-Schriftenreihe/Zitzler{\_}1999.pdf:pdf},
isbn = {3826568311},
issn = {10769757},
journal = {TIK-Schriftenreihe},
number = {30},
pages = {1--122},
pmid = {17255001},
primaryClass = {cs},
title = {{Evolutionary Algorithms for Multiobjective Optimization: Methods and Applications}},
url = {http://www.tik.ee.ethz.ch/{~}sop/publicationListFiles/zitz1999a.pdf{\%}0Ahttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.9023{\&}rep=rep1{\&}type=pdf},
volume = {30},
year = {1999}
}
@book{Sheriff1995,
address = {Cambridge},
author = {Sheriff, R. E. and Geldart, L. P.},
doi = {10.1017/CBO9781139168359},
editor = {{Cambridge University Press}},
isbn = {9781139168359},
publisher = {Cambridge University Press},
title = {{Exploration Seismology}},
url = {https://books.google.fr/books?id=wRYgAwAAQBAJ http://ebooks.cambridge.org/ref/id/CBO9781139168359},
year = {1995}
}
@article{Scales1997,
author = {Scales, John A. and Snieder, Roel},
doi = {10.1190/1.6241045.1},
issn = {0016-8033},
journal = {Geophysics},
month = {jul},
number = {4},
pages = {1045--1046},
title = {{To Bayes or not to Bayes?}},
url = {http://library.seg.org/doi/10.1190/1.6241045.1},
volume = {62},
year = {1997}
}
@article{Font2004,
abstract = {The maximum intersection (MAXI) method, which derives from the master station method (MSM), determines within a 3-D velocity model the absolute hypocentral location based on observed arrival times. First, the spatial node that better satisfies the arrival time differences computed at all station pairs, plus or minus an error tolerance value (in seconds), is defined as the preliminary hypocentral solution (PRED). Second, because PRED depends neither on the estimate of origin time nor on the residual root mean square (rms), residual outliers are objectively detected and cleaned out from the original data set without any iterative process or weighting. Third, a statistical minimization (residual rms) is conducted in a small domain around the PRED node, which results in a unique FINAL solution. The MAXI method is applied to the determination of earthquake hypocentres (with the proper station correction terms) in the southernmost extremity of the Ryukyu subduction zone, where several dense seismic clusters occur near the seismogenic plate interface. The location of earthquakes, recorded at both the Taiwanese and Japanese networks, is obtained for about a thousand events (between 1992 and 1997). The process uses a detailed 3-D velocity model based on multiple geophysical data sources obtained in the junction area between subduction and collision (east of Taiwan). The earthquake clustering and the significant drop in residual statistics (1.20, 0.80 and 0.35 s, for Taiwanese catalogue, MSM and MAXIM solutions respectively) indicate the accuracy of the method, which can be used to routinely determine absolute hypocentre location based on observed arrival times.},
author = {Font, Yvonne and Kao, Honn and Lallemand, Serge and Liu, Char Shine and Chiao, Ling Yun},
doi = {10.1111/j.1365-246X.2004.02317.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Font et al.{\_}2004.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {3-D,Earthquake,Hypocentre,Location,Outliner,Taiwan,location},
mendeley-tags = {location},
number = {2},
pages = {655--675},
title = {{Hypocentre determination offshore of eastern Taiwan using the maximum intersection method}},
volume = {158},
year = {2004}
}
@article{Ross2014,
abstract = {We develop a set of algorithms for automatic detection and picking of direct P and S waves, as well as fault zone head waves (FZHW), generated by earthquakes on faults that separate different lithologies and recorded by local seismic networks. The S-wave picks are performed using polarization analysis and related filters to remove P-wave energy from the seismograms, and utilize STA/LTA and kurtosis detectors in tandem to lock on the phase arrival. The early portions of P waveforms are processed with STA/LTA, kurtosis and skewness detectors for possible first-arriving FZHW. Identification and picking of direct P and FZHW is performed by a multistage algorithm that accounts for basic characteristics (motion polarities, time difference, sharpness and amplitudes) of the two phases. The algorithm is shown to perform well on synthetic seismograms produced by a model with a velocity contrast across the fault, and observed data generated by earthquakes along the Parkfield section of the San Andreas fault and the Hayward fault. The developed techniques can be used for systematic processing of large seismic waveform data sets recorded near major faults.},
author = {Ross, Z. E. and Ben-Zion, Y.},
doi = {10.1093/gji/ggu267},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Ross, Ben-Zion{\_}2014.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Body waves,Earthquake source observations,Interface waves,Time-series analysis,Wave propagation,picking},
mendeley-tags = {picking},
month = {aug},
number = {1},
pages = {368--381},
title = {{Automatic picking of direct P, S seismic phases and fault zone head waves}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggu267},
volume = {199},
year = {2014}
}
@article{Hansen2011,
abstract = {CMA Tutorial},
archivePrefix = {arXiv},
arxivId = {1604.00772},
author = {Hansen, Nikolaus},
doi = {10.1007/11007937_4},
eprint = {1604.00772},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Hansen{\_}2011.pdf:pdf},
isbn = {3540290060},
issn = {14349922},
keywords = {cmaes},
mendeley-tags = {cmaes},
number = {2006},
pages = {1--34},
title = {{The CMA evolution strategy: A tutorial}},
url = {http://www.lri.fr/{~}hansen/cmatutorial110628.pdf},
volume = {102},
year = {2011}
}
@article{Will2016,
abstract = {The microseismic activity observed in and around a geologic formation undergoing carbon dioxide (CO2) injection is a combination of natural, or “background”, microseismicity plus that activity which is induced by injection operations. Since injection pressure within storage target formations are maintained safely below fracture pressure this induced activity typically originates at natural pre-existing zones of mechanical weakness presented by structural or stratigraphic features. The combination of mechanical properties and in situ stresses dictate the focal mechanism for microseismic emissions, an understanding of which facilitates the use of observed microseismicity for regulatory compliance and project management. Under favorable conditions microseismic activity may be unambiguously correlated with structural and/or stratigraphic features directly observed in seismic data, thus providing strong constraints to interpretation of observed microseismicity for focal mechanisms. However, in many cases, such as at the Illinois Basin–Decatur Project (IBDP), this direct correlation is elusive and other indirect support is required. Analysis of microseismicity at IBDP has been performed within the context of the integrated reservoir and mechanical earth models developed as part of the site characterization and monitoring program. The IBDP integrated modeling workflow involved continuous and geotechnically consistent data integration for geologic modeling, calibrated flow simulation, three-dimensional (3D) mechanical earth model, and coupled hydro-mechanical simulation. Using the coupled model, scenario-based forward modeling of microseismicity was performed for hypothetical focal mechanisms inferred from observed data. The experience gained at IBDP illustrates the importance of integrated modeling in the interpretation of microseismic activity for focal mechanisms and provides valuable insights into critical data gaps which could be the target of future basic research efforts.},
author = {Will, Robert and Smith, Valerie and Lee, Don and Senel, Ozgur},
doi = {10.1016/j.ijggc.2015.12.020},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Will et al.{\_}2016(2).pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Geomechanics,Integration,Microseismic,Modeling,Prediction,Simulation,decatur},
mendeley-tags = {decatur},
pages = {389--403},
publisher = {Elsevier Ltd},
title = {{Data integration, reservoir response, and application}},
url = {http://dx.doi.org/10.1016/j.ijggc.2015.12.020},
volume = {54},
year = {2016}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Icml/Blundell et al.{\_}2015.pdf:pdf},
isbn = {9781510810587},
journal = {Icml},
keywords = {neural network},
mendeley-tags = {neural network},
month = {may},
pages = {1613--1622},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424{\%}5Cnhttp://www.arxiv.org/pdf/1505.05424.pdf http://arxiv.org/abs/1505.05424},
volume = {37},
year = {2015}
}
@article{Hunter2007,
author = {Hunter, John D.},
doi = {10.1109/MCSE.2007.55},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
number = {3},
pages = {90--95},
title = {{Matplotlib: A 2D Graphics Environment}},
url = {http://ieeexplore.ieee.org/document/4160265/},
volume = {9},
year = {2007}
}
@article{GerhardPratt1998,
author = {{Gerhard Pratt} and Shin, Changsoo and Hicks},
doi = {10.1046/j.1365-246X.1998.00498.x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {newton},
mendeley-tags = {newton},
month = {may},
number = {2},
pages = {341--362},
title = {{Gauss-Newton and full Newton methods in frequency-space seismic waveform inversion}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.1998.00498.x},
volume = {133},
year = {1998}
}
@article{Keilis-Borok1967,
author = {Keilis-Borok, V I and Yanovskaya, T B},
journal = {Geophys. J},
keywords = {mcmc},
mendeley-tags = {mcmc},
pages = {223--233},
title = {{Inverse seismic problems (structural review)}},
volume = {13},
year = {1967}
}
@article{Knopoff1970,
abstract = {Models of earthquake sources that have no volume change, no net force, and no net torque as criteria for the radiation of first motions, have five degrees of freedom in their spatial orientation. The usual double-couple model has only three degrees of freedom. The most general source of high-frequency seismic motions must be a linear combination of a double couple and another source called the compensated linear-vector dipole. A radiation pattern of amplitudes of first motions on the focal sphere cannot be uniquely decomposed into the radiation patterns due to the two sources.},
author = {Knopoff, L and Randall, M J},
doi = {10.1029/JB075i026p04957},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Knopoff, Randall{\_}1970.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {sep},
number = {26},
pages = {4957--4963},
title = {{The compensated linear-vector dipole: A possible mechanism for deep earthquakes}},
url = {http://doi.wiley.com/10.1029/JB075i026p04957},
volume = {75},
year = {1970}
}
@article{Gentili2006,
abstract = {The large amount of digital data recorded by permanent and temporary seismic networks makes automatic analysis of seismograms and automatic wave onset time picking schemes of great importance for timely and accurate event locations. We propose a fast and efficient P- and S-wave onset time, automatic detection method based on neural networks. The neural networks adopted here are particular neural trees, called IUANT2, characterized by a high generalization capability. Comparison between neural network automatic onset picking and standard, manual methods, shows that the technique presented here is generally robust and that it is capable to correctly identify phase-types while providing estimates of their accuracies. In addition, the automatic post processing method applied here can remove the ambiguity deriving from the incorrect association of events occurring closely in time. We have tested the methodology against standard STA/LTA phase picks and found that this neural approach performs better especially for low signal-to-noise ratios. We adopt the recall, precision and accuracy estimators to appraise objectively the results and compare them with those obtained with other methodologies.Tests of the proposed method are presented for 342 earthquakes recorded by 23 different stations (about 5000 traces). Our results show that the distribution of the differences between manual and automatic picking has a standard deviation of 0.064 s and 0.11 s for the P and the S waves, respectively. Our results show also that the number of false alarms deriving from incorrect detection is small and, thus, that the method is inherently robust.},
author = {Gentili, S. and Michelini, A.},
doi = {10.1007/s10950-006-2296-6},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Seismology/Gentili, Michelini{\_}2006.pdf:pdf},
isbn = {1383-4649},
issn = {1383-4649},
journal = {Journal of Seismology},
keywords = {Arrival time,Artificial neural networks,Automatic picking,Pattern recognition,Seismic network,machine learning,neural network,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
number = {1},
pages = {39--63},
title = {{Automatic picking of P and S phases using a neural tree}},
url = {http://link.springer.com/10.1007/s10950-006-2296-6},
volume = {10},
year = {2006}
}
@article{Delprat-Jannaud1993,
author = {Delprat-Jannaud, Florence and Lailly, Patrick},
doi = {10.1029/92JB02441},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
month = {apr},
number = {B4},
pages = {6589--6605},
title = {{Ill-posed and well-posed formulations of the reflection travel time tomography problem}},
url = {http://doi.wiley.com/10.1029/92JB02441},
volume = {98},
year = {1993}
}
@article{Schott1999a,
abstract = {In this paper, we will report on the application of Bayesian inference to DC resistivity inversion for 1-D multilayer models. The posterior probability distribution is explored through a Markov process based upon a Gibbs's sampler. The process would lead to unrealistic estimates without additional prior information, which takes the form of a second Markov chain where the transition kernel corresponds to a smoothness constraint. The outcomes are posterior marginal probabilites for each parameter, as well as, if required, joint probabilities for pairs of parameters. We will discuss the main properties of the method in the light of a theoretical example and illustrate its capabilities with some field examples taken from various contexts.},
author = {Schott, Jean-Jacques and Roussignol, Michel and Menvielle, Michel and Nomenjahanary, Flavien R.},
doi = {10.1046/j.1365-246x.1999.00905.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Schott et al.{\_}1999.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Electrical resistivity,Inversion,Layered media,Monte Carlo Markov chain,mcmc},
mendeley-tags = {mcmc},
month = {sep},
number = {3},
pages = {769--783},
title = {{Bayesian inversion with Markov chains-II. The one-dimensional DC multilayer case}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246x.1999.00905.x},
volume = {138},
year = {1999}
}
@article{Poliannikov2011,
abstract = {Hydraulic fracturing is the process of injecting high-pressure fluids into a reservoir to induce fractures and thus improve reservoir productivity. Microseismic event localization is used to locate created fractures. Traditionally, events are localized individually. Available infor- mation about events already localized is not used to help estimate other source locations. Traditional localization methods yield an uncertainty that is inversely proportional to the square root of the number of receivers. However, in applications where multiple fractures are created, multiple sources in a reference fracture may provide redundant information about unknown events in subsequent fractures that can boost the signal-to-noise ratio, im- proving estimates of the event positions. We propose to use sources in fractures closer to the monitoring well to help localize events further away. It is known through seismic inter- ferometry that with a 2D array of receivers, the travel time between two sources may be recovered from a cross-correlogram of two common source gathers. This allows an event in the second fracture to be localized relative to an event in the reference fracture. A difficulty arises when receivers are located in a single monitoring well. When the receiver array is 1D, classical interferometry cannot be directly employed because the problem becomes un- derdetermined. In our approach, interferometry is used to partially redatum microseismic events from the second fracture onto the reference fracture so that they can be used as virtual receivers, providing additional information complementary to that provided by the physical receivers. Our error analysis shows that, in addition to the gain obtained by having multiple physical receivers, the location uncertainty is inversely proportional to the square root of the number of sources in the reference fracture. Since the number of microseism sources is usually high, the proposed method will usually result in more accurate location estimates as compared to the traditional methods.},
author = {Poliannikov, Oleg V. and {E. Malcolm}, Alison and Djikpesse, Hugues and Prange, Michael},
doi = {10.1190/geo2010-0325.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Poliannikov et al.{\_}2011.pdf:pdf},
issn = {00168033},
journal = {Geophysics},
number = {6},
pages = {WC27},
title = {{Interferometric hydrofracture microseism localization using neighboring fracture}},
volume = {76},
year = {2011}
}
@book{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M},
booktitle = {Handbook of Markov Chain Monte Carlo},
doi = {10.1201/b10905},
editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
eprint = {arXiv:1206.1901v1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Handbook of Markov Chain Monte Carlo/Neal{\_}2011.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
keywords = {hamiltonian dynamics,mcmc},
month = {may},
pages = {113--162},
pmid = {25246403},
publisher = {Chapman and Hall/CRC},
series = {Chapman {\&} Hall/CRC Handbooks of Modern Statistical Methods},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{Pugh2016,
abstract = {Earthquake source inversion is highly dependent on location determination and velocity models. Uncertainties in both the model parameters and the observations need to be rigorously incorporated into an inversion approach. Here, we show a probabilistic Bayesian method that allows formal inclusion of the uncertainties in the moment tensor inversion. This method allows the combination of different sets of far-field observations, such as P-wave and S-wave polarities and amplitude ratios, into one inversion. Additional observations can be included by deriving a suitable likelihood function from the uncertainties. This inversion produces samples from the source posterior probability distribution, including a best-fitting solution for the source mechanism and associated probability. The inversion can be constrained to the double-couple space or allowed to explore the gamut of moment tensor solutions, allowing volumetric and other non-double-couple components. The posterior probability of the double-couple and full moment tensor source models can be evaluated from the Bayesian evidence, using samples from the likelihood distributions for the two source models, producing an estimate of whether or not a source is double-couple. Such an approach is ideally suited to microseismic studies where there are many sources of uncertainty and it is often difficult to produce reliability estimates of the source mechanism, although this can be true of many other cases. Using full-waveform synthetic seismograms, we also show the effects of noise, location, network distribution and velocity model uncertainty on the source probability density function. The noise has the largest effect on the results, especially as it can affect other parts of the event processing. This uncertainty can lead to erroneous non-double-couple source probability distributions, even when no other uncertainties exist. Although including amplitude ratios can improve the constraint on the source probability distribution, the measurements are often systematically affected by noise, leading to deviation from their noise-free true values and consequently adversely affecting the source probability distribution, especially for the full moment tensor model. As an example of the application of this method, four events from the Krafla volcano in Iceland are inverted, which show clear differentiation between non-double-couple and double-couple sources, reflected in the posterior probability distributions for the source models.},
author = {Pugh, D. J. and White, R. S. and Christie, P. A. F.},
doi = {10.1093/gji/ggw186},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Pugh, White, Christie{\_}2016.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Earthquake source observations,Numerical solutions,Probability distributions,Volcano seismology,moment tensor},
mendeley-tags = {moment tensor},
month = {aug},
number = {2},
pages = {1009--1038},
title = {{A Bayesian method for microseismic source inversion}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggw186},
volume = {206},
year = {2016}
}
@article{Shapiro2004,
author = {Shapiro, Nikolai M and Campillo, Michel},
issn = {1944-8007},
journal = {Geophysical Research Letters},
number = {7},
publisher = {Wiley Online Library},
title = {{Emergence of broadband Rayleigh waves from correlations of the ambient seismic noise}},
volume = {31},
year = {2004}
}
@article{Kuperkoch2010,
abstract = {We present an algorithm for automatic P-phase arrival time determination for local and regional seismic events based on higher order statistics (HOS). Using skewness or kurtosis a characteristic function is determined to which a new iterative picking algorithm is applied. For P-phase identification we apply the Akaike Information Criterion to the characteristic function, while for a precise determination of the P-phase arrival time a pragmatic picking algorithm is applied to a recalculated characteristic function. In addition, an automatic quality estimate is obtained, based on the slope and the signal-to-noise ratio, both calculated from the characteristic function. To get rid of erroneous picks, a Jackknife procedure and an envelope function analysis is used. The algorithm is applied to a large data set with very heterogeneous qualities of P-onsets acquired by a temporary, regional seismic network of the EGELADOS-project in the southern Aegean. The reliability and robustness of the proposed algorithm is tested by comparing more than 3000 manually derived P readings, serving as reference picks, with the corresponding automatically estimated P-wave arrival times. We find an average deviation from the reference picks of 0.26 ± 0.64 s when using kurtosis and 0.38 ± 0.75 s when using skewness. If automatically as excellent classified picks are considered only, the average difference from the reference picks is 0.07 ± 0.31 s and 0.07 ± 0.41 s, respectively. However, substantially more P-arrival times are determined when using kurtosis, indicating that the characteristic function derived from kurtosis estimation is to be preferred. Since the characteristic function is calculated recursively, the algorithm is very fast and hence suited for earthquake early warning purposes. Furthermore, a comparative study with automatically derived P-readings using Allen's and Baer {\&} Kradolfer's picking algorithms applied to the same data set demonstrates better quantitative and qualitative performance of the HOS approach. This study shows, that precise automatic P-onset determination is feasible, even when using data sets with very heterogeneous signal-to-noise ratio.},
author = {K{\"{u}}perkoch, L. and Meier, T. and Lee, J. and Friederich, W. and {Working Group}, EGELADOS},
doi = {10.1111/j.1365-246X.2010.04570.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/K{\"{u}}perkoch et al.{\_}2010(2).pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Body waves,Early warning,Time series analysis,picking},
mendeley-tags = {picking},
month = {mar},
title = {{Automated determination of P -phase arrival times at regional and local distances using higher order statistics}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2010.04570.x},
year = {2010}
}
@book{Cerveny2001,
address = {Cambridge},
author = {{\v{C}}erven{\'{y}}, V.},
doi = {10.1017/CBO9780511529399},
isbn = {9780511529399},
publisher = {Cambridge University Press},
title = {{Seismic Ray Theory}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511529399},
year = {2001}
}
@article{Rutqvist2012,
abstract = {This paper provides a review of the geomechanics and modeling of geomechanics associated with geologic carbon storage (GCS), focusing on storage in deep sedimentary formations, in particular saline aquifers. The paper first introduces the concept of storage in deep sedimentary formations, the geomechanical processes and issues related with such an operation, and the relevant geomechanical modeling tools. This is followed by a more detailed review of geomechanical aspects, including reservoir stress-strain and microseismicity, well integrity, caprock sealing performance, and the potential for fault reactivation and notable (felt) seismic events. Geo-mechanical observations at current GCS field deployments, mainly at the In Salah CO 2 storage project in Algeria, are also integrated into the review. The In Salah project, with its injection into a relatively thin, low-permeability sandstone is an excellent analogue to the saline aquifers that might be used for large scale GCS in parts of Northwest Europe, the U.S. Midwest, and China. Some of the lessons learned at In Salah related to geomechanics are discussed, including how monitoring of geomechanical responses is used for detecting subsurface geomechanical changes and tracking fluid movements, and how such monitoring and geomechanical analyses have led to preventative changes in the injection parameters. Recently, the importance of geomechanics has become more widely recognized among GCS stakeholders, especially with respect to the potential for triggering notable (felt) seismic events and how such events could impact the long-term integrity of a CO2 repository (as well as how it could impact the public perception of GCS). As described in the paper, to date, no notable seismic event has been reported from any of the current CO2 storage projects, although some unfelt microseismic activities have been detected by geophones. However, potential future commercial GCS operations from large power plants will require injection at a much larger scale. For such large-scale injections, a staged, learn-as-you-go approach is recommended, involving a gradual increase of injection rates combined with continuous monitoring of geomechanical changes, as well as siting beneath a multiple layered overburden for multiple flow barrier protection, should an unexpected deep fault reactivation occur.},
author = {Rutqvist, Jonny},
doi = {10.1007/s10706-011-9491-0},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geotechnical and Geological Engineering/Rutqvist{\_}2012.pdf:pdf},
issn = {0960-3182},
journal = {Geotechnical and Geological Engineering},
keywords = {CO2 storage,Caprock,Fault reactivation,Geomechanics,Modeling,Saline formations,Seismicity,flac,tough2},
mendeley-tags = {flac,tough2},
month = {jun},
number = {3},
pages = {525--551},
title = {{The Geomechanics of CO2 Storage in Deep Sedimentary Formations}},
url = {http://link.springer.com/10.1007/s10706-011-9491-0},
volume = {30},
year = {2012}
}
@inproceedings{Bishop1989,
author = {Bishop, J M},
booktitle = {Artificial Neural Networks, 1989., First IEE International Conference on (Conf. Publ. No. 313)},
pages = {329--331},
publisher = {IET},
title = {{Stochastic searching networks}},
year = {1989}
}
@article{Saragiotis2002,
abstract = {The automatic and accurate P phase arrival identification is a fundamental problem for seismologists worldwide. Several approaches have been reported in the literature, but most of them only selectively deal with the problem and are severely affected by noise presence. In this paper, a new approach based on higher-order statistics (HOS) is introduced that overcomes the subjectivity of human intervention and eliminates the noise factor. By using skewness and kurtosis, two algorithms have been formed, namely, Phase Arrival Identification-Skewness/Kurtosis (PAI-S/K), and some advantages have been gained over the usual approaches, resulting in the automatic identification of the transition from Gaussianity to non-Gaussianity that coincides with the onset of the seismic event, despite noise presence. Experimental results on real seismic data, gathered by the Seismological Network of the Department of Geophysics of Aristotle University, demonstrate an excellent performance of the PAI-S/K scheme, regarding both accuracy and noise robustness. The simplicity of the proposed method makes it an attractive candidate for huge seismic data assessment in a real-time context.},
author = {Saragiotis, Christos D. and Hadjileontiadis, Leontios J. and Panas, Stravos M.},
doi = {10.1109/TGRS.2002.800438},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Geoscience and Remote Sensing/Saragiotis, Hadjileontiadis, Panas{\_}2002.pdf:pdf},
isbn = {0196-2892},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Automatic seismic P phase arrival identification,Higher-order statistics (HOS),Phase arrival identification-skewness/kurtosis (PA,Real-time implementation,Seismic signal processing,picking},
mendeley-tags = {picking},
month = {jun},
number = {6},
pages = {1395--1404},
title = {{PAI-S/K: A robust automatic seismic P phase arrival identification scheme}},
url = {http://ieeexplore.ieee.org/document/1020271/},
volume = {40},
year = {2002}
}
@article{Goudie2017,
abstract = {MultiBUGS (https://www.multibugs.org) is a new version of the general-purpose Bayesian modelling software BUGS that implements a generic algorithm for parallelising Markov chain Monte Carlo (MCMC) algorithms to speed up posterior inference of Bayesian models. The algorithm parallelises evaluation of the product-form likelihoods formed when a parameter has many children in the directed acyclic graph (DAG) representation; and parallelises sampling of conditionally-independent sets of parameters. A heuristic algorithm is used to decide which approach to use for each parameter and to apportion computation across computational cores. This enables MultiBUGS to automatically parallelise the broad range of statistical models that can be fitted using BUGS-language software, making the dramatic speed-ups of modern multi-core computing accessible to applied statisticians, without requiring any experience of parallel programming. We demonstrate the use of MultiBUGS on simulated data designed to mimic a hierarchical e-health linked-data study of methadone prescriptions including 425,112 observations and 20,426 random effects. Posterior inference for the e-health model takes several hours in existing software, but MultiBUGS can perform inference in only 28 minutes using 48 computational cores.},
archivePrefix = {arXiv},
arxivId = {1704.03216},
author = {Goudie, Robert J. B. and Turner, Rebecca M. and {De Angelis}, Daniela and Thomas, Andrew},
eprint = {1704.03216},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Goudie et al.{\_}2017.pdf:pdf},
keywords = {bayesian analysis,bugs,directed acyclic graph,gibbs sam-,hierarchical models,markov chain monte carlo,parallel computing,pling},
month = {apr},
pages = {1--19},
title = {{MultiBUGS: A parallel implementation of the BUGS modelling framework for faster Bayesian inference}},
url = {http://arxiv.org/abs/1704.03216},
year = {2017}
}
@inproceedings{Gong2011,
abstract = {Achieving good performance with a parallel genetic algorithm requires properly configuring control parameters such as mutation rate, crossover rate, and population size. We consider the problem of setting control parameter values in a standard, island-model distributed genetic algorithm. As an alternative to tuning parameters by hand or using a self-adaptive approach, we propose a very simple strategy which statically assigns random control parameter values to each processor. Experiments on benchmark problems show that this simple approach can yield results which are competitive with homogeneous distributed genetic algorithm using parameters tuned specifically for each of the benchmarks.},
author = {Gong, Yiyuan and Fukunaga, Alex},
booktitle = {2011 IEEE Congress of Evolutionary Computation (CEC)},
doi = {10.1109/CEC.2011.5949703},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2011 IEEE Congress of Evolutionary Computation (CEC)/Gong, Fukunaga{\_}2011.pdf:pdf},
isbn = {978-1-4244-7834-7},
issn = {Pending},
keywords = {hpc},
mendeley-tags = {hpc},
month = {jun},
pages = {820--827},
publisher = {IEEE},
title = {{Distributed island-model genetic algorithms using heterogeneous parameter settings}},
url = {http://ieeexplore.ieee.org/document/5949703/},
year = {2011}
}
@article{Figueiredo2014,
abstract = {In recent years, the Extreme Learning Machine (ELM) has been hybridized with the Particle Swarm Optimization (PSO) and such hybridization is called PSO-ELM. In most of these hybridizations, the PSO uses the Global topology. However, other topologies were designed to improve the performance of the PSO. In the literature, it is well known that the performance of the PSO depends on its topology, and there is not a best topology for all problems. Thus, in this paper, we investigate the effect of eight PSO topologies on performance of the PSO-ELM. The results showed empirically that the Global topology was more promising than all other topologies in optimizing the PSO-ELM according to the root mean squared error (RMSE) on the validation set in most of the evaluated datasets. However, no correlation was detected between this good performance on the RMSE and the testing accuracy. {\textcopyright} 2013 Elsevier B.V.},
author = {Figueiredo, Elliackin M.N. and Ludermir, Teresa B.},
doi = {10.1016/j.neucom.2013.05.047},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Neurocomputing/Figueiredo, Ludermir{\_}2014.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {ELM-PSO,Extreme learning machine,PSO topology,Particle swarm optimization},
pages = {4--12},
publisher = {Elsevier},
title = {{Investigating the use of alternative topologies on performance of the PSO-ELM}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.05.047},
volume = {127},
year = {2014}
}
@article{Press1968,
author = {Press, Frank},
doi = {10.1029/JB073i016p05223},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Press{\_}1968.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research},
keywords = {doi:10.,http://dx.doi.org/10.1029/JB073i016p05223,mcmc},
mendeley-tags = {mcmc},
month = {aug},
number = {16},
pages = {5223--5234},
title = {{Earth models obtained by Monte Carlo Inversion}},
url = {http://doi.wiley.com/10.1029/JB073i016p05223},
volume = {73},
year = {1968}
}
@article{Ge2003,
abstract = {Iterative algorithms are of particular importance in source location as they provide a much more flexible means to solve nonlinear equations, which is essential in order to deal with a wide range of practical problems. The most important iterative algorithms are Geiger's method and the Simplex method. This article provides an overview of iterative algorithms as well as an in-depth analysis of several major methods.},
author = {Ge, Maochen},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of acoustic emission/Ge{\_}2003.pdf:pdf},
journal = {Journal of acoustic emission},
keywords = {location},
mendeley-tags = {location},
pages = {29--51},
title = {{Analysis of source location algorithms, Part II: Iterative methods}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:ANALYSIS+OF+SOURCE+LOCATION+ALGORITHMS+Part+II+:+Iterative+methods{\#}0},
volume = {21},
year = {2003}
}
@article{Wang1996,
abstract = {An artificial neural network-based pattern classification system is applied to seismic event detection. We have designed two types of Artificial Neural Detector (AND) for real-time earthquake detection. Type A artificial neural detector (AND-A) uses the recursive STA/LTA time series as input data, and type B (AND-B) uses moving window spectrograms as input data to detect earth-quake signals. The two AND's are trained under supervised learning by using a set of seismic recordings, and then the trained AND's are applied to another set of recordings for testing. Results show that the accuracy of the artificial neural network-based seismic detectors is better than that of the conventional algorithms solely based on the STA/LTA threshold. This is especially true for signals with either low signal-to-noise ratio or spikelike noises.},
author = {Wang, Jin and Teng, Ta-Liang},
doi = {10.1016/0148-9062(96)86904-X},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Rock Mechanics and Mining Sciences {\&} Geomechanics Abstracts/Wang, Teng{\_}1996.pdf:pdf},
isbn = {0037-1106},
issn = {01489062},
journal = {International Journal of Rock Mechanics and Mining Sciences {\&} Geomechanics Abstracts},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
month = {apr},
number = {3},
pages = {A107},
title = {{Artificial neural network-based seismic detector}},
url = {http://linkinghub.elsevier.com/retrieve/pii/014890629686904X},
volume = {33},
year = {1996}
}
@article{Will2016a,
abstract = {Microseismic monitoring at the Illinois Basin – Decatur Project (IBDP) is accomplished using a combination of commercially available components that are integrated to provide real-time analysis and remote processing capabilities by means of a purpose-built data management system and secure web portal. The processing workflow consists of real-time, remote access, and in-house processing components, which provides a seamless path from in-field quality control to final locations with short turn-around times. Event location is performed using an adaptation of the Geiger method, which is designed to be robust for the sparse but localized observation sets typically encountered with injection monitoring. Observed microseismicity displayed distinct linear clustering and increased in distance from the injection well over time, presenting challenges for location accuracy of more distal events. The velocity model, a key component in event location and characterization, evolved through early stages of the project as new wells were drilled providing the opportunity for improved observation geometry and acquisition of additional controlled energy source points for model calibration. The experience gained at IBDP highlights the importance of field systems and processing flows that allow adaptation to evolving operational conditions and microseismic event activity.},
author = {Will, Robert and El-Kaseeh, George and Jaques, Paul and Carney, Michael and Greenberg, Sallie and Finley, Robert},
doi = {10.1016/j.ijggc.2016.01.007},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Will et al.{\_}2016.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Acquisition,Geiger,Mechanism,Microseismic,Processing,Trigger,decatur},
mendeley-tags = {decatur},
month = {nov},
pages = {404--420},
publisher = {Elsevier Ltd},
title = {{Microseismic data acquisition, processing, and event characterization at the Illinois Basin – Decatur Project}},
url = {http://dx.doi.org/10.1016/j.ijggc.2016.01.007 https://linkinghub.elsevier.com/retrieve/pii/S175058361630007X},
volume = {54},
year = {2016}
}
@article{Whitley1994,
abstract = {This tutorial covers the canonical genetic algorithm as well as more experimental forms of genetic algorithms, including parallel island models and parallel cellular genetic algorithms. The tutorial also illustrates genetic search by hyperplane sampling. The theoretical foun- dations of genetic algorithms are reviewed, include the schema theorem as well as recently developed exact models of the canonical genetic algorithm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Whitley, Darrell},
doi = {10.1007/BF00175354},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Statistics and Computing/Whitley{\_}1994.pdf:pdf},
isbn = {1101001100},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Genetic algorithms,ga,parallel algorithms,search},
mendeley-tags = {ga},
month = {jun},
number = {2},
pages = {65--85},
pmid = {848},
title = {{A genetic algorithm tutorial}},
url = {http://link.springer.com/10.1007/BF00175354},
volume = {4},
year = {1994}
}
@inproceedings{Mohamed2010a,
abstract = {Reservoir modelling is frequently used in the oil industry to measure the risk associated with alternative production scenarios. However, reservoir models themselves contain a high level of uncertainty because of the typically very limited, sparse and multi-scaled reservoir knowledge. The effect of this uncertainty can be assessed by producing a set of diverse models that match the production data reasonably well and using these models to quantify uncertainty in predicting the future performance of the reservoir. Evolutionary and swarm intelligence algorithms have become very popular for history matching due to their simplicity and parallel implementation capacity. This paper focuses on the application of Particle Swarm Optimisation (PSO) for history matching the Brugge field (a recent SPE benchmark case study). The parameterisation of the model is based on principal component analysis (PCA) for modelling spatially correlated random fields (e.g. porosity, net-to-gross and permeability) applied to the set of initial realisations which describe the range of prior beliefs. The PSO is then used to find the set of possible combinations of parameters, represented by the PCA eigenvalues, which match the historical data and honour the static data from the wells present in the initial realisations. We show that PSO is able to find multiple good and diverse history matched models for the Brugge reservoir without exhaustive sampling of the parameter space. Uncertainty of production forecasts are quantified by P10-P50-P90 uncertainty envelope obtained from the ensemble of PSO models. The history matching results are compared with the ones obtained with Ensemble Kalman Filter data assimilation method. These results show the ability of PSO to handle large history matching problems and obtain results comparable to the EnKF for this case study.},
author = {Mohamed, Lina and Christie, Mike A. and Demyanov, Vasily and Robert, Emmanuel and Kachuma, Dick},
booktitle = {SPE Annual Technical Conference and Exhibition},
doi = {10.2118/135264-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Annual Technical Conference and Exhibition/Mohamed et al.{\_}2010.pdf:pdf},
isbn = {9781555633004},
keywords = {pso},
mendeley-tags = {pso},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Application of Particle Swarms for History Matching in the Brugge Reservoir}},
url = {http://www.onepetro.org/doi/10.2118/135264-MS},
year = {2010}
}
@article{Biswas2015,
abstract = {A very fast simulated annealing (VFSA) global optimization is used to interpret residual gravity anomaly. Since, VFSA optimization yields a large number of best-fitted models in a vast model space; the nature of uncertainty in the interpretation is also examined simultaneously in the present study. The results of VFSA optimization reveal that various parameters show a number of equivalent solutions when shape of the target body is not known and shape factor 'q' is also optimized together with other model parameters. The study reveals that amplitude coefficient k is strongly dependent on shape factor. This shows that there is a multi-model type uncertainty between these two model parameters derived from the analysis of cross-plots. However, the appraised values of shape factor from various VFSA runs clearly indicate whether the subsurface structure is sphere, horizontal or vertical cylinder type structure. Accordingly, the exact shape factor (1.5 for sphere, 1.0 for horizontal cylinder and 0.5 for vertical cylinder) is fixed and optimization process is repeated. After fixing the shape factor, analysis of uncertainty and cross-plots shows a well-defined uni-model characteristic. The mean model computed after fixing the shape factor gives the utmost consistent results. Inversion of noise-free and noisy synthetic data as well as field data demonstrates the efficacy of the approach.},
author = {Biswas, Arkoprovo},
doi = {10.1016/j.gsf.2015.03.001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geoscience Frontiers/Biswas{\_}2015.pdf:pdf;:C$\backslash$:/Users/keurf/OneDrive/Articles/Geoscience Frontiers/Biswas{\_}2015(2).pdf:pdf},
issn = {16749871},
journal = {Geoscience Frontiers},
keywords = {Gravity anomaly,Idealized body,Ore exploration,Subsurface structure,Uncertainty,VFSA,vfsa},
mendeley-tags = {vfsa},
month = {nov},
number = {6},
pages = {875--893},
publisher = {Elsevier Ltd},
title = {{Interpretation of residual gravity anomaly caused by simple shaped bodies using very fast simulated annealing global optimization}},
url = {http://dx.doi.org/10.1016/j.gsf.2015.03.001 https://linkinghub.elsevier.com/retrieve/pii/S1674987115000389},
volume = {6},
year = {2015}
}
@article{Fonseca1995,
author = {Fonseca, Carlos M and Fleming, Peter J},
issn = {1063-6560},
journal = {Evolutionary computation},
number = {1},
pages = {1--16},
publisher = {MIT Press},
title = {{An overview of evolutionary algorithms in multiobjective optimization}},
volume = {3},
year = {1995}
}
@article{Rawlinson2010,
abstract = {The goal of this paper is to provide an overview of the current state of the art in seismic tomography, and trace its origins from pioneering work in the early 1970s to its present status as the pre-eminent tool for imaging the Earth's interior at a variety of scales. Due to length limitations, we cannot hope to cover every aspect of this diverse topic or include mathematical derivations of the underlying principles; rather, we will provide a largely descriptive coverage of the methodology that is targeted at readers not intimately familiar with the topic. The relative merits of local versus global parameterization, ray tracing versus wavefront tracking, backprojection versus gradient based inversion and synthetic testing versus model covariance are explored. A variety of key application areas are also discussed, including body wave traveltime tomography, surface wave tomography, attenuation tomography and ambient noise tomography. Established and emerging trends, many of which are driven by the ongoing rapid increases in available computing power, will also be examined, including finite frequency tomography, full waveform tomography and joint tomography using multiple datasets. Several practical applications of seismic tomography, including body wave traveltime, attenuation and surface waveform, are presented in order to reinforce prior discussion of theory. ?? 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1402.3243},
author = {Rawlinson, N. and Pozgay, S. and Fishwick, S.},
doi = {10.1016/j.pepi.2009.10.002},
eprint = {1402.3243},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Rawlinson, Pozgay, Fishwick{\_}2010.pdf:pdf},
isbn = {0148-0227},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {Body wave,Earth structure,Inversion,Ray tracing,Seismic tomography,Surface wave},
month = {feb},
number = {3-4},
pages = {101--135},
pmid = {24553238},
title = {{Seismic tomography: A window into deep Earth}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031920109002106},
volume = {178},
year = {2010}
}
@inproceedings{Yang2012,
author = {Yang, Xin-She},
booktitle = {International conference on unconventional computing and natural computation},
pages = {240--249},
publisher = {Springer},
title = {{Flower pollination algorithm for global optimization}},
year = {2012}
}
@inproceedings{Kennedy1995,
abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described},
author = {Kennedy, J and Eberhart, R},
booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
doi = {10.1109/ICNN.1995.488968},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of ICNN'95 - International Conference on Neural Networks/Kennedy, Eberhart{\_}1995.pdf:pdf},
isbn = {0-7803-2768-3},
issn = {19353812},
keywords = {Artificial neural networks,Birds,Educational institutions,Genetic algorithms,Humans,Marine animals,Optimization methods,Particle swarm optimization,Performance evaluation,Testing,artificial intelligence,artificial life,evolution,genetic algorithms,multidimensional search,neural nets,neural network,nonlinear functions,optimization,particle swarm,pso,search problems,simulation,social metaphor},
mendeley-tags = {pso},
pages = {1942--1948},
pmid = {20371407},
publisher = {IEEE},
title = {{Particle swarm optimization}},
url = {http://ieeexplore.ieee.org/document/488968/},
volume = {4},
year = {1995}
}
@article{Bottero2016,
abstract = {Markov chain Monte Carlo sampling methods are widely used for non-linear Bayesian inver-sion where no analytical expression for the forward relation between data and model parameters is available. Contrary to the linear(ized) approaches, they naturally allow to evaluate the un-certainties on the model found. Nevertheless their use is problematic in high-dimensional model spaces especially when the computational cost of the forward problem is significant and/or the a posteriori distribution is multimodal. In this case, the chain can stay stuck in one of the modes and hence not provide an exhaustive sampling of the distribution of interest. We present here a still relatively unknown algorithm that allows interaction between several Markov chains at different temperatures. These interactions (based on importance resampling) ensure a robust sampling of any posterior distribution and thus provide a way to efficiently tackle complex fully non-linear inverse problems. The algorithm is easy to implement and is well adapted to run on parallel supercomputers. In this paper, the algorithm is first introduced and applied to a synthetic multimodal distribution in order to demonstrate its robustness and efficiency compared to a simulated annealing method. It is then applied in the framework of first arrival traveltime seismic tomography on real data recorded in the context of hydraulic fracturing. To carry out this study a wavelet-based adaptive model parametrization has been used. This allows to integrate the a priori information provided by sonic logs and to reduce optimally the dimension of the problem.},
author = {Bottero, Alexis and Gesret, Alexandrine and Romary, Thomas and Noble, Mark and Maisons, Christophe},
doi = {10.1093/gji/ggw272},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Bottero et al.{\_}2016.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {inverse theory,mcmc,probability distributions,tomography,wavelet transform},
mendeley-tags = {mcmc},
month = {oct},
number = {1},
pages = {374--392},
title = {{Stochastic seismic tomography by interacting Markov chains}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggw272},
volume = {207},
year = {2016}
}
@article{Tanaka2010,
author = {Tanaka, Toshiaki and Itoi, Ryuichi},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings World Geothermal Congress/Tanaka, Itoi{\_}2010.pdf:pdf},
journal = {Proceedings World Geothermal Congress},
keywords = {computer software,graphical user interface,numerical modeling,three-dimensional visualization,tough2},
mendeley-tags = {tough2},
number = {April},
pages = {1--4},
title = {{Development of Numerical Modeling Environment for TOUGH2 Simulator on the Basis of Graphical User Interface (GUI)}},
year = {2010}
}
@article{Shaw2007,
abstract = {Particle swarm optimization (PSO) is a global optimization strategy that simulates the social behavior observed in a flock (swarm) of birds searching for food. A simple search strategy in PSO guides the algorithm toward the best solution through constant updating of the cognitive knowledge and social behavior of the particles in the swarm. To evaluate the applicability of PSO to inversion of geophysical data, we inverted three noise-corrupted synthetic sounding data sets over a multilayered 1D earth model by using DC, induced polarization (IP), and magnetotelluric (MT) methods. The results show that acceptable solutions can be obtained with a swarm of about 300 particles and that convergence occurs in less than 100 iterations. The time required to execute a PSO algorithm is comparable to that of a genetic algorithm (GA). Similarly, the models estimated from PSO and GA are close to the true solutions. Whereas a ridge regression (RR) algorithm converges in four to eight iterations, it yields satisfactory results only when the initial model is very close to the true model. Models estimated from PSO explain observed, vertical electric sounding (VES) and MT data, from Bhiwani district, Haryana, India, and the Chottanagpur gneissic complex, Dhanbad, India. The results are consistent with RR and GA inversions.},
author = {Shaw, Ranjit and Srivastava, Shalivahan},
doi = {10.1190/1.2432481},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Shaw, Srivastava{\_}2007.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {pso},
mendeley-tags = {pso},
month = {mar},
number = {2},
pages = {F75--F83},
title = {{Particle swarm optimization: A new tool to invert geophysical data}},
url = {http://library.seg.org/doi/abs/10.1190/1.2432481{\%}5Cnhttp://library.seg.org/doi/pdf/10.1190/1.2432481 http://library.seg.org/doi/10.1190/1.2432481},
volume = {72},
year = {2007}
}
@article{Yeh2013,
abstract = {A new open source graphical tool, called TIM, has been developed to aid the workflow of TOUGH2 reservoir model development and calibration. Visualisation of model pa- rameters and simulation results is TIM's main feature. It also allows the user to manipulate the model data file inter- actively. In contrast to some other software tools which excel at qualitative 3-D visualisation and presentation, TIM mainly aims at producing easily accessible 2-D layer and 2- D slice plots that integrate the display of colour, text and flow arrows. This allows clearer quantitative assessment of local model behaviour which is often important for manual model calibration. In this respect, the software fills a gap among the available graphical pre-/post-processors for TOUGH2. Instead of a more generic design focused on complete flexibility, the user interface is tailored to match the common workflow of model calibration. The operation of the software is designed to ease the cycle of parameter adjustment, simulation, and visualisation and plotting of results. The software is written in the Python programming language, and makes use of the PyTOUGH library and the application framework PyQt. This allowed very rapid development, and provides cross-platform functionality and easy extensibility. The software has already proven very useful with only limited time and energy invested in its development. 1.},
author = {Yeh, Angus and Croucher, Adrian E. and O'Sullivan, Michael J.},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/35th New Zealand Geothermal Workshop 2013 Proceedings/Yeh, Croucher, O'Sullivan{\_}2013.pdf:pdf},
journal = {35th New Zealand Geothermal Workshop: 2013 Proceedings},
keywords = {GUI,MULgraph,PyQt,PyTOUGH,TOUGH2,graphics,python,reservoir modelling,tough2,visualisation},
mendeley-tags = {tough2},
number = {November},
title = {{Tim – Yet Another Graphical Tool for TOUGH2}},
year = {2013}
}
@misc{Senel2012,
author = {Senel, Ozgur},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Senel{\_}2012.pdf:pdf},
keywords = {decatur},
mendeley-tags = {decatur},
title = {{IBDP Integrated Reservoir Modeling}},
year = {2012}
}
@article{Bodin2009,
abstract = {The reversible jump algorithm is a statistical method for Bayesian inference with a variable number of unknowns. Here, we apply this method to the seismic tomography problem. The approach lets us consider the issue of model parametrization (i.e. the way of discretizing the velocity field) as part of the inversion process. The model is parametrized using Voronoi cells with mobile geometry and number. The size, position and shape of the cells defining the velocity model are directly determined by the data. The inverse problem is tackled within a Bayesian framework and explicit regularization of model parameters is not required. The mobile position and number of cells means that global damping procedures, controlled by an optimal regularization parameter, are avoided. Many velocity models with variable numbers of cells are generated via a transdimensional Markov chain and information is extracted from the ensemble as a whole. As an aid to interpretation we visualize the expected earth model that is obtained via Monte Carlo integration in a straightforward manner. The procedure is particularly adept at imaging rapid changes or discontinuities in wave speed. While each velocity model in the final ensemble consists of many discontinuities at cell boundaries, these are smoothed out in the averaged ensemble solution while those required by the data are reinforced. The ensemble of models can also be used to produce uncertainty estimates and experiments with synthetic data suggest that they represent actual uncertainty surprisingly well. We use the fast marching method in order to iteratively update the ray geometry and account for the non-linearity of the problem. The method is tested here with synthetic data in a 2-D application and compared with a subspace method that is a more standard matrix-based inversion scheme. Preliminary results illustrate the advantages of the reversible jump algorithm. A real data example is also shown where a tomographic image of Rayleigh wave group velocity for the Australian continent is constructed together with uncertainty estimates.},
author = {Bodin, Thomas and Sambridge, Malcolm},
doi = {10.1111/j.1365-246X.2009.04226.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Bodin, Sambridge{\_}2009.pdf:pdf},
isbn = {0956540X$\backslash$r1365246X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Australia,Computational seismology,Inverse theory,Probability distribution,Seismic tomography,Tomography,rjmcmc},
mendeley-tags = {rjmcmc},
month = {sep},
number = {3},
pages = {1411--1436},
title = {{Seismic tomography with the reversible jump algorithm}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2009.04226.x},
volume = {178},
year = {2009}
}
@article{Angeline1994,
author = {Angeline, Peter J and Saunders, Gregory M and Pollack, Jordan B},
issn = {1045-9227},
journal = {IEEE transactions on Neural Networks},
number = {1},
pages = {54--65},
publisher = {IEEE},
title = {{An evolutionary algorithm that constructs recurrent neural networks}},
volume = {5},
year = {1994}
}
@article{Bosch2006,
abstract = {We jointly invert gravity and magnetic data following a Monte Carlo method that provides estimation for a 3Dmodel of the structure and physical properties of the medium. In par- ticular, the model layer geometry and the density and mag- netic susceptibility fields within layers are estimated, and their uncertainties are described with posterior probabilities. This method combines the gravity and magnetic data with prior information of the mass density and magnetic suscepti- bility statistics, and statistical constraints on the model inter- face positions. The resulting model realizations jointly com- ply with the observations and the prior statistical information.},
author = {Bosch, Miguel and Meza, Ronny and Jim{\'{e}}nez, Rosa and H{\"{o}}nig, Alfredo},
doi = {10.1190/1.2209952},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Bosch et al.{\_}2006.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {mcmc},
mendeley-tags = {mcmc},
month = {jul},
number = {4},
pages = {G153--G156},
title = {{Joint gravity and magnetic inversion in 3D using Monte Carlo methods}},
url = {http://library.seg.org/doi/10.1190/1.2209952},
volume = {71},
year = {2006}
}
@article{Kikuchi1991,
abstract = {We have developed a method that inverts seismic body waves to determine the mechanism and rupture pattern of earthquakes. The rupture pattern is represented as a sequence of subevents distributed on the fault plane. This method is an extension of our earlier method in which the subevent mechanisms were fixed. In the new method, the subevent mechanisms are determined from the data and are allowed to vary during the sequence. When subevent mechanisms are allowed to vary, however, the inversion often becomes unstable because of the complex trade-offs between the mechanism, the timing, and the location of the subevents. Many different subevent sequences can explain the same data equally well, and it is important to determine the range of allowable solutions. Some constraints must be imposed on the solution to stabilize the inversion. We have developed a procedure to explore the range of allowable solutions and appropriate constraints. In this procedure, a network of grid points is constructed on the $\tau$ - I plane, where $\tau$ and I are, respectively, the onset time and the distance from the epicenter of a subevent; the best-fit subevent is determined at all grid points. Then the correlation is computed between the synthetic waveform for each subevent and the observed waveform. The correlation as a function of $\tau$ and I and the best-fit mechanisms computed at each $\tau$ - I grid point depict the character of allowable solutions and facilitate a decision on the appropriate constraints to be imposed on the solution. The method is illustrated using the data for the 1976 Guatemala earthquake.},
author = {Kikuchi, Masayuki and Kanamori, Hiroo},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Kikuchi, Kanamori{\_}1991.pdf:pdf},
issn = {00319201},
journal = {Bulletin of the Seismological Society of America},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {sep},
number = {6},
pages = {2335--2350},
title = {{Inversion of complex body waves-III}},
volume = {81},
year = {1991}
}
@inproceedings{Loshchilov2014,
abstract = {We propose a computationally efficient limited memory Covariance Matrix Adaptation Evolution Strategy for large scale optimization, which we call the LM-CMA-ES. The LM-CMA-ES is a stochastic, derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems in continuous domain. Inspired by the limited memory BFGS method of Liu and Nocedal (1989), the LM-CMA-ES samples candidate solutions according to a covariance matrix reproduced from {\$}m{\$} direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the time and memory complexity of the sampling to {\$}O(mn){\$}, where {\$}n{\$} is the number of decision variables. When {\$}n{\$} is large (e.g., {\$}n{\$} {\textgreater} 1000), even relatively small values of {\$}m{\$} (e.g., {\$}m=20,30{\$}) are sufficient to efficiently solve fully non-separable problems and to reduce the overall run-time.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1404.5520},
author = {Loshchilov, Ilya},
booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
doi = {10.1145/2576768.2598294},
eprint = {1404.5520},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14/Loshchilov{\_}2014.pdf:pdf},
isbn = {9781450326629},
keywords = {cma-es,cmaes,evolution strategies,large scale optimization},
mendeley-tags = {cmaes},
pages = {397--404},
publisher = {ACM Press},
title = {{A computationally efficient limited memory CMA-ES for large scale optimization}},
url = {http://arxiv.org/abs/1404.5520 http://dl.acm.org/citation.cfm?doid=2576768.2598294},
year = {2014}
}
@article{Kaven2014,
abstract = {The viability of carbon capture and storage (CCS) to reduce emissions of greenhouse gases depends on the ability to safely sequester large quantities of CO2over geologic time scales. One concern with CCS is the potential for induced seismicity. We report on ongoing seismic monitoring by the U.S. Geological Survey (USGS) at a CCS demonstration site in Decatur, IL, in an effort to understand the potential hazards posed by injection-induced seismicity associated with geologic CO2sequestration. At Decatur, super-critical CO2is injected at 2.1 km depth into the Mt. Simon Sandstone, which directly overlies granitic basement. The primary sealing cap is the Eau Claire Shale at a depth of about 1.5 km. The USGS seismic network was installed starting in July 2013 and consists of 12 stations, three of which include borehole sensors at depths of 150 m. The aperture of this network is about 8 km, centered on the injection well. We derived a one-dimensional velocity model from a vertical seismic profile acquired by Archer-Daniels-Midland (ADM) and the Illinois State Geological Survey (ISGS) to a depth of 2.2 km, tied into acoustic logs from a deep observation well and the USGS borehole stations. We assume a constant ratio of P- To S-wave velocities of 1.83, as derived from simultaneous earthquake relocations and velocity inversions. We use this velocity model to locate seismic events, all of which are within the footprint of our network. Moment magnitudes of events located to date range from -1.52 to 1.07. We further improved the hypocentral precision of microseismic events when travel times and waveforms are sufficiently similar by employing double-difference relocation techniques, with relative location errors less than 80 m horizontally and 100 m vertically. Relocated seismicity tends to group in three distinct clusters: ∼0.4 to 1.0 km NE, 1.8 to 2.3 km N, and ∼2.0 to 2.6 km WNW from the injection well. The first cluster of microseismicity forms a roughly linear trend, which may represent a pre-existing geologic structure. Most of these micro-earthquakes occur in the granitic basement at depths greater than 2.2 km, well below the caprock, and likely do not compromise the integrity of the seal.},
author = {Kaven, J. Ole and Hickman, Stephen H. and McGarr, Arthur F. and Walter, Steven and Ellsworth, William L.},
doi = {10.1016/j.egypro.2014.11.461},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Kaven et al.{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {CO2 sequestration,Decatur (IL),Induced seismicity,Seismic monitoring,decatur},
mendeley-tags = {decatur},
number = {Il},
pages = {4264--4272},
publisher = {Elsevier B.V.},
title = {{Seismic monitoring at the Decatur, IL, CO2 sequestration demonstration site}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.461 https://linkinghub.elsevier.com/retrieve/pii/S1876610214022760},
volume = {63},
year = {2014}
}
@article{Schwefel1984,
author = {Schwefel, Hans-Paul},
issn = {0254-5330},
journal = {Annals of Operations Research},
number = {2},
pages = {165--167},
publisher = {Springer},
title = {{Evolution strategies: A family of non-linear optimization techniques based on imitating some principles of organic evolution}},
volume = {1},
year = {1984}
}
@article{Moser1992,
abstract = {The shortest path method [Moser, 1991 ] for the calculation of seismic ray paths and travel times along them' can be applied directly in the hypocenter location method proposed by Tarantola and Valette [1982]. It uses the analogy between seismic rays in the Earth and shortest paths in networks to construct first arrival times from one point to all other points of a three-dimensional grid simultaneously in a fast, robust way, in Earth models of arbitrary complexity. Doing this for all stations of a seismic array, one can find the hypocenter location by minimizing the difference between the observed and the calculated travel times at the stations over the three-dimensional grid. The concept of probability density functions allows then for a fully nonlinear examination of the uncertainties in the hypocenter location, due to uncertainties in the travel time data, numerical errors in the calculated travel times and, to a limited extent, incomplete knowledge about the Earth model. The result is a three-dimensional contour map of regions of equal confidence for the earthquake location. The method becomes especially attractive when more than one event recorded by the same array is studied, because the calculation of the travel times, which is relatively the most time consuming operation, has to be done only once. The method is applied on the location of an event that occurred on January 18, 1989, in Israel.},
author = {Moser, T. J. and van Eck, T. and Nolet, G.},
doi = {10.1029/91JB03176},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Moser, van Eck, Nolet{\_}1992.pdf:pdf},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
keywords = {location},
mendeley-tags = {location},
number = {B5},
pages = {6563},
title = {{Hypocenter determination in strongly heterogeneous Earth models using the shortest path method}},
url = {http://doi.wiley.com/10.1029/91JB03176},
volume = {97},
year = {1992}
}
@article{Peruzzetto2018,
abstract = {Several days of passive seismic broad-band recordings (vertical component) from a dense 3 × 6 km array installed near Ch{\'{e}}mery (France), with about 100 seismometers, are analysed for wavefield characterization between 0.1 and 3 Hz. Backazimuth is determined by using the Multiple Signal Characterization (MUSIC) algorithm at frequencies below 1 Hz, and non-coherent cross-correlation beamforming above 1 Hz, since the latter is less sensitive to aliasing issues. A novel method of determining the wavefield velocity is introduced, consisting of processing a cross-correlation common-offset gather by the MUSIC algorithm. The fundamental and three higher modes of Rayleigh waves (R0, R1, R2 and R3) are identified under 1 Hz. Above 1.5 Hz, the Lg phase is detected, while R0 and R1 are also present. Roughly between 1 and 1.5 Hz, a quicker phase, probably Pg, is detected. Both Pg and Lg are dominant during night time, suggesting they have a natural origin, which is also consistent with their backazimuth pointing towards the Atlantic. Large scale 2-D spectral-element simulations using deep- and shallow-water ocean sources confirm the possibility of the Lg phase excitation. Thus, even above 1 Hz, natural sources can explain the major part of the ambient noise energy during quiet time periods.},
author = {Peruzzetto, Marc and Kazantsev, Alexandre and Luu, Keurfon and M{\'{e}}taxian, Jean-Philippe and Huguet, Fr{\'{e}}d{\'{e}}ric and Chauris, Herv{\'{e}}},
doi = {10.1093/gji/ggy311},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Peruzzetto et al.{\_}2018.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Guided waves,Numerical modelling,Seismic interferometry,Seismic noise,Surface waves and free oscillations,Wave propagation},
month = {nov},
number = {2},
pages = {760--779},
title = {{Broad-band ambient noise characterization by joint use of cross-correlation and MUSIC algorithm}},
url = {https://academic.oup.com/gji/article/215/2/760/5060752},
volume = {215},
year = {2018}
}
@article{Park2014,
abstract = {A set of three algorithms named PET2OGS is developed to integrate the static model (Petrel) with the dynamic model (OpenGeoSys). PET2OGS consists of three sub-algorithms that convert finite difference methods (FDMs) grids to finite element methods (FEMs) grids. The algorithms and the workflow of the integration procedures are described in detail. After the proposed algorithms are tested on a variety of grids both in homogeneous and heterogeneous media, the integrated platform of the static and dynamic models is applied to model CO2 storage in a saline aquifer. A successful demonstration of the proposed algorithms proved a robust integration of the platform. With some minor modifications of the algorithms in the part of input and output, the proposed algorithms can be extended to integrate different combinations of FDM-based static models and FEM-based dynamic models beyond the example combination in the paper. {\textcopyright} 2013 Elsevier Ltd.},
author = {Park, C.-H. and Shinn, Y.J. and Park, Y.-C. and Huh, D.-G. and Lee, S.K.},
doi = {10.1016/j.cageo.2013.09.014},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Park et al.{\_}2014.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {CO2 storage,Dynamic modeling,FDM,FEM,OpenGeoSys,Petrel,Static modeling,mesh},
mendeley-tags = {mesh},
month = {jan},
pages = {95--102},
publisher = {Elsevier},
title = {{PET2OGS: Algorithms to link the static model of Petrel with the dynamic model of OpenGeoSys}},
url = {http://dx.doi.org/10.1016/j.cageo.2013.09.014 https://linkinghub.elsevier.com/retrieve/pii/S0098300413002549},
volume = {62},
year = {2014}
}
@article{Julian1977,
author = {Julian, B. and Gubbins, D.},
journal = {Journal of Geophysics},
number = {1},
pages = {95--114},
title = {{Three-dimensional seismic ray tracing}},
volume = {43},
year = {1977}
}
@article{Molyneux1999,
author = {Molyneux, Joseph B. and Schmitt, Douglas R.},
doi = {10.1190/1.1444653},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {1492--1501},
title = {{First‐break timing: Arrival onset times by direct correlation}},
url = {https://library.seg.org/doi/10.1190/1.1444653},
volume = {64},
year = {1999}
}
@article{Han2009,
abstract = {The particle swarm optimization (PSO) is an adaptive optimization based on swarm intelligence. The basic principle and the method of it being used in seismic location were introduced. To get a more accurate result, the objective function is the residual square sum of the observational travel-time and theoretical travel-time of the same earthquake return two stations. Compared with Genetic Algorithm on Seismic Location, PSO, after numerous experiments, proved its distinct superiority to locate the hypocenter more quickly and accurately. PSO is potentially useful for seismic location. {\textcopyright} 2009 IEEE.},
author = {Han, Dong-xue and Wang, Gai-yun},
doi = {10.1109/WGEC.2009.48},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2009 Third International Conference on Genetic and Evolutionary Computing/Han, Wang{\_}2009.pdf:pdf},
isbn = {978-1-4244-5245-3},
journal = {2009 Third International Conference on Genetic and Evolutionary Computing},
keywords = {Double-difference,Genetic algorithm,Particle swarm optimization,Seismic location,pso},
mendeley-tags = {pso},
number = {1},
pages = {641--644},
title = {{Application of Particle Swarm Optimization to Seismic Location}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77950654301{\&}partnerID=tZOtx3y1},
year = {2009}
}
@article{Chen2015,
author = {Chen, Stephen and Montgomery, James and Boluf{\'{e}}-R{\"{o}}hler, Antonio},
doi = {10.1007/s10489-014-0613-2},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Applied Intelligence/Chen, Montgomery, Boluf{\'{e}}-R{\"{o}}hler{\_}2015.pdf:pdf},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {Curse of dimensionality,Differential evolution,Exploitation,Exploration,Large scale global optimization,Particle swarm optimization,de,pso},
mendeley-tags = {de,pso},
month = {apr},
number = {3},
pages = {514--526},
title = {{Measuring the curse of dimensionality and its effects on particle swarm optimization and differential evolution}},
url = {http://link.springer.com/10.1007/s10489-014-0613-2},
volume = {42},
year = {2015}
}
@article{Zhang2011,
abstract = {TOUGH+CO2 is a new simulator for modeling of CO2 geologic sequestration in saline aquifers. It is a member of TOUGH+, the successor to the TOUGH2 family of codes for multicomponent, multiphase fluid and heat flow simulation. The code accounts for heat and up to 3 mass components, which are partitioned into three possible phases. In the code, the thermodynamics and thermophysical properties of H2O-NaCl-CO2 mixtures are determined based on system status and subdivided into six different phase combinations. By solving coupled mass and heat balance equations, TOUGH+CO2 can model non-isothermal or isothermal CO2 injection, phase behavior and flow of fluids and heat under typical conditions of temperature, pressure and salinity in CO2 geologic storage projects. The code takes into account effects of salt precipitation on porosity and permeability changes, and the wettability phenomena. The new simulator inherits all capabilities of TOUGH2 in handling fractured media and using unstructured meshes for complex simulation domains. The code adds additional relative permeability and capillary pressure functions. The FORTRAN 95 OOP architecture and other new language features have been extensively used to enhance memory use and computing efficiency. In addition, a domain decomposition approach has been implemented for parallel simulation. All these features lead to increased computational efficiency, and allow applicability of the code to multi-core/processor parallel computing platforms with excellent scalability. {\textcopyright} 2010.},
archivePrefix = {arXiv},
arxivId = {astro-ph.SR/1111.5978},
author = {Zhang, Keni and Moridis, George and Pruess, Karsten},
doi = {10.1016/j.cageo.2010.09.011},
eprint = {1111.5978},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Zhang, Moridis, Pruess{\_}2011.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {CO2 geologic sequestration,Modeling,Multiphase flow,Parallel computing,Saline aquifer,TOUGH+,TOUGH2,tough2},
mendeley-tags = {tough2},
month = {jun},
number = {6},
pages = {714--723},
primaryClass = {astro-ph.SR},
publisher = {Elsevier},
title = {{TOUGH+CO2: A multiphase fluid-flow simulator for CO2 geologic sequestration in saline aquifers}},
url = {http://dx.doi.org/10.1016/j.cageo.2010.09.011 https://linkinghub.elsevier.com/retrieve/pii/S0098300410003201},
volume = {37},
year = {2011}
}
@article{Nash1991,
author = {Nash, Stephen G. and Nocedal, Jorge},
doi = {10.1137/0801023},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {quasi-newton},
mendeley-tags = {quasi-newton},
month = {aug},
number = {3},
pages = {358--372},
title = {{A Numerical Study of the Limited Memory BFGS Method and the Truncated-Newton Method for Large Scale Optimization}},
url = {http://epubs.siam.org/doi/10.1137/0801023},
volume = {1},
year = {1991}
}
@article{Toushmalani2013,
abstract = {Particle swarm optimization is a heuristic global optimization method and also an optimization algorithm, which is based on swarm intelligence. It comes from the research on the bird and fish flock movement behavior. In this paper we introduce and use this method in gravity inverse problem. We discuss the solution for the inverse problem of determining the shape of a fault whose gravity anomaly is known. Application of the proposed algorithm to this problem has proven its capability to deal with difficult optimization problems. The technique proved to work efficiently when tested to a number of models.},
author = {Toushmalani, Reza},
doi = {10.1186/2193-1801-2-315},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SpringerPlus/Toushmalani{\_}2013.pdf:pdf},
isbn = {0256-1115},
issn = {2193-1801},
journal = {SpringerPlus},
keywords = {fault,gravity inversion,particle swarm optimization,pso},
mendeley-tags = {pso},
month = {dec},
number = {1},
pages = {315},
title = {{Gravity inversion of a fault by Particle swarm optimization (PSO)}},
url = {https://springerplus.springeropen.com/articles/10.1186/2193-1801-2-315},
volume = {2},
year = {2013}
}
@phdthesis{Bakari2015,
author = {Bakari, Issam},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Bakari{\_}2015.pdf:pdf},
keywords = {microseismic},
mendeley-tags = {microseismic},
pages = {85},
school = {Universit{\'{e}} du Qu{\'{e}}bec},
title = {{Localisation des {\'{e}}picentres des munitions non explos{\'{e}}es dans les exercices de tirs}},
year = {2015}
}
@article{Warpinski2009,
abstract = {Since its inception in the 1970s and its commercialization circa 2000, microseismic monitoring has proved to be an invaluable tool for understanding underground processes. While its most common and notable use has been hydraulic- fracture mapping, it also is used for reservoir monitoring of thermal processes, drill-cuttings injection, geothermal hot- dry-rock stimulations, reservoir surveillance, and many other processes in oil and gas and mining. Fig. 1 shows a typical layout for a monitoring test, with the offset monitoring well at some reasonable distance from the fracture (inset) and a receiver array somewhere near the depth of the fracture treatment. Because the amplitude of the microseism decays with distance, there is a maximum monitoring distance that can be used in any test with respect to both horizontal and vertical positioning. Designed cor- rectly, this type of monitoring can provide information on fracture height, length, azimuth, asymmetry, dip, and com- plexity, which can be used to optimize the fracture design and field development. To most outsiders, the entire process of microseismic monitoring appears to be something of an art, and the “what,” “why,” and “how” details are not very clear even Norm Warpinski, SPE, is the Direc- tor of Technology Development for Pinnacle—A Halliburton Service, where he is in charge of developing new tools and analyses for hydraulic- fracture mapping, reservoir monitor- ing, and hydraulic-fracture design and analysis, and integrated solutions for reservoir development. Previously, Warpinski worked at Sandia National Laboratories on various projects in oil and gas, geothermal, carbon sequestration, waste repositories, and other geomechanics issues. He has extensive experience in various types of hydraulic-fracture mapping and model- ing and has been involved in large-scale field experiments from both the hardware and software sides. Warpinski also has worked on formation evaluation, geomechanics, natural fractures, in-situ stresses, rock behavior, and rock testing. He earned a BS degree in mechanical engineering from Illinois Institute of Technology and MS and PhD degrees in mechanical engineering from the University of Illinois. 80 though there have probably been more than 6,000 fracture treatments monitored since 2000, in rocks ranging from tight sandstones and gas shales to carbonates and even vol- canic, and at depths ranging from several hundred feet to more than 13,000 ft. The purpose of this article is to lay out a basic framework for planning, executing, analyzing, and interpreting a microseismic mapping project and, hopefully, add some rigor to the process that can be used for guidelines or standards.},
author = {Warpinski, Norm},
doi = {10.2118/118537-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Petroleum Technology/Warpinski{\_}2009.pdf:pdf},
isbn = {0149-2136},
issn = {0149-2136},
journal = {Journal of Petroleum Technology},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {nov},
number = {11},
pages = {80--85},
title = {{Microseismic Monitoring: Inside and Out}},
url = {http://www.onepetro.org/mslib/servlet/onepetropreview?id=SPE-118537-MS{\&}soc=SPE},
volume = {61},
year = {2009}
}
@article{Sambridge1999b,
abstract = {Monte Carlo direct search methods, such as genetic algorithms, simulated annealing, etc., are often used to explore a finite-dimensional parameter space. They require the solving of the forward problem many times, that is, making predictions of observables from an earth model. The resulting ensemble of earth models represents all ‘information' collected in the search process. Search techniques have been the subject of much study in geophysics; less attention is given to the appraisal of the ensemble. Often inferences are based on only a small subset of the ensemble, and sometimes a single member. This paper presents a new approach to the appraisal problem. To our knowledge this is the first time the general case has been addressed, that is, how to infer information from a complete ensemble, previously generated by any search method. The essence of the new approach is to use the information in the available ensemble to guide a resampling of the parameter space. This requires no further solving of the forward problem, but from the new ‘resampled' ensemble we are able to obtain measures of resolution and trade-off in the model parameters, or any combinations of them. The new ensemble inference algorithm is illustrated on a highly non-linear wave-form inversion problem. It is shown how the computation time and memory requirements scale with the dimension of the parameter space and size of the ensemble. The method is highly parallel, and may easily be distributed across several computers. Since little is assumed about the initial ensemble of earth models, the technique is applicable to a wide variety of situations. For example, it may be applied to perform ‘error analysis' using the ensemble generated by a genetic algorithm, or any other direct search method.},
author = {Sambridge, Malcolm},
doi = {10.1046/j.1365-246x.1999.00900.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}1999(2).pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {numerical techniques,receiver functions,uncertainty quantification,waveform inversion},
mendeley-tags = {uncertainty quantification},
month = {sep},
number = {3},
pages = {727--746},
title = {{Geophysical inversion with a neighbourhood algorithm-II. Appraising the ensemble}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246x.1999.00900.x},
volume = {138},
year = {1999}
}
@article{Kaven2015,
author = {Kaven, J. O. and Hickman, S. H. and McGarr, A. F. and Ellsworth, W. L.},
doi = {10.1785/0220150062},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Seismological Research Letters/Kaven et al.{\_}2015.pdf:pdf},
issn = {0895-0695},
journal = {Seismological Research Letters},
keywords = {decatur},
mendeley-tags = {decatur},
month = {jul},
number = {4},
pages = {1096--1101},
title = {{Surface Monitoring of Microseismicity at the Decatur, Illinois, CO 2 Sequestration Demonstration Site}},
url = {https://pubs.geoscienceworld.org/srl/article/86/4/1096-1101/315472},
volume = {86},
year = {2015}
}
@inproceedings{Galvis2016,
abstract = {This work is aimed to detect and classify surface waves by characterizing the movement of ground particles when seis-mic waves are produced and, based on the information from multicomponent seismic records, differentiating how seismic data behaves when surface waves is present. Seismic attributes were used in order to enhance the frequency and polarization information of seismic wave-fields in seismic records; these were computed from short-time windows of multicomponent seismic data in order to build the feature vectors. The unsupervised pattern recognition technique K-means was used to separate surface waves areas from reflection seismic data.},
author = {Galvis, Ivan Javier S{\'{a}}nchez and Villa-Acu{\~{n}}a, Yenni and Bueno, Daniel Alfonso Sierra and Gualdr{\'{o}}n, C{\'{e}}sar Antonio Duarte and Agudelo, William},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
doi = {10.1190/segam2016-13961709.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Galvis et al.{\_}2016.pdf:pdf},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
month = {sep},
pages = {4961--4965},
publisher = {Society of Exploration Geophysicists},
title = {{3-C surface-waves detection and classification via unsupervised pattern recognition from seismic attributes}},
url = {http://library.seg.org/doi/10.1190/segam2016-13961709.1},
year = {2016}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Automatic Control/Akaike{\_}1974.pdf:pdf},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {{A New Look at the Statistical Model Identification}},
volume = {19},
year = {1974}
}
@article{Ramirez2011,
abstract = {In this research, we consider the supervised learning problem of seismic phase classification. In seismology, knowledge of the seismic activity arrival time and phase leads to epicenter localization and surface velocity estimates useful in developing seismic early warning systems and detecting man-made seismic events. Formally, the activity arrival time refers to the moment at which a seismic wave is first detected and the seismic phase classifies the physics of the wave. We propose a new perspective for the classification of seismic phases in three-channel seismic data collected within a network of regional recording stations. Our method extends current techniques and incorporates concepts from machine learning. Machine learning techniques attempt to leverage the concept of "learning'' the patterns associated with different types of data characteristics. In this case, the data characteristics are the seismic phases. This concept makes sense because the characteristics of the phase types are dictated by the physics of wave propagation. Thus by "learning'' a signature for each type of phase, we can apply classification algorithms to identify the phase of incoming data from a database of known phases observed over the recording network. Our method first uses a multi-scale feature extraction technique for clustering seismic data on low-dimensional manifolds. We then apply kernel ridge regression on each feature manifold for phase classification. In addition, we have designed an information theoretic measure used to merge regression scores across the multi-scale feature manifolds. Our approach complements current methods in seismic phase classification and brings to light machine learning techniques not yet fully examined in the context of seismology. We have applied our technique to a seismic data set from the Idaho, Montana, Wyoming, and Utah regions collected during 2005 and 2006. This data set contained compression wave and surface wave seismic phases. Through - ross-validation, our method achieves a 74.6{\%} average correct classification rate when compared to analyst classifications.},
author = {Ramirez, Juan and Meyer, Francois G.},
doi = {10.1109/ICMLA.2011.91},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011/Ramirez, Meyer{\_}2011.pdf:pdf},
isbn = {9780769546070},
journal = {Proceedings - 10th International Conference on Machine Learning and Applications, ICMLA 2011},
keywords = {Manifold Signal Processing,Seismology,Supervised Learning},
pages = {382--388},
title = {{Machine learning for seismic signal processing: Seismic phase classification on a manifold}},
volume = {1},
year = {2011}
}
@article{Sambridge1993,
author = {Sambridge, Malcolm and Gallagher, Kerry},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Sambridge, Gallagher{\_}1993.pdf:pdf},
isbn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
keywords = {ga},
mendeley-tags = {ga},
number = {5},
pages = {1467--1491},
title = {{Earthquake hypocenter location using genetic algorithms}},
volume = {83},
year = {1993}
}
@article{Sasaki1998,
abstract = {Microseismicity accompanying hydraulic injection experiments at the Hijiori hot dry rock site was monitored by a network of ten borehole seismic stations deployed at an average distance of 2 km from the injection well. While expanding hydraulic fractures are almost aseismic, they can induce microseismic events. These events are probably caused by shear failures induced by high pore fluid pressures occurring on planes of weakness in the rock surrounding the main hydraulic fracture. Thus we can use these induced events to locate the hydraulic fracture and follow its growth. Microseismic events induced during the 1988 hydraulic fracturing experiment with a high injection pressure were located near the injection point in the early stage of the experiment and clearly migrated towards the east and distributed along a vertical plane. The strike of seismicity is nearly parallel to the direction of the maximum principal stress. The vertical orientation and east–west strike of the seismic events are essentially coplanar with the caldera ring-fault structure in the southern portion of the Hijiori Caldera. This indicates that a preexisting fracture zone was being re-opened and developed in the direction of the maximum principal stress, although microseismic events were caused by shear failures. The space–time distribution of the microseismic events shows that the events migrated at a rate proportional to time to the power 2/3. Assuming that migration of events is attributed to fracture propagation, the propagation of the hydraulically stimulated fracture can be explained by one of two petroleum industry models tested. Seismicity accompanying the 1989 circulation test with a low injection pressure was diffuse and showed a seismic cloud. The permeability was estimated from the hypocenter migration as 10{\^{}}-16 m², which is intermediate between the permeability of core samples of granodiorite taken from the production well and the permeability of fractured rocks obtained by an injection test between the injection well and the production well. It is therefore concluded that the seismic cloud accompanying the circulation test is due to the permeation of water into joints which slip when the effective stress is reduced by the increased pore fluid pressure accompanying the hydraulic injection. Microseismic events occur and migrate along those joints.},
author = {Sasaki, Shunji},
doi = {10.1016/S0040-1951(97)00314-4},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Tectonophysics/Sasaki{\_}1998.pdf:pdf},
isbn = {00401951},
issn = {00401951},
journal = {Tectonophysics},
keywords = {fracture propagation model,hot dry rock,hydraulic fracturing,induced seismicity,microseismic event,microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {apr},
number = {1-3},
pages = {171--188},
title = {{Characteristics of microseismic events induced during hydraulic fracturing experiments at the Hijiori hot dry rock geothermal energy site, Yamagata, Japan}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0040195197003144},
volume = {289},
year = {1998}
}
@article{Hansen2011a,
abstract = {This paper investigates the behavior of PSO (particle swarm optimization) and CMA-ES (covariance matrix adaptation evolution strategy) on ill-conditioned functions. The paper also highlights momentum as important common concept used in both algorithms and reviews important invariance properties. On separable, ill-conditioned functions, PSO performs very well and outperforms CMA-ES by a factor of up to five. On the same but rotated functions, the performance of CMA-ES is unchanged, while the performance of PSO declines dramatically: on non-separable, ill-conditioned functions we find the search costs (number of function evaluations) of PSO increasing roughly proportional with the condition number and CMA-ES outperforms PSO by orders of magnitude. The strong dependency of PSO on rotations originates from random events that are only independent within the given coordinate system. The CMA-ES adapts the coordinate system where the independent events take place and is rotational invariant. We argue that invariance properties, like rotational invariance, are desirable, because they increase the predictive power of performance results by inducing problem equivalence classes. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Hansen, Nikolaus and Ros, Raymond and Mauny, Nikolas and Schoenauer, Marc and Auger, Anne},
doi = {10.1016/j.asoc.2011.03.001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Applied Soft Computing Journal/Hansen et al.{\_}2011.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {CMA-ES,Covariance matrix adaptation,Evolution strategy,Ill-conditioned problems,Invariance,Non-separable problems,PSO,Particle swarm optimization,Performance assessment,cmaes},
mendeley-tags = {cmaes},
number = {8},
pages = {5755--5769},
title = {{Impacts of invariance in search: When CMA-ES and PSO face ill-conditioned and non-separable problems}},
volume = {11},
year = {2011}
}
@inproceedings{Akimoto2014,
abstract = {We propose a novel natural gradient based stochastic search algorithm, VD-CMA, for the optimization of high dimensional numerical functions. The algorithm is comparisonbased and hence invariant to monotonic transformations of the objective function. It adapts a multivariate normal distribution with a restricted covariance matrix with twice the dimension as degrees of freedom, representing an arbitrarily oriented long axis and additional axis-parallel scaling. We derive the different components of the algorithm and show linear internal time and space complexity. We find empirically that the algorithm adapts its covariance matrix to the inverse Hessian on convex-quadratic functions with an Hessian with one short axis and different scaling on the diagonal. We then evaluate VD-CMA on test functions and compare it to different methods. On functions covered by the internal model of VD-CMA and on the Rosenbrock function, VD-CMA outperforms CMA-ES (having quadratic internal time and space complexity) not only in internal complexity but also in number of function calls with increasing dimension.},
address = {New York, New York, USA},
author = {Akimoto, Youhei and Auger, Anne and Hansen, Nikolaus},
booktitle = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
doi = {10.1145/2576768.2598258},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14/Akimoto, Auger, Hansen{\_}2014.pdf:pdf},
isbn = {9781450326629},
keywords = {cmaes,covariance matrix adaptation,hessian,information geometric optimization,matrix,natural gradient,theory},
mendeley-tags = {cmaes},
pages = {373--380},
publisher = {ACM Press},
title = {{Comparison-based natural gradient optimization in high dimension}},
url = {http://dl.acm.org/citation.cfm?doid=2576768.2598258},
year = {2014}
}
@article{Holland1973,
author = {Holland, John H.},
doi = {10.1137/0202009},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
month = {jun},
number = {2},
pages = {88--105},
title = {{Genetic Algorithms and the Optimal Allocation of Trials}},
url = {http://epubs.siam.org/doi/10.1137/0202009},
volume = {2},
year = {1973}
}
@article{Chapman1985,
author = {Chapman, Chris},
journal = {Journal of Geophysics},
pages = {27--43},
title = {{Ray theory and its extensions: WKBJ and Maslov seismogram}},
volume = {58},
year = {1985}
}
@article{Siebel2007,
author = {Siebel, Nils T and Sommer, Gerald},
issn = {1448-5869},
journal = {International Journal of Hybrid Intelligent Systems},
number = {3},
pages = {171--183},
publisher = {IOS Press},
title = {{Evolutionary reinforcement learning of artificial neural networks}},
volume = {4},
year = {2007}
}
@article{Moridis2016,
author = {Moridis, George J.},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Moridis{\_}2016.pdf:pdf},
keywords = {meshmaker,tough2},
mendeley-tags = {meshmaker,tough2},
number = {February},
title = {{User's manual of the MeshMaker v1.5 code}},
year = {2016}
}
@article{Press1970a,
abstract = {A suite of the most recently available geophysical data are inverted by an improved Monte Carlo procedure. The data are derived from surface waves for oceanic paths, eigenvibrations of the earth, elastic wave travel time and dt/d$\Delta$ data, mass and moment of inertia of the earth. A low velocity zone is required for the suboceanic mantle as is a high density lithosphere. The high density is related to eclogite fractionation from the underlying, partially molten asthenosphere in a process involving the creation and spreading of the lithosphere. If the asthenosphere is pyrolite or peridotite then an increase of mean atomic weigh across the transition zone seems required. Fairborn's new dt/d$\Delta$ data for the lower mantle seem to show a higher shear velocity gradient than previously supposed. If correct, a compensatory lower density gradient is required. This may indicate a depletion of iron with depth in the lower mantle. The density at the top of the core is surprisingly well constrained to the range 9.9-10.2 g/cm3 a value appropriate for a mixture of iron and about 15 wt{\%} silicon. {\textcopyright} 1970.},
author = {Press, Frank},
doi = {10.1016/0031-9201(70)90039-7},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Press{\_}1970.pdf:pdf},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {mcmc},
mendeley-tags = {mcmc},
month = {jan},
number = {C},
pages = {3--22},
title = {{Earth models consistent with geophysical data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0031920170900397},
volume = {3},
year = {1970}
}
@article{Dai1997,
abstract = {An automatic approach is developed to pick P and {\$} arrivals from single component (l-C) recordings of local earthquake data. In this approach a back propagation neural network (BPNN) accepts a normalized segment (window of 40 samples) of absolute amplitudes from the 1-C recordings as its input pattern, calculating two output values between 0 and 1. The outputs (0,1) or (1,0) correspond to the presence of an arrival or background noise within a moving window. The two outputs form a time series. The P and {\$} arrivals are then retrieved from this series by using a threshold and a local maximum rule. The BPNN is trained by only 10 pairs of P arrivals and background noise segments from the vertical component (V-C) recordings. It can also successfully pick seismic arrivals from the horizontal components (E-W and N-S). Its performance is different for each of the three components due to strong effects of ray path and source position on the seismic waveforms. For the data from two stations of TDP3 seismic network, the success rates are 93{\%}, 89{\%}, and 83{\%} for P arrivals and 75{\%}, 91{\%}, and 87{\%} for {\$} arrivals from the V-C, E-W, and N-S recordings, respectively. The accuracy of the onset times picked fxom each individual 1-C recording is similar. Adding a constraint on the error to be 10 ms (one sample increment), 66{\%}, 59{\%} and 63{\%} of the P arrivals and 53{\%}, 61{\%}, and 58{\%} of the {\$} arrivals are picked from the V-C, E-W and N-S recordings respectively. Its performance is lower than a similar three-component picking approach but higher than other 1-C picking methods.},
author = {Dai, Hengchang and MacBeth, Colin},
doi = {10.1029/97JB00625},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Dai, MacBeth{\_}1997.pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
month = {jul},
number = {B7},
pages = {15105--15113},
title = {{The application of back-propagation neural network to automatic picking seismic arrivals from single-component recordings}},
url = {http://doi.wiley.com/10.1029/97JB00625},
volume = {102},
year = {1997}
}
@article{Haskell1953,
author = {Haskell, Norman A},
issn = {1943-3573},
journal = {Bulletin of the seismological Society of America},
number = {1},
pages = {17--34},
publisher = {The Seismological Society of America},
title = {{The dispersion of surface waves on multilayered media}},
volume = {43},
year = {1953}
}
@article{White1989,
author = {White, D. J.},
doi = {10.1111/j.1365-246X.1989.tb00498.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/White{\_}1989.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {inversion,seismic refraction,tomography,traveltimes},
month = {may},
number = {2},
pages = {223--245},
title = {{Two-Dimensional Seismic Refraction Tomography}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1989.tb00498.x},
volume = {97},
year = {1989}
}
@article{Duane1987,
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
author = {Duane, Simon and Kennedy, A.D. and Pendleton, Brian J. and Roweth, Duncan},
doi = {10.1016/0370-2693(87)91197-X},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physics Letters B/Duane et al.{\_}1987.pdf:pdf},
isbn = {0370-2693},
issn = {03702693},
journal = {Physics Letters B},
month = {sep},
number = {2},
pages = {216--222},
title = {{Hybrid Monte Carlo}},
url = {http://www.sciencedirect.com/science/article/pii/037026938791197X http://linkinghub.elsevier.com/retrieve/pii/037026938791197X},
volume = {195},
year = {1987}
}
@article{Zhang2005,
abstract = {The constriction factor method (CFM) is a new variation of the basic particle swarm optimization (PSO), which has relatively better convergent nature. The effects of the major parameters on CFM were systematically investigated based on some benchmark functions. The constriction factor, velocity constraint, and population size all have significant impact on the performance of CFM for PSO. The constriction factor and velocity constraint have optimal values in practical application, and improper choice of these factors will lead to bad results. Increasing population size can improve the solution quality, although the computing time will be longer. The characteristics of CFM parameters are described and guidelines for determining parameter values are given in this paper.},
author = {Zhang, Li-ping and Yu, Huan-jun and Hu, Shang-xu},
doi = {10.1631/jzus.2005.A0528},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Zhejiang University SCIENCE/Zhang, Yu, Hu{\_}2005.pdf:pdf},
issn = {1009-3095},
journal = {Journal of Zhejiang University SCIENCE},
keywords = {Constriction factor method (CFM),Parameter selection,Particle swarm optimization (PSO),pso},
mendeley-tags = {pso},
number = {6},
pages = {528--534},
title = {{Optimal choice of parameters for particle swarm optimization}},
volume = {6A},
year = {2005}
}
@article{Pan2016,
abstract = {This paper focuses on the progress in geomechanical modeling associated with carbon dioxide (CO2) geological storage. The detailed review of some geomechanical aspects, including numerical methods, stress analysis, ground deformation, fault reactivation, induced seismicity and crack propagation, is presented. It is indicated that although all the processes involved are not fully understood, integration of all available data, such as ground survey, geological conditions, microseismicity and ground level deformation, has led to many new insights into the rock mechanical response to CO2 injection. The review also shows that in geomechanical modeling, continuum modeling methods are predominant compared with discontinuum methods. It is recommended to develop continuum–discontinuum numerical methods since they are more convenient for geomechanical modeling of CO2 geological storage, especially for fracture propagation simulation. The Mohr–Coulomb criterion is widely used in prediction of rock mass mechanical behavior. It would be better to use a criterion considering the effect of the intermediate principal stress on rock mechanical behavior, especially for the stability analysis of deeply seated rock engineering. Some challenges related to geomechanical modeling of CO2 geological storage are also discussed.},
author = {Pan, Pengzhi and Wu, Zhenhua and Feng, Xiating and Yan, Fei},
doi = {10.1016/j.jrmge.2016.10.002},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Rock Mechanics and Geotechnical Engineering/Pan et al.{\_}2016.pdf:pdf},
issn = {16747755},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
keywords = {Carbon dioxide (CO2) geological storage,Continuum numerical method,Continuum–discontinuum numerical method,Fault reactivation,Fault representation,Fracture propagation,Geomechanical modeling,Induced seismicity,tough2},
mendeley-tags = {tough2},
month = {dec},
number = {6},
pages = {936--947},
publisher = {Elsevier Ltd},
title = {{Geomechanical modeling of CO2 geological storage: A review}},
url = {http://dx.doi.org/10.1016/j.jrmge.2016.10.002 https://linkinghub.elsevier.com/retrieve/pii/S1674775516300580},
volume = {8},
year = {2016}
}
@article{Hingee2011,
abstract = {There is significant seismic activity in the region around Australia, largely due to the plate boundaries to the north and to the east of the mainland. This activity results in serious seismic and tsunami hazard in the coastal areas of Australia. Hence seismicity is and will be monitored in real time by Geoscience Australia (GA), which uses a network of permanent broadband seismometers. Seismic moment tensor (MT) solutions are currently determined using 1-D, radially symmetric models of Earth and this requires augmentation by recording stations located outside of Australia. A 3-D model of the Australian continent developed recently using full waveform tomography now offers the opportunity to significantly improve the determination of MT solutions of earthquakes from tectonically active regions. A complete-waveform, time-domain MT inversion method has been developed using a point-source approximation. A series of synthetic tests using first a 1-D and then a 3-D structural model has been performed. The feasibility of deploying 3-D versus 1-D Earth structure for the inversion of seismic data has been studied and the advantages of using the 3-D structural model were illustrated with examples. The 3-D model is superior to the 1-D model, as a number of sensitivity tests show. The ultimate goal of this work is an automated MT inversion system in Australia relying on GA and other international stations, although more work remains to be done before the full implementation of such a scheme in real time.},
author = {Hingee, Myall and Tkal{\v{c}}i{\'{c}}, Hrvoje and Fichtner, Andreas and Sambridge, Malcolm},
doi = {10.1111/j.1365-246X.2010.04897.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Hingee et al.{\_}2011.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Early warning,Earthquake source observations,Wave propagation},
month = {feb},
number = {2},
pages = {949--964},
title = {{Seismic moment tensor inversion using a 3-D structural model: applications for the Australian region}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2010.04897.x},
volume = {184},
year = {2011}
}
@book{Deb2001,
author = {Deb, Kalyanmoy},
isbn = {047187339X},
publisher = {John Wiley {\&} Sons},
title = {{Multi-objective optimization using evolutionary algorithms}},
volume = {16},
year = {2001}
}
@article{Venter2006,
abstract = {A parallel Particle Swarm Optimization (PSO) algorithm is presented.$\backslash$nParticle swarm optimization$\backslash$n$\backslash$nis a fairly recent addition to the family of non-gradient based, probabilistic$\backslash$nsearch algorithms that is$\backslash$n$\backslash$nbased on a simpliﬁed social model and is closely tied to swarming$\backslash$ntheory. Although PSO algorithms$\backslash$n$\backslash$npresent several attractive properties to the designer, they are plagued$\backslash$nby high computational cost as$\backslash$n$\backslash$nmeasured by elapsed time. One approach to reduce the elapsed time$\backslash$nis to make use of coarse-grained$\backslash$n$\backslash$nparallelization to evaluate the design points. Previous parallel PSO$\backslash$nalgorithms were mostly implemented$\backslash$n$\backslash$nin a synchronous manner, where all design points within a design iteration$\backslash$nare evaluated before the next$\backslash$n$\backslash$niteration is started. This approach leads to poor parallel speedup$\backslash$nin cases where a heterogeneous parallel$\backslash$n$\backslash$nenvironment is used and/or where the analysis time depends on the$\backslash$ndesign point being analyzed. This$\backslash$n$\backslash$npaper introduces an asynchronous parallel PSO algorithm that greatly$\backslash$nimproves the parallel eﬃciency.$\backslash$n$\backslash$nThe asynchronous algorithm is benchmarked on a cluster assembled of$\backslash$nApple Macintosh G5 desktop$\backslash$n$\backslash$ncomputers, using the multi-disciplinary optimization of a typical$\backslash$ntransport aircraft wing as an example.},
author = {Venter, Gerhard and Sobieszczanski-Sobieski, Jaroslaw},
doi = {10.2514/1.17873},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Aerospace Computing, Information, and Communication/Venter, Sobieszczanski-Sobieski{\_}2006.pdf:pdf},
isbn = {1542-9423$\backslash$r1542-9423},
issn = {1542-9423},
journal = {Journal of Aerospace Computing, Information, and Communication},
keywords = {hpc,parallel,pso},
mendeley-tags = {hpc,parallel,pso},
number = {3},
pages = {123--137},
title = {{Parallel Particle Swarm Optimization Algorithm Accelerated by Asynchronous Evaluations}},
volume = {3},
year = {2006}
}
@article{Bavelas1950,
author = {Bavelas, Alex},
doi = {10.1121/1.1906679},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {6},
pages = {725--730},
title = {{Communication Patterns in Task‐Oriented Groups}},
url = {http://asa.scitation.org/doi/10.1121/1.1906679},
volume = {22},
year = {1950}
}
@inproceedings{Akram2017,
author = {Akram, Jubran and Ovcharenko, Oleg and Peter, Daniel},
booktitle = {SEG Technical Program Expanded Abstracts 2017},
doi = {10.1190/segam2017-17761195.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2017/Akram, Ovcharenko, Peter{\_}2017.pdf:pdf},
month = {aug},
pages = {2929--2933},
publisher = {Society of Exploration Geophysicists},
title = {{A robust neural network-based approach for microseismic event detection}},
url = {http://library.seg.org/doi/10.1190/segam2017-17761195.1},
year = {2017}
}
@article{Eyre2015,
abstract = {Understanding the source mechanisms of microseismic events is important for understanding the fracturing behavior and evolving stress field within a reservoir, knowledge of which can help to improve production and minimize seismic risk. The most common method for calculating the source mechanisms is momenttensor inversion, which can provide the magnitudes, modes, and orientations of fractures. An overview of three common methods includes their advantages and limitations: the first-arrival polarity method, amplitude methods, and the full-waveform method. The first-arrival method is the quickest to implement but also the crudest, likely producing the least reliable results. Amplitude methods are also relatively simple but can better constrain the inversion because of the increased number of observations, especially those using S/P amplitude ratios. Full-waveform methods can provide results of very good quality, including source-time functions, but involve much more complex and expensive calculations and rely on accurate seismic-velocity models.},
author = {Eyre, Thomas S and Baan, Mirko Van Der},
doi = {10.1190/tle34080882.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The Leading Edge/Eyre, Baan{\_}2015.pdf:pdf},
isbn = {1070-485X$\backslash$r1938-3789},
issn = {1070-485X},
journal = {The Leading Edge},
keywords = {microseismic,moment tensor},
mendeley-tags = {microseismic,moment tensor},
number = {August},
pages = {882--888},
title = {{Overview of moment-tensor inversion of microseismic events}},
year = {2015}
}
@article{Xiong2018,
author = {Xiong, Jie and Liu, Caiyun and Chen, Yuantao and Zhang, Song},
issn = {1816-093X},
journal = {Engineering Letters},
number = {1},
title = {{A Non-linear Geophysical Inversion Algorithm for the MT Data Based on Improved Differential Evolution.}},
volume = {26},
year = {2018}
}
@article{Stanley2002,
author = {Stanley, Kenneth O and Miikkulainen, Risto},
issn = {1063-6560},
journal = {Evolutionary computation},
number = {2},
pages = {99--127},
publisher = {MIT Press},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Iwan2012,
abstract = {Optimization appears in many aspects of engineering problems. There are quite numbers of modern optimization algorithms proposed in the last two decades to solve optimization problems. Particle swarm optimization (PSO) and differential evolution (DE) are among the well-known modern optimization algorithms. This paper presents a comparative study for min-max constrained optimization using PSO and DE. Here, the constrained optimization is represented by some selected standard benchmark functions. A new constraint handling and stopping criterion technique is also adopted in the optimization algorithm. Generally, in terms of repeatability and the quality of the obtained solutions, DE outperforms PSO. {\textcopyright} 2012 The Authors.},
author = {Iwan, Mahmud and Akmeliawati, R. and Faisal, Tarig and Al-Assadi, Hayder M.A.A.},
doi = {10.1016/j.proeng.2012.07.317},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Procedia Engineering/Iwan et al.{\_}2012.pdf:pdf},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {Constrained optimization,Differential evolution,Particle swarm optimization,de,pso},
mendeley-tags = {de,pso},
number = {Iris},
pages = {1323--1328},
title = {{Performance Comparison of Differential Evolution and Particle Swarm Optimization in Constrained Optimization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877705812027178},
volume = {41},
year = {2012}
}
@article{Koza1994,
author = {Koza, John R},
issn = {0960-3174},
journal = {Statistics and computing},
number = {2},
pages = {87--112},
publisher = {Springer},
title = {{Genetic programming as a means for programming computers by natural selection}},
volume = {4},
year = {1994}
}
@article{Rutqvist2017,
abstract = {After the initial development of the first TOUGH-based geomechanics model 15 years ago based on linking TOUGH2 multiphase flow simulator to the FLAC3D geomechanics simulator, at least 15 additional TOUGH-based geomechanics models have appeared in the literature. This development has been fueled by a growing demand and interest for modeling coupled multiphase flow and geomechanical processes related to a number of geoengineering applications, such as in geologic CO2 sequestration, enhanced geothermal systems, unconventional hydrocarbon production, and most recently, related to reservoir stimulation and injection-induced seismicity. This paper provides a brief overview of these TOUGH-based geomechanics models, focusing on some of the most frequently applied to a diverse set of problems associated with geomechanics and its couplings to hydraulic, thermal and chemical processes.},
author = {Rutqvist, Jonny},
doi = {10.1016/j.cageo.2016.09.007},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers and Geosciences/Rutqvist{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Fluid flow,Geomechanics,Modeling,THMC,TOUGH,flac,tough2},
mendeley-tags = {flac,tough2},
number = {September 2016},
pages = {56--63},
publisher = {Elsevier Ltd},
title = {{An overview of TOUGH-based geomechanics models}},
url = {http://dx.doi.org/10.1016/j.cageo.2016.09.007},
volume = {108},
year = {2017}
}
@article{Rutqvist2015,
abstract = {In this paper, we summarize the results of coupled thermal, hydraulic, and mechanical (THM) modeling in support of the Northwest Geysers EGS Demonstration Project, which aims at enhancing production from a known High Temperature Reservoir (HTR) (280–400 °C) located under the conventional (240 °C) geothermal steam reservoir. The THM modeling was conducted to investigate geomechanical effects of cold-water injection during the stimulation of the EGS, first to predict the extent of the stimulation zone for a given injection schedule, and then to conduct interpretive analyses of the actual stimulation. By using a calibrated THM model based on historic injection and microseismic data at a nearby well, we could reasonably predict the extent of the stimulation zone around the injection well, at least for the first few months of injection. However, observed microseismic evolution and pressure responses over the one-year stimulation-injection revealed more heterogeneous behavior as a result of more complex geology, including a network of shear zones. Therefore, for an interpretive analysis of the one-year stimulation campaign, we included two sets of vertical shear zones within the model; a set of more permeable NW-striking shear zones and a set of less permeable NE-striking shear zones. Our modeling indicates that the microseismic events in this system are related to shear reactivation of pre-existing fractures, triggered by the combined effects of injection-induced cooling around the injection well and rapid (but small) changes in steam pressure as far as a kilometer from the injection well. Overall, the integrated monitoring and modeling of microseismicity, ground surface deformations, reservoir pressure, fluid chemical composition, and seismic tomography depict an EGS system hydraulically bounded by some of the NE-striking low permeability shear zones, with the more permeable NW-striking shear zone providing liquid flow paths for stimulation deep (several kilometers) down into the HTR. The modeling indicates that a significant mechanical degradation (damage) inferred from seismic tomography, and potential changes in fracture porosity inferred from cross-well pressure responses, are related to shear rupture in the stimulation zone driven by both pressure and cooling effects.},
author = {Rutqvist, Jonny and Jeanne, Pierre and Dobson, Patrick F. and Garcia, Julio and Hartline, Craig and Hutchings, Lawrence and Singh, Ankit and Vasco, Donald W. and Walters, Mark},
doi = {10.1016/j.geothermics.2015.08.002},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geothermics/Rutqvist et al.{\_}2016(2).pdf:pdf},
issn = {03756505},
journal = {Geothermics},
keywords = {Coupled THM Modeling,EGS,Ground surface deformations,Seismic tomography,Seismicity,Stimulation,The Geysers,flac,tough2},
mendeley-tags = {flac,tough2},
month = {sep},
pages = {120--138},
publisher = {CNR-Istituto di Geoscienze e Georisorse},
title = {{The Northwest Geysers EGS Demonstration Project, California – Part 2: Modeling and interpretation}},
url = {http://dx.doi.org/10.1016/j.geothermics.2015.08.002 https://linkinghub.elsevier.com/retrieve/pii/S0375650515001030},
volume = {63},
year = {2016}
}
@article{Maraschini2010,
abstract = {The analysis of surface wave propagation is often used to estimate the S-wave velocity profile at a site. In this paper, we propose a stochastic approach for the inversion of surface waves, which allows apparent dispersion curves to be inverted. The inversion method is based on the integrated use of two-misfit functions. A misfit function based on the determinant of the Haskell-Thomson matrix and a classical Euclidean distance between the dispersion curves. The former allows all the modes of the dispersion curve to be taken into account with a very limited computational cost because it avoids the explicit calculation of the dispersion curve for each tentative model. It is used in a Monte Carlo inversion with a large population of profiles. In a subsequent step, the selection of representative models is obtained by applying a Fisher test based on the Euclidean distance between the experimental and the synthetic dispersion curves to the best models of the Monte Carlo inversion. This procedure allows the set of the selected models to be identified on the basis of the data quality. It also mitigates the influence of local minima that can affect the Monte Carlo results. The effectiveness of the procedure is shown for synthetic and real experimental data sets, where the advantages of the two-stage procedure are highlighted. In particular, the determinant misfit allows the computation of large populations in stochastic algorithms with a limited computational cost. {\textcopyright} 2010 The Authors Journal compilation {\textcopyright} 2010 RAS.},
author = {Maraschini, Margherita and Foti, Sebastiano},
doi = {10.1111/j.1365-246X.2010.04703.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Maraschini, Foti{\_}2010.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Elasticity and anelasticity,Inverse theory,Probability distributions,Site effects,Surface waves and free oscillations,Wave propagation},
number = {3},
pages = {1557--1566},
title = {{A Monte Carlo multimodal inversion of surface waves}},
volume = {182},
year = {2010}
}
@incollection{Arabas2010,
abstract = {In this paper we show that the technique of handling bound-ary constraints has a significant influence on the efficiency of the Differential Evolution method. We study the effects of applying several such techniques taken from the literature. The comparison is based on ex-periments performed for a standard DE/rand/1/bin strategy using the CEC2005 benchmark. The paper reports the results of experiments and provides their simple statistical analysis. Among several constraint han-dling methods, a winning approach is to repeat the differential mutation by resampling the population until a feasible mutant is obtained. Coupling the aforementioned method with a simple DE/rand/1/bin strategy allows to achieve results that outperform in many cases results of almost all other methods tested during the CEC2005 competition, including the original DE/rand/1/bin strategy.},
address = {Berlin, Heidelberg},
author = {Arabas, Jaros{\l}law and Szczepankiewicz, Adam and Wroniak, Tomasz},
booktitle = {Parallel Problem Solving from Nature, PPSN XI},
doi = {10.1007/978-3-642-15871-1_42},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Parallel Problem Solving from Nature, PPSN XI/Arabas, Szczepankiewicz, Wroniak{\_}2010.pdf:pdf},
isbn = {3642158706},
issn = {03029743},
keywords = {de},
mendeley-tags = {de},
number = {PART 2},
pages = {411--420},
publisher = {Springer Berlin Heidelberg},
title = {{Experimental Comparison of Methods to Handle Boundary Constraints in Differential Evolution}},
url = {http://link.springer.com/10.1007/978-3-642-15871-1{\_}42},
volume = {6239 LNCS},
year = {2010}
}
@article{Malinverno2000,
author = {Malinverno, Alberto and Torres-Verd{\'{i}}n, Carlos},
doi = {10.1088/0266-5611/16/5/313},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Inverse Problems/Malinverno, Torres-Verd{\'{i}}n{\_}2000.pdf:pdf},
issn = {0266-5611},
journal = {Inverse Problems},
keywords = {mcmc},
mendeley-tags = {mcmc},
month = {oct},
number = {5},
pages = {1343--1356},
title = {{Bayesian inversion of DC electrical measurements with uncertainties for reservoir monitoring}},
url = {http://stacks.iop.org/0266-5611/16/i=5/a=313?key=crossref.8ca89871a20dcef8fde3ba9e7bea633f},
volume = {16},
year = {2000}
}
@article{Whitley1999,
abstract = {Parallel Genetic Algorithms have often been reported to yield better performance than Genetic Algorithms which use a single large panmictic population. In the case of the Island Model genetic algorithm, it has been informally argued that having multiple subpopulations helps to preserve genetic diversity, since each island can potentially follow a different search trajectory through the search space. It is also possible that since linearly separable problems are often used to test Genetic Algorithms, that Island Models may simply be particularly well suited to exploiting the separable nature of the test problems. We explore this possibility by using the infinite population models of simple genetic algorithms to study how Island Models can track multiple search trajectories. We also introduce a simple model for better understanding when Island Model genetic algorithms may have an advantage when processing some test problems. We provide empirical results for both linearly separa...},
author = {Whitley, Darrell and Rana, Soraya and Heckendorn, Robert B},
doi = {10.1.1.36.7225},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Computing and Information Technology/Whitley, Rana, Heckendorn{\_}1999.pdf:pdf},
issn = {1330-1136},
journal = {Journal of Computing and Information Technology},
keywords = {hpc},
mendeley-tags = {hpc},
pages = {33--47},
title = {{The island model genetic algorithm: On separability, population size and convergence}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7225{\&}rep=rep1{\&}type=pdf},
volume = {7},
year = {1999}
}
@book{Sandham2003,
address = {Dordrecht},
doi = {10.1007/978-94-017-0271-3},
editor = {Sandham, William A. and Leggett, Miles},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Unknown{\_}2003.pdf:pdf},
isbn = {978-90-481-6476-9},
publisher = {Springer Netherlands},
series = {Modern Approaches in Geophysics},
title = {{Geophysical Applications of Artificial Neural Networks and Fuzzy Logic}},
url = {http://link.springer.com/10.1007/978-94-017-0271-3},
volume = {21},
year = {2003}
}
@article{Buchen1996,
abstract = {The theory of Love- and Rayleigh-wave dispersion for plane-layered earth models has undergone a number of developments since the initial work of Thomson and Haskell. Most of these were concerned with computational difficulties associated with numerical overflow and loss of precision at high frequencies in the original Thomson-Haskell formalism. Several seemingly distinct approaches have been followed, including the delta matrix, reduced delta matrix, Schwab-Knopoff, fast Schwab-Knopoff, Kennett's Reflection-Transmission Matrix and Abo-Zena methods. This paper analyses all these methods in detail and finds explicit transformations connecting them. It is shown that they are essentially equivalent and, contrary to some claims made, each solves the loss of precision problem equally well. This is demonstrated both theoretically and computationally. By extracting the best computational features of the various methods, we develop a new algorithm (sec Appendix A5), called the fast delta matrix algorithm. To date, this is the simplest and most efficient algorithm for surface-wave dispersion computations (see Fig. 4). The theory given in this paper provides a complete review of the principal methods developed for Love- and Rayleigh-wave dispersion of free modes in plane-layered perfectly elastic, isotropic earth models and puts to rest controversies that have arisen with regard to computational stability.},
author = {Buchen, P W and Ben-Hador, R.},
doi = {10.1111/j.1365-246X.1996.tb05642.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Buchen, Ben-Hador{\_}1996.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {elastic-wave theory,guided waves,love waves,rayleigh waves,surface,surface wave,thomson haskell},
mendeley-tags = {surface wave,thomson haskell},
month = {mar},
number = {3},
pages = {869--887},
title = {{Free-mode surface-wave computations}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1996.tb05642.x},
volume = {124},
year = {1996}
}
@article{Gesret2015,
abstract = {Earthquake hypocentre locations are crucial in many domains of application (academic and industrial) as seismic event location maps are commonly used to delineate faults or fractures. The interpretation of these maps depends on location accuracy and on the reliability of the associated uncertainties. The largest contribution to location and uncertainty errors is due to the fact that the velocity model errors are usually not correctly taken into account. We propose a new Bayesian formulation that integrates properly the knowledge on the velocity model into the formulation of the probabilistic earthquake location. In this work, the velocity model uncertainties are first estimated with a Bayesian tomography of active shot data. We implement a sampling Monte Carlo type algorithm to generate velocity models distributed according to the posterior distribution. In a second step, we propagate the velocity model uncertainties to the seismic event location in a probabilistic framework. This enables to obtain more reliable hypocentre locations as well as their associated uncertainties accounting for picking and velocity model uncertainties. We illustrate the tomography results and the gain in accuracy of earthquake location for two synthetic examples and one real data case study in the context of induced microseismicity. 1 I N T RO D U C T I O N Earthquake hypocentre locations are central in seismogenic and tectonic interpretation. Indeed visual inspection of earthquake lo-cation maps is commonly used to delineate faults or fractures and mislocation can lead to an incorrect model of the Earth structure. For example, diffuse patterns that are sometimes observed can be due to unmapped or hidden faults but can also arise due to lo-cation inaccuracy. A precise location is particularly important in the microseismic context where the seismicity induced by mine or reservoir exploitation is often the only available tool to follow the fracturation. In addition, the spatial distribution of the uncertainties associated to the hypocentre location should also be estimated to interpret the results quantitatively. It is thus of primary importance to have accurate seismic event locations and their true associated uncertainties to obtain reliable interpretations. The location of earthquake hypocentre is a typical inverse prob-lem that can be solved with iterative linearized inversion approaches such as implemented in widely used softwares (e.g. Lee {\&} Lahr 1975; Lahr 1989, 2002). These algorithms aim to retrieve the max-imum likelihood solution that corresponds to the minimum of the misfit function between observed and computed P and S traveltimes. The major drawback of such local optimization methods is to give only a unique solution that can be inaccurate. Indeed as this prob-lem is non-linear, its solution can be multimodal and the algorithm can be stuck in a local minimum that does not correspond to the true hypocentre location. The shape of the misfit surface will be especially irregular for a poor network coverage and for a hetero-geneous medium (with velocity gradients and/or interfaces). Very often the uncertainties are estimated at the final hypocentre location under the Gaussian assumption, for example by scaling the partial derivatives of traveltime with respect to the hypocentre coordinates by the time residuals (Flinn 1965) or by prior estimates for picking and traveltime errors (Evernden 1969; Jordan {\&} Sverdrup 1981). These approaches lead to confidence regions of elliptical shape that can be erroneous due to the strong assumption that the misfit func-tion has a single optimum and a Gaussian shape. Since the work of Tarantola {\&} Valette (1982), the probabilistic Bayesian formulation is often preferred (e.g. Moser et al. 1992; Wittlinger et al. 1993; Lo-max et al. 2001; Husen et al. 2003) as it allows to retrieve the global minimum of the misfit function even for a non-linear/multimodal solution. With this Bayesian approach, the final solution is not a single point anymore but the complete posterior probability density function (pdf) of the event location. This formulation allows for},
author = {Gesret, A. and Desassis, N. and Noble, M. and Romary, T. and Maisons, C.},
doi = {10.1093/gji/ggu374},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Gesret et al.{\_}2015.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Probability distribution,Seismic tomography,Theoretical seismology,location,mcmc,microseismic,sa},
mendeley-tags = {location,mcmc,microseismic,sa},
number = {1},
pages = {52--66},
title = {{Propagation of the velocity model uncertainties to the seismic event location}},
volume = {200},
year = {2015}
}
@article{Leetaru2014,
abstract = {The Illinois Basin - Decatur Project (IBDP) is a large-scale carbon capture and storage (CCS) demonstration project managed by the Midwest Geologic Sequestration Consortium (MGSC). IBDP is injecting 1 million tonnes of carbon dioxide (CO2) in the Cambrian Mt. Simon Sandstone over three years at a rate of 1,000 tonnes per day. At the IBDP site the top of the Mt. Simon Sandstone is overlain by 100 m (300 ft) of tight silt and shale in the Eau Claire Formation that forms the primary seal that prevents possible migration of CO2into the overlying strata. Below the Mt. Simon Sandstone is a pre-Mt. Simon interval that is characterized by its poor reservoir quality. In the United States, the pore space storage rights are typically owned by the surface landowner. Therefore, it is imperative that CCS projects such as IBDP be able to estimate the size and movement of the CO2plume within the reservoir. An integrated earth model and subsequent reservoir flow simulations are the first steps to CO2plume management. An inversion of the 3D surface reflection seismic data was used to create a template for the distribution of the reservoir properties acquired from the well data. Inversion of the 3D seismic data produced two inversion volumes, an acoustic impedance (AI) cube, and a porosity cube. Wireline porosity logs were upscaled to the 3D model and then co-simulated with the seismic inversion porosity. Log permeability was upscaled to the simulation model and co-simulated with porosity. The resulting porosity and permeability fields were rescaled into the simulation grid which was discretized to achieve the required simulation accuracy while maintaining computational efficiency. The reservoir flow simulation based on the integrated earth model suggests that the CO2will follow the high permeability intervals and there will be very limited vertical migration of the CO2. These flow simulations also will help with plume management in a new planned injection well near the IBDP project site which will be perforated at higher intervals in the Mt. Simon Sandstone than those in the original injection well.},
author = {Leetaru, Hannes E. and Smith, Valerie and Will, Robert and Freiburg, Jared and Brown, Alan L.},
doi = {10.1016/j.egypro.2014.11.313},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Leetaru et al.{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Illinois,Mt. simon sandstone,Reservoir managment,decatur},
mendeley-tags = {decatur},
pages = {2903--2910},
publisher = {Elsevier B.V.},
title = {{The Application of an Integrated Earth Model in Reservoir Management of a CO2 Plume}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.313 https://linkinghub.elsevier.com/retrieve/pii/S1876610214021286},
volume = {63},
year = {2014}
}
@article{Kawakatsu2008,
abstract = {The time reversal operation in seismic source estimation is considered. We show that the time reversal operation, equally the adjoint operation, for seismic source imaging gives an approximate solution tomore conventional seismic source inverse problem through the ‘happy approximation' underlined by Claerbout. Practical applications of such methods in a long- period range to monitor earth's activities in realtime are also discussed},
author = {Kawakatsu, Hitoshi and Montagner, Jean Paul},
doi = {10.1111/j.1365-246X.2008.03926.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Kawakatsu, Montagner{\_}2008.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Theoretical seismology,moment tensor},
mendeley-tags = {moment tensor},
number = {2},
pages = {686--688},
title = {{Time-reversal seismic-source imaging and moment-tensor inversion}},
volume = {175},
year = {2008}
}
@article{Jordan1981,
abstract = {Improved methods for single-and multiple-event hypocenter determinations are developed and applied to the problem of locating earthquake clusters in the South-Central Pacific Ocean. Bayesian statistical methods are used to incorporate a priori information about arrival-time variance into the derivation of hypocenter confidence ellipsoids, permitting a more realistic calculation of critical parameters in the case where the number of stations is small. The diagonal elements of certain projection operators, called “data importances” by Minster et al. (1974), are used to evaluate network balance. The hypocentroid of an event cluster is defined to be the average location of events within the cluster, and the deviations of individual hypocenters from the hypocentroid are called cluster vectors. The problem of estimating the cluster vectors can be decoupled from the problem of estimating the hypocentroid by a simple but fundamental mathematical result, here termed the hypocentroidal decomposition theorem. The algorithm based on this analysis appears to have many advantages over other published methods for multiple-event location, both in its efficient use of available information and its computational speed. The application of this method to three clusters of shallow intraplate seismicity in the South-Central Pacific, designated Regions A, B, and C, demonstrates that the seismicity within each cluster is very localized; the rms lengths of the cluster vectors for each group of epicenters are estimated to be only 9, 6, and 12 km, respectively. Estimates of the epicentroids are},
author = {Jordan, Thomas H and Sverdrup, Keith A},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Jordan, Sverdrup{\_}1981.pdf:pdf},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
keywords = {location},
mendeley-tags = {location},
number = {4},
pages = {1105--1130},
title = {{Teleseismic location techniques and their application to earthquake clusters in the South-Central Pacific}},
url = {http://www.bssaonline.org/content/71/4/1105.abstract},
volume = {71},
year = {1981}
}
@book{Foti2014,
abstract = {Summary: "Surface wave methods analysis the dispersive nature of surface wave propagation in heterogeneous media to measure shear wave velocity or material damping ratio profiles, and enable earthquake site response to be assessed. This is the only comprehensive reference that provides a unified treatment of surface wave propagation, signal processing, inverse theory and the testing protocols that form the basis of modern surface wave methods. The use of these tests has increased dramatically since the 1980s, but they are too often performed and interpreted in a variety of ways that are confusing. This book answers the pressing need for a guide to the basic principles as well as outlining a set of reliable, dependable and accepted practices. It is written for geotechnical engineers, engineering seismologists and geophysicists as well as academics in these fields"-- Provided by publisher. 1. Overview of surface wave methods -- 2. Linear wave propagation in vertically inhomogeneous continua -- 3. Measurement of surface waves -- 4. Dispersion analysis -- 5. Attenuation analysis -- 6. Inversion -- 7. Case histories -- 8. Advanced surface wave methods.},
author = {Foti, Sebastiano and Lai, Carlo and Rix, Glenn and Strobbia, Claudio},
doi = {10.1201/b17268},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Foti et al.{\_}2014(2).pdf:pdf;:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Foti et al.{\_}2014.pdf:pdf},
isbn = {9781482266825},
pages = {455},
title = {{Surface Wave Methods for Near-Surface Site Characterization}},
year = {2014}
}
@article{Koh2006,
abstract = {The high computational cost of complex engineering optimization problems has motivated the development of parallel optimization algorithms. A recent example is the parallel particle swarm optimization (PSO) algorithm, which is valuable due to its global search capabilities. Unfortunately, because existing parallel implementations are synchronous (PSPSO), they do not make efficient use of computational resources when a load imbalance exists. In this study, we introduce a parallel asynchronous PSO (PAPSO) algorithm to enhance computational efficiency. The performance of the PAPSO algorithm was compared to that of a PSPSO algorithm in homogeneous and heterogeneous computing environments for small- to medium-scale analytical test problems and a medium-scale biomechanical test problem. For all problems, the robustness and convergence rate of PAPSO were comparable to those of PSPSO. However, the parallel performance of PAPSO was significantly better than that of PSPSO for heterogeneous computing environments or heterogeneous computational tasks. For example, PAPSO was 3.5 times faster than was PSPSO for the biomechanical test problem executed on a heterogeneous cluster with 20 processors. Overall, PAPSO exhibits excellent parallel performance when a large number of processors (more than about 15) is utilized and either (1) heterogeneity exists in the computational task or environment, or (2) the computation-to-communication time ratio is relatively small.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Koh, Byung-Il and George, Alan D. and Haftka, Raphael T. and Fregly, Benjamin J.},
doi = {10.1002/nme.1646},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal for Numerical Methods in Engineering/Koh et al.{\_}2006.pdf:pdf},
isbn = {1-4244-0719-2},
issn = {0029-5981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {game theory,learning (artificial intelligence),par,pso},
mendeley-tags = {pso},
month = {jul},
number = {4},
pages = {578--595},
pmid = {17224972},
publisher = {IEEE},
title = {{Parallel asynchronous particle swarm optimization}},
url = {http://ieeexplore.ieee.org/document/4298806/ http://doi.wiley.com/10.1002/nme.1646},
volume = {67},
year = {2006}
}
@article{Jeanne2016,
abstract = {Carbon capture and storage (CCS) in geologic formations has been recognized as a promising option for reducing carbon dioxide (CO2) emissions from large stationary sources. However, the pressure buildup inside the storage formation can potentially induce slip along preexisting faults, which could lead to felt seismic ground motion and also provide pathways for brine/CO2 leakage into shallow drinking water aquifers. To assess the geomechanical stability of faults, it is of crucial importance to know the in situ state of stress. In situ stress measurements can provide some information on the stresses acting on faults but with considerable uncertainties. In this paper, we investigate how such uncertainties, as defined by the variation of stress measurements obtained within the study area, could influence the assessment of the geomechanical stability of faults and the characteristics of potential injection-induced seismic events. Our modeling study is based on a hypothetical industrial-scale carbon sequestration project assumed to be located in the Southern San Joaquin Basin in California, USA. We assess the stability on the major (25 km long) fault that bounds the sequestration site and is subjected to significant reservoir pressure changes as a result of 50 years of CO2 injection. We present a series of geomechanical simulations in which the resolved stresses on the fault were varied over ranges of values corresponding to various stress measurements performed around the study area. The simulation results are analyzed by a statistical approach. Our main results are that the variations in resolved stresses as defined by the range of stress measurements had a negligible effect on the prediction of the seismic risk (maximum magnitude), but an important effect on the timing, the seismicity rate (number of seismic events) and the location of seismic activity.},
author = {Jeanne, Pierre and Rutqvist, Jonny and Wainwright, Haruko M. and Foxall, William and Bachmann, Corinne and Zhou, Quanlin and Rinaldi, Antonio Pio and Birkholzer, Jens},
doi = {10.1016/j.jrmge.2016.06.008},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Rock Mechanics and Geotechnical Engineering/Jeanne et al.{\_}2016.pdf:pdf},
issn = {16747755},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
keywords = {Carbon dioxide (CO2) sequestration,Geomechanical simulations,Induced seismicity,Uncertainty on in situ stress,flac,tough2},
mendeley-tags = {flac,tough2},
month = {dec},
number = {6},
pages = {873--885},
publisher = {Elsevier Ltd},
title = {{Effects of in situ stress measurement uncertainties on assessment of predicted seismic activity and risk associated with a hypothetical industrial-scale geologic CO2 sequestration operation}},
url = {http://dx.doi.org/10.1016/j.jrmge.2016.06.008 https://linkinghub.elsevier.com/retrieve/pii/S1674775516301664},
volume = {8},
year = {2016}
}
@article{Bunks1995,
author = {Bunks, Carey and Saleck, Fatimetou M. and Zaleski, S. and Chavent, G.},
doi = {10.1190/1.1443880},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {1457--1473},
title = {{Multiscale seismic waveform inversion}},
url = {http://library.seg.org/doi/10.1190/1.1443880},
volume = {60},
year = {1995}
}
@article{Davies1979,
abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
author = {Davies, D L and Bouldin, D W},
doi = {10.1109/TPAMI.1979.4766909},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE transactions on pattern analysis and machine intelligence/Davies, Bouldin{\_}1979.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
number = {2},
pages = {224--227},
pmid = {21868852},
title = {{A cluster separation measure}},
volume = {1},
year = {1979}
}
@article{Green1995,
abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the param-eter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with appli-cations to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
author = {Green, Peter J.},
doi = {10.2307/2337340},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Biometrika/Green{\_}1995.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Change-point analysis,Image segmentation,Jump diffusion,Markov chain Monte Carlo,Multiple binomial experiments,Multiple shrinkage,Some key words,Step function,Voronoi tessellation,rjmcmc},
mendeley-tags = {rjmcmc},
month = {dec},
number = {4},
pages = {711},
title = {{Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination}},
url = {http://biomet.oxfordjournals.org/content/82/4/711.short http://www.jstor.org/stable/2337340?origin=crossref},
volume = {82},
year = {1995}
}
@inproceedings{Bolufe-Rohler2015,
author = {Bolufe-Rohler, Antonio and Fiol-Gonzalez, Sonia and Chen, Stephen},
booktitle = {2015 IEEE Congress on Evolutionary Computation (CEC)},
doi = {10.1109/CEC.2015.7257125},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2015 IEEE Congress on Evolutionary Computation (CEC)/Bolufe-Rohler, Fiol-Gonzalez, Chen{\_}2015.pdf:pdf},
isbn = {978-1-4799-7492-4},
keywords = {hybridization,large scale global optimization,minimum population search,multimodality},
month = {may},
pages = {1958--1965},
publisher = {IEEE},
title = {{A minimum population search hybrid for large scale global optimization}},
url = {http://ieeexplore.ieee.org/document/7257125/},
year = {2015}
}
@article{Rutqvist2011,
abstract = {This paper presents recent advancement in and applications of TOUGH-FLAC, a simulator for multiphase fluid flow and geomechanics. The TOUGH-FLAC simulator links the TOUGH family multiphase fluid and heat transport codes with the commercial FLAC3D geomechanical simulator. The most significant new TOUGH-FLAC development in the past few years is a revised architecture, enabling a more rigorous and tight coupling procedure with improved computational efficiency. The applications presented in this paper are related to modeling of crustal deformations caused by deep underground fluid movements and pressure changes as a result of both industrial activities (the In Salah CO2 Storage Project and the Geysers Geothermal Field) and natural events (the 1960s Matsushiro Earthquake Swarm). Finally, the paper provides some perspectives on the future of TOUGH-FLAC in light of its applicability to practical problems and the need for high-performance computing capabilities for field-scale problems, such as industrial-scale CO2 storage and enhanced geothermal systems. It is concluded that despite some limitations to fully adapting a commercial code such as FLAC3D for some specialized research and computational needs, TOUGH-FLAC is likely to remain a pragmatic simulation approach, with an increasing number of users in both academia and industry. {\textcopyright} 2010 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {1403.4687},
author = {Rutqvist, Jonny},
doi = {10.1016/j.cageo.2010.08.006},
eprint = {1403.4687},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Rutqvist{\_}2011.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {FLAC3D,Fluid flow,Geomechanics,Modeling,TOUGH,flac,tough2},
mendeley-tags = {flac,tough2},
month = {jun},
number = {6},
pages = {739--750},
pmid = {1975378},
publisher = {Elsevier},
title = {{Status of the TOUGH-FLAC simulator and recent applications related to coupled fluid flow and crustal deformations}},
url = {http://dx.doi.org/10.1016/j.cageo.2010.08.006 https://linkinghub.elsevier.com/retrieve/pii/S0098300410003080},
volume = {37},
year = {2011}
}
@book{Oliphant2006,
author = {Oliphant, Travis},
publisher = {USA: Trelgol Publishing},
title = {{A guide to NumPy}},
year = {2006}
}
@article{Gilmore2016,
abstract = {The objective of the FutureGen 2.0 Project was to demonstrate, at the commercial scale, the technical feasibility of implementing carbon capture and storage in a deep saline formation in Illinois, USA. Over approximately 5 years, the FutureGen Industrial Alliance, Inc. (Alliance) conducted a detailed site-selection process and identified a site for carbon sequestration storage in Morgan County, Illinois. A comprehensive geologic and hydrologic characterization of the storage site was conducted and included the collection of seismic data and the drilling and characterization of a stratigraphic borehole. The characterization data provided critical input for developing a site-specific conceptual model and subsequent numerical modeling simulations. The modeling simulations, coupled with upstream designs of the pipeline and power plant, supported the development of a detailed design that included the injection wells and associated control and monitoring infrastructure. The Alliance used all of these data to develop the required documentation to support the applications for four underground injection control (UIC) permits (one for each proposed well). In August 2014, the U.S. Environmental Protection Agency issued four, first-of-their-kind, Class VI UIC permits for carbon sequestration in the United States to the Alliance. This equated to one permit per planned injection well. The information and data generated under this project have been made publicly available through reports and publications, including this journal and others.},
author = {Gilmore, Tyler and Bonneville, Alain and Sullivan, Charlotte and Kelley, Mark and Appriou, Delphine and Vermeul, Vince and White, Signe and Zhang, Fred and Bjornstad, Bruce and Cornet, Francois and Gerst, Jacqueline and Gupta, Neeraj and Hund, Gretchen and Horner, Jake and Last, George and Lanigan, Dave and Oostrom, Mart and McNeil, Caitlin and Moody, Mark and Rockhold, Mark and Elliott, Mike and Spane, Frank and Strickland, Chris and Swartz, Lucy and Thorne, Paul and Brown, Christopher and Hoffmann, Jeffrey and Humphreys, Kenneth},
doi = {10.1016/j.ijggc.2016.07.022},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Gilmore et al.{\_}2016.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {CO2 reservoir,Carbon sequestration,Characterization,FutureGen,decatur},
mendeley-tags = {decatur},
month = {oct},
pages = {1--10},
publisher = {Elsevier Ltd},
title = {{Characterization and design of the FutureGen 2.0 carbon storage site}},
url = {http://dx.doi.org/10.1016/j.ijggc.2016.07.022 https://linkinghub.elsevier.com/retrieve/pii/S1750583616303851},
volume = {53},
year = {2016}
}
@article{TavakoliF.2017,
abstract = {Velocity macromodel building is a crucial step in the seismic imaging workflow as it provides the necessary background model for migration or full waveform inversion. In this study, we present a new formulation of stereotomography that can handle more efficiently long-offset acquisition, complex geological structures and large-scale data sets. Stereotomography is a slope tomographic method based upon a semi-automatic picking of local coherent events. Each local coherent event, characterized by its two-way traveltime and two slopes in common-shot and common-receiver gathers, is tied to a scatterer or a reflector segment in the subsurface. Ray tracing provides a natural forward engine to compute traveltime and slopes but can suffer from non-uniform ray sampling in presence of complex media and long-offset acquisitions. Moreover, most implementations of stereotomography explicitly build a sensitivity matrix, leading to the resolution of large systems of linear equations, which can be cumbersome when large-scale data sets are considered. Overcoming these issues comes with a new matrix-free formulation of stereotomography: a factored eikonal solver based on the fast sweeping method to compute first-arrival traveltimes and an adjoint-state formulation to compute the gradient of the misfit function. By solving eikonal equation from sources and receivers, we make the computational cost proportional to the number of sources and receivers while it is independent of picked events density in each shot and receiver gather. The model space involves the subsurface velocities and the scatterer coordinates, while the dips of the reflector segments are implicitly represented by the spatial support of the adjoint sources and are updated through the joint localization of nearby scatterers. We present an application on the complex Marmousi model for a towed-streamer acquisition and a realistic distribution of local events. We show that the estimated model, built without any prior knowledge of the velocities, provides a reliable initial model for frequency-domain FWI of long-offset data for a starting frequency of 4 Hz, although some artefacts at the reservoir level result from a deficit of illumination. This formulation of slope tomography provides a computationally efficient alternative to waveform inversion method such as reflection waveform inversion or differential-semblance optimization to build an initial model for pre-stack depth migration and conventional FWI. 1 I N T RO D U C T I O N Building a velocity macromodel from reflection data remains one of the most crucial and challenging issues in seismic imaging. The difficulty arises from the nonlinearity of the inverse problem associ-ated with the long-wavelength reconstruction of the subsurface; by contrast the imaging of reflectivity by migration is a far more linear problem (Claerbout 1985). Velocity model building provides the necessary background or starting model to perform pre-stack-depth migration (Etgen et al. 2009) or full waveform inversion (FWI; Tarantola 1984). Among the most popular methods for velocity macromodel build-ing, reflection traveltime tomography (Bishop et al. 1985; Farra {\&} Madariaga 1988) and migration-based velocity analysis (MVA; Gardner et al. 1974; Al-Yahya 1989) were specifically designed for seismic reflection data. Reflection traveltime tomography, which builds a velocity model through the minimization of the travel-time residuals, is computationally efficient but relies on tedious picking of continuous horizons. MVA methods rely on an explicit scale separation between the background velocities and the reflec-tivity to iteratively alternate the velocity update and the migration. The velocity update is driven by flattening the reflectors in the},
author = {{Tavakoli F.}, Bohran and Operto, St{\'{e}}phane and Ribodetti, Alessandra and Virieux, Jean},
doi = {10.1093/gji/ggx111},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Tavakoli F. et al.{\_}2017.pdf:pdf},
isbn = {9789462822177},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {inverse theory,numerical modelling,seismic,tomography},
mendeley-tags = {tomography},
month = {jun},
number = {3},
pages = {1629--1647},
title = {{Slope tomography based on eikonal solvers and the adjoint-state method}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggx111},
volume = {209},
year = {2017}
}
@article{Allen1982,
abstract = {Automatic phase-picking algorithms are designed to detect a seismic signal on a single trace and to time the arrival of the signal precisely. Because of the requirement for precise timing, a phase-picking algorithm is inherently less sensitive than one designed only to detect the presence of a signal, but still can approach the performance of a skilled analyst. A typical algorithm filters the input data and then generates a function characterizing the seismic time series. This function may be as simple as the absolute value of the series, or it may be quite complex. Event detection is accomplished by comparing the function or its short-term average (STA) with a threshold value (THR), which is commonly some multiple of a long-term average (LTA) of a characteristic function. If the STA exceeds THR, a trigger is declared. If the event passes simple criteria, it is reported. Sensitivity, expected timing error, false-trigger rate, and false-report rate are interrelated measures of performance controlled by choice of the characteristic function and several operating parameters. At present, computational power limits most systems to one-pass, time-domain algorithms. Rapidly advancing semi-conductor technology, however, will make possible much more powerful multi-pass approaches incorporating frequency-domain detection and pseudo-offline timing. },
author = {Allen, Rex},
journal = {Bulletin of the Seismological Society of America },
month = {dec},
number = {6B },
pages = {S225--S242},
title = {{Automatic phase pickers: Their present use and future prospects}},
url = {http://www.bssaonline.org/content/72/6B/S225.abstract},
volume = {72 },
year = {1982}
}
@article{Croucher2018,
author = {Croucher, Adrian},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Croucher{\_}2018.pdf:pdf},
keywords = {tough2},
mendeley-tags = {tough2},
number = {July},
title = {{PyTOUGH User's guide}},
year = {2018}
}
@article{Jeanne2017,
abstract = {Carbon capture and storage (CCS) in geological formations is considered as a promising option that could limit CO2 emissions from human activities into the atmosphere. However, there is a risk that pressure buildup inside the storage formation can induce slip along preexisting faults and create seismic event felt by the population. To prevent this to happen a geomechanical fault stability analysis should be performed, considering uncertainties of input parameters. In this paper, we investigate how the distribution of the coefficient of friction and the applied frictional law could influence the assessment of fault stability and the characteristics of potential injection-induced seismic events. Our modelling study is based on a hypothetical industrial-scale carbon sequestration project located in the Southern San Joaquin Basin in California, USA, where the stability on a major (25 km long) fault that bounds the sequestration site is assessed during 50 years of CO2 injection. We conduct nine simulations in which the distributions of the coefficients of static and dynamic friction are changed to simulate a hardening and softening phase before and during rupture. Our main findings are: (i) variations in friction along the fault have an important effect on the predicted seismic activity, with maximum magnitude ranging from 1.88 to 5.88 and number of seismic events ranging from 338 to 3272; (ii) the extreme values of the coefficient of friction (lowest and highest) present along the rupture area control how much stress is accumulated before rupture; and (iii) an argillaceous caprock can prevent the development of large magnitude seismic events but favor the occurrence of a large number of smaller events.},
author = {Jeanne, Pierre and Rutqvist, Jonny and Foxall, William and Rinaldi, Antonio Pio and Wainwright, Haruko M. and Zhou, Quanlin and Birkholzer, Jens and Layland-Bachmann, Corinne},
doi = {10.1016/j.ijggc.2017.09.018},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Jeanne et al.{\_}2017.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {CO2 sequestration,Coefficient of friction,Geomechanical simulations,Induced seismicity,flac,tough2},
mendeley-tags = {flac,tough2},
number = {September 2016},
pages = {254--263},
publisher = {Elsevier},
title = {{Effects of the distribution and evolution of the coefficient of friction along a fault on the assessment of the seismic activity associated with a hypothetical industrial-scale geologic CO2 sequestration operation}},
url = {http://dx.doi.org/10.1016/j.ijggc.2017.09.018},
volume = {66},
year = {2017}
}
@article{Sabbione2010,
abstract = {We have developed three methods for the automatic picking of first breaks that can be used for marine, dynamite, or vibroseis shot records: a modified Coppens's method, an entropy-based method, and a variogram fractal-dimension method. The techniques are based on the fact that the transition between noise and noise plus signal can be automatically identified by detecting rapid changes in a certain attribute (energy ratio, entropy, or fractal dimension), which we calculate within moving windows along the seismic trace. The application of appropriate edge-preserving smoothing operators to enhance these transitions allowed us to develop an automated strategy that can be used to easily signal the precise location of the first-arrival onset. Furthermore, we propose a mispick-correcting technique to exploit the benefits of the data present in the entire shot record, which allows us to adjust the trace-by-trace picks and to discard picks associated with bad or dead traces. As a result, the consistency of the first-break picks is significantly improved. The methods are robust under noisy conditions, computationally efficient, and easy to apply. Results using dynamite and vibroseis field data show that accurate and consistent picks can be obtained in an automated manner even under the presence of correlated noise, bad traces, pulse changes, and indistinct first breaks.},
author = {Sabbione, Juan I and Velis, Danilo},
doi = {10.1190/1.3463703},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Sabbione, Velis{\_}2010.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {jul},
number = {4},
pages = {V67--V76},
title = {{Automatic first-breaks picking: New strategies and algorithms}},
url = {http://geophysics.geoscienceworld.org/content/75/4/V67.abstract http://library.seg.org/doi/10.1190/1.3463703},
volume = {75},
year = {2010}
}
@article{Wiggins1969,
author = {Wiggins, Ralph A.},
doi = {10.1029/JB074i012p03171},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Wiggins{\_}1969.pdf:pdf},
isbn = {0148-0227},
issn = {01480227},
journal = {Journal of Geophysical Research},
keywords = {doi:10.1029/JB074i012p03171,http://dx.doi.org/10.1029/JB074i012p03171,mcmc},
mendeley-tags = {mcmc},
month = {jun},
number = {12},
pages = {3171--3181},
title = {{Monte Carlo inversion of body-wave observations}},
url = {http://doi.wiley.com/10.1029/JB074i012p03171},
volume = {74},
year = {1969}
}
@inproceedings{Mussi2011,
abstract = {This paper describes our latest implementation of Particle Swarm Optimization (PSO) with simple ring topology for modern Graphic Processing Units (GPUs). To achieve both the fastest execution time and the best performance, we designed a parallel version of the algorithm, as fine-grained as possible, without introducing explicit synchronization mechanisms among the particles' evolution processes. The results we obtained show a significant speed-up with respect to both the sequential version of the algorithm run on an up-to-date CPU and our previously developed parallel implementation within the nVIDIA™ CUDA™ architecture. Copyright 2011 ACM.},
address = {New York, New York, USA},
author = {Mussi, Luca and Nashed, Youssef S.G. and Cagnoni, Stefano},
booktitle = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
doi = {10.1145/2001576.2001786},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11/Mussi, Nashed, Cagnoni{\_}2011.pdf:pdf},
isbn = {9781450305570},
keywords = {implementation,mization,parallelization,particle swarm opti-,pso,speed-up technique},
mendeley-tags = {pso},
pages = {1555},
publisher = {ACM Press},
title = {{GPU-based asynchronous particle swarm optimization}},
url = {http://portal.acm.org/citation.cfm?doid=2001576.2001786},
year = {2011}
}
@article{Beyer2002,
abstract = {This article gives a comprehensive introduction into one of the main branches of evolutionary computation – the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed.},
author = {Beyer, Hans-Georg and Beyer, Hans-Georg and Schwefel, Hans-Paul and Schwefel, Hans-Paul},
doi = {10.1023/A:1015059928466},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Natural Computing/Beyer et al.{\_}2002.pdf:pdf},
isbn = {1567-7818},
issn = {1572-9796},
journal = {Natural Computing},
keywords = {abbreviations,bbh,building block hypothesis,cma,computational intelligence,covariance matrix adaptation,csa,cumulative step-size adaptation,darwinian evolution,design principles for genetic,ea,ec,evolution strategies,evolutionary,evolutionary algorithm,evolutionary computation,operators,optimization},
number = {1},
pages = {3 -- 52},
title = {{Evolution strategies – A comprehensive introduction}},
url = {http://www.springerlink.com/content/2311qapbrwgrcyey{\%}5Cnhttp://link.springer.com/10.1023/A:1015059928466},
volume = {1},
year = {2002}
}
@article{Tape2013,
abstract = {A seismic moment tensor is a description of an earthquake source, but the description is indirect. The moment tensor describes seismic radiation rather than the actual physical process that initiates the radiation. A moment tensor 'model' then ties the physical process to the moment tensor. The model is not unique, and the physical process is therefore not unique. In the classical moment tensor model, an earthquake arises from slip along a planar fault, but with the slip not necessarily in the plane of the fault. The model specifies the resulting moment tensor in terms of the slip vector, the fault normal vector and the Lam{\'{e}} elastic parameters, assuming isotropy. We review the classical model in the context of the fundamental lune. The lune is closely related to the space of moment tensors, and it provides a setting that is conceptually natural as well as pictorial. In addition to the classical model, we consider a crack plus double-couple model (CDC model) in which a moment tensor is regarded as the sum of a crack tensor and a double couple.},
author = {Tape, W. and Tape, Carl},
doi = {10.1093/gji/ggt302},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Tape, Tape{\_}2013.pdf:pdf},
isbn = {1365-246X},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Earthquake source observations,Theoretical seismology,moment tensor},
mendeley-tags = {moment tensor},
month = {dec},
number = {3},
pages = {1701--1720},
title = {{The classical model for moment tensors}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggt302},
volume = {195},
year = {2013}
}
@article{Song2012,
abstract = {Rayleigh waves have been used increasingly as an appealing tool to obtain near-surface shear (S)-wave velocity profiles. However, inversion of Rayleigh wave dispersion curves is challenging for most local-search methods due to its high nonlinearity and to its multimodality. In this study, we proposed and tested a new Rayleigh wave dispersion curve inversion scheme based on particle swarm optimization (PSO). PSO is a global optimization strategy that simulates the social behavior observed in a flock (swarm) of birds searching for food. A simple search strategy in PSO guides the algorithm toward the best solution through constant updating of the cognitive knowledge and social behavior of the particles in the swarm.To evaluate calculation efficiency and stability of PSO to inversion of surface wave data, we first inverted three noise-free and three noise-corrupted synthetic data sets. Then, we made a comparative analysis with genetic algorithms (GA) and a Monte Carlo (MC) sampler and reconstructed a histogram of model parameters sampled on a low-misfit region less than 15{\%} relative error to further investigate the performance of the proposed inverse procedure. Finally, we inverted a real-world example from a waste disposal site in NE Italy to examine the applicability of PSO on Rayleigh wave dispersion curves. Results from both synthetic and field data demonstrate that particle swarm optimization can be used for quantitative interpretation of Rayleigh wave dispersion curves. PSO seems superior to GA and MC in terms of both reliability and computational efforts. The great advantages of PSO are fast in locating the low misfit region and easy to implement. Also there are only three parameters to tune (inertia weight or constriction factor, local and global acceleration constants). Theoretical results exist to explain how to tune these parameters. {\textcopyright} 2012 Elsevier B.V.},
author = {Song, Xianhai and Tang, Li and Lv, Xiaochun and Fang, Hongping and Gu, Hanming},
doi = {10.1016/j.jappgeo.2012.05.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Song et al.{\_}2012.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Dispersion curves,Genetic algorithms,Monte carlo,Particle swarm optimization,Rayleigh waves,pso},
mendeley-tags = {pso},
month = {sep},
pages = {1--13},
publisher = {Elsevier B.V.},
title = {{Application of particle swarm optimization to interpret Rayleigh wave dispersion curves}},
url = {http://dx.doi.org/10.1016/j.jappgeo.2012.05.011 http://linkinghub.elsevier.com/retrieve/pii/S0926985112000961},
volume = {84},
year = {2012}
}
@article{Swenson2003,
abstract = {Although powerful, the TOUGH2 codes were initially developed as research tools, using a text- based input file. This presents significant hurdles to a new user, who must create an input file that describes a valid mesh and specifies the appropriate solution controls. Possible simulator options are often activated by flags that are confusing and may appear in unrelated locations in the input file. It is hard enough to solve non-isothermal, multi- component, multi-phase problems without the additional difficulty of preparing complex and lengthy input files. In addition, the user must develop their own toolkit to display results. Even experienced users may need additional help to exploit models with tens of thousands of cells or to enable rapid remeshing for convergence studies. PetraSim – an integrated program for TOUGH2 model creation, analysis, and results display – has undergone continual development since its first release in August, 2002. The latest version includes support for TOUGHREACT and TOUGH- Fx/HYDRATE. New features to be demonstrated in version 3.0 include: support for “extra cells” that are not part of the physical mesh (used for special boundary conditions, etc.), display of a 2D image (such as a map) in the 3D views, improved support for wells, improved 2D editor performance including cell search features, and several other interface enhancements. Use of PetraSim and the TOUGH2 programs will be demonstrated in the poster session.},
author = {Swenson, Daniel and Hardeman, Brian and Presson, Casey and Thornton, Charlie},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/PROCEEDINGS, TOUGH Symposium 2003/Swenson et al.{\_}2003.pdf:pdf},
journal = {PROCEEDINGS, TOUGH Symposium 2003},
keywords = {tough2},
mendeley-tags = {tough2},
pages = {8},
title = {{Using PetraSim to create, execute, and post-process TOUGH2 models}},
url = {http://esd.lbl.gov/TOUGHPLUS/proceedings/2003/SwensonEtAl.pdf},
year = {2003}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects he well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate' number of clusters.},
author = {Rousseeuw, Peter J},
doi = {10.1016/0377-0427(87)90125-7},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Computational and Applied Mathematics/Rousseeuw{\_}1987.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Graphical display,classification,cluster analysis,clustering validity},
mendeley-tags = {cluster analysis},
month = {nov},
pages = {53--65},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0377042787901257},
volume = {20},
year = {1987}
}
@article{Pruess2007,
abstract = {ECO2N is a fluid property module for the TOUGH2 simulator (Version 2.0) that was designed for applications involving geologic storage of CO2in saline aquifers. It includes a comprehensive description of the thermodynamics and thermophysical properties of H2O-NaCl-CO2mixtures, that reproduces fluid properties largely within experimental error for the temperature, pressure and salinity conditions of interest (10 °C ≤ T ≤ 110 °C; P ≤ 600 bar; salinity up to full halite saturation). Flow processes can be modeled isothermally or non-isothermally, and phase conditions represented may include a single (aqueous or CO2-rich) phase, as well as two-phase mixtures. Fluid phases may appear or disappear in the course of a simulation, and solid salt may precipitate or dissolve. ECO2N can model super- as well as sub-critical conditions, but it does not make a distinction between liquid and gaseous CO2and hence is not applicable for processes that involve two CO2-rich phases. This paper highlights significant features of ECO2N, and presents illustrative applications. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Pruess, Karsten and Spycher, Nicolas},
doi = {10.1016/j.enconman.2007.01.016},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Conversion and Management/Pruess, Spycher{\_}2007.pdf:pdf},
isbn = {0196-8904},
issn = {01968904},
journal = {Energy Conversion and Management},
keywords = {Brine density,CO2injection well,Phase partitioning,Salt precipitation,Self-sharpening front,Thermophysical properties,tough2},
mendeley-tags = {tough2},
month = {jun},
number = {6},
pages = {1761--1767},
title = {{ECO2N – A fluid property module for the TOUGH2 code for studies of CO2 storage in saline aquifers}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0196890407000313},
volume = {48},
year = {2007}
}
@article{Bauer2016,
abstract = {The Illinois Basin-Decatur Project safely and successfully injected, over three years, nearly 1.1 million tons (1 million tonnes) of supercritical carbon dioxide (CO2) into the base of a 1640 ft (500 m) thick saline sandstone reservoir at a depth of 7025 ft (2.14 km). The injection interval, with its high porosity and permeability, allowed for injection pressures to be far below fracture pressures during the daily 1102 tons (1000 tonnes) injection rate. Microseismicity was monitored 1.5 years before injection, through the 3 years of injection and now during permanent shut-in which began in November 2014. The overall average of locatable events per day, during injection, was a little over 4, and events appear to be related to development on previously undetected planes of weakness. Some of these planes and active areas may be related to features developed during diagenetic or compactional processes associated with the Precambrian surface topography. Microseismicity during transient shut-in did not show a rate of decrease, large changes in magnitude, distance from the injection well, or depth.},
author = {Bauer, Robert A. and Carney, Michael and Finley, Robert J.},
doi = {10.1016/j.ijggc.2015.12.015},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Bauer, Carney, Finley{\_}2016.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {CO2 sequestration,Illinois Basin-Decatur Project,Induced,Microseismicity,decatur},
mendeley-tags = {decatur},
month = {nov},
pages = {378--388},
publisher = {Elsevier Ltd},
title = {{Overview of microseismic response to CO2 injection into the Mt. Simon saline reservoir at the Illinois Basin-Decatur Project}},
url = {http://dx.doi.org/10.1016/j.ijggc.2015.12.015 https://linkinghub.elsevier.com/retrieve/pii/S1750583615301638},
volume = {54},
year = {2016}
}
@inproceedings{Evers2009,
abstract = {Particle swarm optimization (PSO) is known to suffer from stagnation once particles have prematurely converged to any particular region of the search space. The proposed regrouping PSO (RegPSO) avoids the stagnation problem by automatically triggering swarm regrouping when premature convergence is detected. This mechanism liberates particles from sub-optimal solutions and enables continued progress toward the true global minimum. Particles are regrouped within a range on each dimension proportional to the degree of uncertainty implied by the maximum deviation of any particle from the globally best position. This is a computationally simple yet effective addition to the computationally simple PSO algorithm. Experimental results show that the proposed RegPSO successfully reduces each popular benchmark tested to its approximate global minimum.},
author = {Evers, George I. and {Ben Ghalia}, Mounir},
booktitle = {2009 IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2009.5346625},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2009 IEEE International Conference on Systems, Man and Cybernetics/Evers, Ben Ghalia{\_}2009.pdf:pdf},
isbn = {978-1-4244-2793-2},
keywords = {automatic regrouping mechanism,convergence,maintaining,particle swarm optimization,premature,pso,stagnation},
mendeley-tags = {pso},
month = {oct},
pages = {3901--3908},
publisher = {IEEE},
title = {{Regrouping particle swarm optimization: A new global optimization algorithm with improved performance consistency across benchmarks}},
url = {http://ieeexplore.ieee.org/document/5346625/},
year = {2009}
}
@article{Shi1999,
abstract = {We empirically study the performance of the particle swarm optimizer (PSO). Four different benchmark functions with asymmetric initial range settings are selected as testing functions. The experimental results illustrate the advantages and disadvantages of the PSO. Under all the testing cases, the PSO always converges very quickly towards the optimal positions but may slow its convergence speed when it is near a minimum. Nevertheless, the experimental results show that the PSO is a promising optimization method and a new approach is suggested to improve PSO's performance near the optima, such as using an adaptive inertia weight},
author = {Shi, Yuhui and Eberhart, R C},
doi = {10.1109/CEC.1999.785511},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on/Shi, Eberhart{\_}1999.pdf:pdf},
isbn = {VO - 3},
issn = {1089-778X},
journal = {Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on},
keywords = {Benchmark testing,Convergence,Equations,Evolutionary computation,Genetic algorithms,Genetic mutations,Genetic programming,Optimization methods,PSO,Particle swarm optimization,Space technology,adaptive inertia weight,adaptive systems,asymmetric initial range settings,benchmark functions,convergence,convergence speed,evolutionary computation,optimal positions,optimization method,particle swarm optimization,particle swarm optimizer,pso,testing,testing functions},
mendeley-tags = {pso},
pages = {1--1950 Vol. 3},
pmid = {20371407},
title = {{Empirical study of particle swarm optimization}},
volume = {3},
year = {1999}
}
@inproceedings{Amdahl1967,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Amdahl, Gene M.},
booktitle = {Proceedings of the April 18-20, 1967, spring joint computer conference on - AFIPS '67 (Spring)},
doi = {10.1145/1465482.1465560},
eprint = {0611061v2},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the April 18-20, 1967, spring joint computer conference on - AFIPS '67 (Spring)/Amdahl{\_}1967.pdf:pdf},
isbn = {1558605398},
issn = {18816096},
keywords = {hpc},
mendeley-tags = {hpc},
pages = {483},
pmid = {21914993},
primaryClass = {arXiv:quant-ph},
publisher = {ACM Press},
title = {{Validity of the single processor approach to achieving large scale computing capabilities}},
url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
year = {1967}
}
@article{Dorigo1996,
author = {Dorigo, Marco and Maniezzo, Vittorio and Colorni, Alberto},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
number = {1},
pages = {29--41},
publisher = {IEEE},
title = {{Ant system: optimization by a colony of cooperating agents}},
volume = {26},
year = {1996}
}
@article{Li2010,
abstract = {A global optimizing approach is developed and implemented to retrieve one-dimensional crustal structure by waveform inversion of teleseismic receiver functions. The global optimization for the inversion is performed using a Differential Evolution (DE) algorithm. This modeling approach allows the user to perturb, within a preset range of reasonable bounds, multiple parameters such as Vp, Vp/Vs, thickness and anisotropy of each layer to fit the receiver function waveforms. Compared with linear modeling methods, the global optimal solution can be achieved with fewer model parameters (e.g., a small number of layers) and hence eliminate potential artifacts in the final model. Receiver function bins with small ray parameter intervals are used in the inversion, which can reduce distortion caused by modeling a single receiver function stacked from many recordings spread over a wide range of epicenter distance. The efficacy of this global optimizing approach is demonstrated with synthetic datasets and real receiver functions from the permanent seismic station BJT. {\textcopyright} 2010 Elsevier Ltd.},
author = {Li, Zhiwei and Hao, Tianyao and Xu, Yi and Xu, Ya and Roecker, Steve},
doi = {10.1016/j.cageo.2009.11.007},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Li et al.{\_}2010.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Anisotropy,Differential evolution,Global optimization,Receiver function,Seismic structure,de},
mendeley-tags = {de},
month = {jul},
number = {7},
pages = {871--880},
publisher = {Elsevier},
title = {{A global optimizing approach for waveform inversion of receiver functions}},
url = {http://dx.doi.org/10.1016/j.cageo.2009.11.007 https://linkinghub.elsevier.com/retrieve/pii/S0098300410001032},
volume = {36},
year = {2010}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Machine Learning Research/Pedregosa et al.{\_}2012.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Romary2010,
author = {Romary, Thomas},
doi = {10.1080/17415970903234620},
issn = {1741-5977},
journal = {Inverse Problems in Science and Engineering},
month = {jan},
number = {1},
pages = {111--130},
title = {{Bayesian inversion by parallel interacting Markov chains}},
url = {http://www.tandfonline.com/doi/abs/10.1080/17415970903234620},
volume = {18},
year = {2010}
}
@inproceedings{Subbey2003,
abstract = {This paper will describe a strategy for rapid quantification of uncertainty in reservoir performance prediction. The strategy is based on a combination of streamline and conventional finite difference simulators. Our uncertainty framework uses the Neighbourhood Approximation algorithm to generate an ensemble of history match models, and has been described previously. A speedup in generating the misfit surface is essential since effective quantification of uncertainty can require thousands of reservoir model runs. Our speedup strategy for quantifying uncertainty in performance prediction involves using an approximate streamline simulator to rapidly explore the parameter space to identify good history matching regions, and to generate an approximate misfit surface. We then switch to a conventional, finite difference simulator, and selectively explore the identified parameter space regions. This paper will show results from a parallel version of the Neighbourhood Approximation algorithm on a Linux cluster, demonstrating the advantages of perfect parallelism. We show how it is possible to sample from the posterior probability distribution both to assess accuracy of the approximate misfit surface, and also to generate automatic history match models.},
author = {Subbey, Sam and Mike, Christie and Sambridge, Malcolm},
booktitle = {SPE Reservoir Simulation Symposium},
doi = {10.2118/79678-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Reservoir Simulation Symposium/Subbey, Mike, Sambridge{\_}2003.pdf:pdf},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{A Strategy for Rapid Quantification of Uncertainty in Reservoir Performance Prediction}},
url = {http://www.onepetro.org/doi/10.2118/79678-MS},
year = {2003}
}
@article{Vermeul2016,
abstract = {As part of the FutureGen 2.0 Project, a design was developed for a first-of-its-kind, commercial-scale, near-zero emissions coal-fueled power plant that includes carbon capture and storage (CCS) in a deep saline reservoir. To assess storage site performance and meet the regulatory requirements of the Class VI Underground Injection Control Program for CO 2 Geologic Sequestration, the FutureGen 2.0 Project evaluated, selected, and designed a suite of monitoring technologies for use in (1) evaluating CO 2 mass balance, (2) detecting significant loss of CO 2 containment, (3) tracking the spatial extent of the CO 2 plume and advancement of the pressure front within the storage reservoir, and (4) identifying the occurrence and location of injection-related induced seismicity. The monitoring program design includes direct monitoring of the injection process (above ground and in the injection wells), injection-zone monitoring, early-leak-detection monitoring directly above the primary confining zone, and compliance monitoring within the lowermost underground source of drinking water (USDW); it also includes measurements of formation pressure and other geochemical/isotopic signatures that provide an indication of changes in CO 2 concentration and/or brine composition, both within the injection zone and immediately above the primary confining zone. In addition to these direct measurements, several indirect geophysical monitoring technologies were included in the monitoring program design such as passive seismic and integrated surface deformation monitoring. Although the FutureGen 2.0 Project was suspended by the U.S. Department of Energy prior to implementation of the monitoring program design, this overview is provided with the hope that other current or future CCS projects will derive benefit from consideration of the approach and monitoring network configuration adopted by the project.},
author = {Vermeul, Vince R. and Amonette, James E. and Strickland, Chris E. and Williams, Mark D. and Bonneville, Alain},
doi = {10.1016/j.ijggc.2016.05.023},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Vermeul et al.{\_}2016.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Carbon capture and storage,Carbon sequestration,Leak detection,MVA,Monitoring,decatur},
mendeley-tags = {decatur},
month = {aug},
pages = {193--206},
publisher = {Elsevier Ltd},
title = {{An overview of the monitoring program design for the FutureGen 2.0 CO2 storage site}},
url = {http://dx.doi.org/10.1016/j.ijggc.2016.05.023 https://linkinghub.elsevier.com/retrieve/pii/S1750583616302663},
volume = {51},
year = {2016}
}
@misc{Gong2015,
abstract = {The increasing complexity of real-world optimization problems raises new challenges to evolutionary computation. Responding to these challenges, distributed evolutionary computation has received considerable attention over the past decade. This article provides a comprehensive survey of the state-of-the-art distributed evolutionary algorithms and models, which have been classified into two groups according to their task division mechanism. Population-distributed models are presented with master-slave, island, cellular, hierarchical, and pool architectures, which parallelize an evolution task at population, individual, or operation levels. Dimension-distributed models include coevolution and multi-agent models, which focus on dimension reduction. Insights into the models, such as synchronization, homogeneity, communication, topology, speedup, advantages and disadvantages are also presented and discussed. The study of these models helps guide future development of different and/or improved algorithms. Also highlighted are recent hotspots in this area, including the cloud and MapReduce-based implementations, GPU and CUDA-based implementations, distributed evolutionary multiobjective optimization, and real-world applications. Further, a number of future research directions have been discussed, with a conclusion that the development of distributed evolutionary computation will continue to flourish.},
author = {Gong, Yue Jiao and Chen, Wei Neng and Zhan, Zhi Hui and Zhang, Jun and Li, Yun and Zhang, Qingfu and Li, Jing Jing},
booktitle = {Applied Soft Computing Journal},
doi = {10.1016/j.asoc.2015.04.061},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Applied Soft Computing Journal/Gong et al.{\_}2015.pdf:pdf},
issn = {15684946},
keywords = {Coevolutionary computation,Distributed evolutionary computation,Evolutionary algorithms,Global optimization,Multiobjective optimization,hpc},
mendeley-tags = {hpc},
month = {sep},
pages = {286--300},
title = {{Distributed evolutionary algorithms and their models: A survey of the state-of-the-art}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494615002987},
volume = {34},
year = {2015}
}
@article{DeBoor1972,
abstract = {In computational dealings with splines, the question of representation is of primary importance. For splines of fixed order on a fixed partition, this is a question of choice of basis, since such splines form a linear space. Only three kinds of bases for spline spaces have actually been given serious attention; those consisting of truncated power functions, of cardinal splines, and of B-splines. Truncated power bases are known to be open to severe ill- conditioning, while cardinal splines are difficult to calculate. By contrast, bases consisting of B-splines are well-conditioned, at least for orders {\textless} 20. Such bases are also local in the sense that at every point only a fixed number (equal to the order) of B-splines is nonzero. B-splines are also evaluated quite easily, using their definition as a divided difference of the truncated power function. Unfortunately, such calculations are ill-conditioned, particularly for partitions of widely varying interval lengths, as is indicated by the fact that special measures have to be taken in case of coincident knots. In this note, a different way of evaluating B-splines is discussed which is very well conditioned yet efficient, and which needs no special adjustments in case of coincident knots. It is also shown that the condition of the B-spline basis increases exponentially with the order.},
author = {de Boor, Carl},
doi = {10.1016/0021-9045(72)90080-9},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Approximation Theory/de Boor{\_}1972.pdf:pdf},
isbn = {9783034876926},
issn = {00219045},
journal = {Journal of Approximation Theory},
month = {jul},
number = {1},
pages = {50--62},
title = {{On calculating with B-splines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0021904572900809},
volume = {6},
year = {1972}
}
@article{Beyreuther2010,
author = {Beyreuther, Moritz and Barsch, Robert and Krischer, Lion and Megies, Tobias and Behr, Yannik and Wassermann, Joachim},
doi = {10.1785/gssrl.81.3.530},
issn = {0895-0695},
journal = {Seismological Research Letters},
month = {may},
number = {3},
pages = {530--533},
title = {{ObsPy: A Python Toolbox for Seismology}},
url = {https://pubs.geoscienceworld.org/srl/article/81/3/530-533/143693},
volume = {81},
year = {2010}
}
@techreport{Clerc2012,
abstract = {Since 2006, three successive standard PSO versions have been put on line on the Particle Swarm Central, namely SPSO 2006, 2007, and 2011. The basic principles of all three versions can be informally described the same way, and in general, this statement holds for almost all PSO variants. However, the exact formulae are slightly different, because they took advantage of the latest theoretical analysis available at the time they were designed.},
author = {Clerc, Maurice},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Clerc{\_}2012.pdf:pdf},
keywords = {pso},
mendeley-tags = {pso},
title = {{Standard Particle Swarm Optimisation}},
url = {http://dx.doi.org/10.1007/978-3-540-71605-1{\_}12},
year = {2012}
}
@phdthesis{VanDenBergh2001,
abstract = {Many scientific, engineering and economic problems involve the optimisation of a set
of parameters. These problems include examples like minimising the losses in a power
grid by finding the optimal configuration of the components, or training a neural net-
work to recognise images of people's faces.  Numerous optimisation algorithms have
been proposed to solve these problems, with varying degrees of success. The Particle
Swarm Optimiser (PSO) is a relatively new technique that has been empirically shown
to perform well on many of these optimisation problems. This thesis presents a theo-
retical model that can be used to describe the long-term behaviour of the algorithm.
An enhanced version of the Particle Swarm Optimiser is constructed and shown to have
guaranteed convergence on local minima. This algorithm is extended further, resulting
in an algorithm with guaranteed convergence on global minima. A model for construct-
ing cooperative PSO algorithms is developed, resulting in the introduction of two new
PSO-based algorithms. Empirical results are presented to support the theoretical proper-
ties predicted by the various models, using synthetic benchmark functions to investigate
specific properties. The various PSO-based algorithms are then applied to the task of
training neural networks, corroborating the results obtained on the synthetic benchmark
functions.},
author = {{Van Den Bergh}, F.},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Van Den Bergh{\_}2001.pdf:pdf},
school = {University of Pretoria},
title = {{An analysis of particle swarm optimizers}},
year = {2001}
}
@article{Goertz-Allmann2017,
abstract = {Abstract microseismicity induced by CO2 injection at Decatur, Illinois, occurs in distinct clusters and shows no obvious correlation with the proceeding pressure front. We analyze some of these clusters in more depth by using a waveform cross-correlation approach. With this approach we can associate about 1400 events from two clusters, with moment magnitudes between 1.1 and −1.7, with specific formations of much smaller vertical dimensions (tens of meters) than the depth resolution of traveltime-based event locations. The differentiation of reservoir and basement events, and the definition of subclusters by waveform correlation, rather than by location, helps to better analyze the spatiotemporal evolution of the events within a cluster. In the Decatur case, this is characterized by event migration from the reservoir into the adjacent basement. The spatial variation of Brune stress drop and Gutenberg b value exhibits signs of a fluid-driven triggering mechanism at the cluster level, revealing a punctual hydraulic connection between reservoir and basement, most likely associated with basement faults cutting into the reservoir. The observed clustering of microseismicity can thus be explained by the lateral heterogeneity of permeability and crustal strength and is overall consistent with a pressure-induced triggering mechanism. Hence, proper long-term risk mitigation for large-scale fluid injection close to the basement requires prior mapping of small subseismic basement-connected faults.},
author = {Goertz-Allmann, B. P. and Gibbons, S. J. and Oye, V. and Bauer, R. and Will, R.},
doi = {10.1002/2016JB013731},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Goertz-Allmann et al.{\_}2017.pdf:pdf},
issn = {21699313},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {CO2 storage,decatur,induced microseismicity,source parameters},
mendeley-tags = {decatur},
month = {may},
number = {5},
pages = {3875--3894},
title = {{Characterization of induced seismicity patterns derived from internal structure in event clusters}},
url = {http://doi.wiley.com/10.1002/2016JB013731},
volume = {122},
year = {2017}
}
@article{Auger2006,
abstract = {We present a comprehensive processing tool for the real-time analysis of the source mechanism of very long period (VLP) seismic data based on waveform inversions performed in the frequency domain for a point source. A search for the source providing the best-fitting solution is conducted over a three-dimensional grid of assumed source locations, in which the Green's functions associated with each point source are calculated by finite differences using the reciprocal relation between source and receiver. Tests performed on 62 nodes of a Linux cluster indicate that the waveform inversion and search for the best-fitting signal over 100,000 point sources require roughly 30 s of processing time for a 2-min-long record. The procedure is applied to post-processing of a data archive and to continuous automatic inversion of real-time data at Stromboli, providing insights into different modes of degassing at this volcano.},
author = {Auger, Emmanuel and D'Auria, Luca and Martini, Marcello and Chouet, Bernard and Dawson, Phillip},
doi = {10.1029/2005GL024703},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Research Letters/Auger et al.{\_}2006.pdf:pdf},
isbn = {0094-8276},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {inversion,moment tensor},
mendeley-tags = {inversion,moment tensor},
number = {4},
pages = {1--5},
pmid = {117},
title = {{Real-time monitoring and massive inversion of source parameters of very long period seismic signals: An application to Stromboli Volcano, Italy}},
volume = {33},
year = {2006}
}
@inproceedings{Waters2009,
abstract = {Abstract It is now well documented that hydraulically fractured horizontal wells are effective at stimulating shale reservoirs. Most commercial shale wells utilize cement or open hole packers to achieve some form of annular isolation. With annular isolation now commonly accepted, the general evolution of these completions has been to stimulate ever shorter sections of the lateral with multiple perforation clusters placed closely together. The objective being to create closely spaced fractures, thereby increasing the reservoir contact area that more effectively imposes a pressure drawdown within the ultra-low permeability reservoir, resulting in a higher recovery factor. The heterogeneous nature of unconventional shale reservoirs makes achieving a complex, closely spaced fracture network problematic. Frequently, geologic structure impacts the created fracture network to such a degree that fluid entry points into the reservoir are of secondary importance. Horizontal well evaluation can be utilized to optimize perforation placement and fracture staging for near-wellbore fracture geometry generation. This has proven to improve well performance. An additional productivity step can be achieved if the far field fracture geometry can be influenced by techniques other than perforation and fracture staging, especially if this technique can be applied in real time during the stimulation treatment. One method that has been successful at altering the created hydraulic fracture geometry is real time microseismic fracture mapping coupled with fracture diversion packages incorporated into the fracturing treatment. Evaluating microseismic events in real time allows one to determine the need to change the fracture geometry being generated. The introduction of diversion packages to achieve this fracture geometry alteration can immediately be evaluated for effectiveness via the microseismic activity. This feedback allows the completions engineer to appropriately use these diversion packages so that effective stimulation along the whole lateral can be achieved. Examples of the use of real time microseismic fracture mapping and fracture diversion packages are shown from the Ft. Worth Basin Barnett Shale formation. Both initial well completions and recompletions are shown. The importance of integrating well, geologic, log, and seismic information, is emphasized so that decisions can be made effectively in real time using the microseismic activity from the treatments. These examples demonstrate that hydraulic fracture geometries can be influenced by the introduction of these diversion packages. Resulting well productivity and fracture pressure responses validate the effectiveness of this real time diversion and evaluation process.},
author = {Waters, George and Ramakrishnan, Harihan and Daniels, John and Bentley, Doug and Belhadi, Jamel and Sparkman, Deane},
booktitle = {Proceedings of Offshore Technology Conference},
doi = {10.4043/OTC-20268-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of Offshore Technology Conference/Waters et al.{\_}2009.pdf:pdf},
isbn = {9781555632441},
month = {may},
pages = {1--11},
publisher = {The Offshore Technology Conference},
title = {{Utilization of Real Time Microseismic Monitoring and Hydraulic Fracture Diversion Technology in the Completion of Barnett Shale Horizontal Wells}},
url = {http://www.onepetro.org/mslib/servlet/onepetropreview?id=OTC-20268-MS{\&}soc=OTC},
year = {2009}
}
@article{Nelder1965,
author = {Nelder, J. A. and Mead, R.},
doi = {10.1093/comjnl/7.4.308},
issn = {0010-4620},
journal = {The Computer Journal},
month = {jan},
number = {4},
pages = {308--313},
title = {{A Simplex Method for Function Minimization}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/7.4.308},
volume = {7},
year = {1965}
}
@inproceedings{Luu2017,
abstract = {Microseismic location uncertainties are mainly due to arrival time picking errors, poorly constrained acquisition geometry and the lack of knowledge of the wave propagation medium. More reliable locations of hypocenters with their associated uncertainties can be obtained by propagating velocity model uncertainties which can be obtained by sampling the velocity model space. We propose to use a Competitive Particle Swarm Optimizer (CPSO) to sample the model space by running the algorithm multiple times and keeping all the models that explain the observed data. Then, we perform a cluster analysis on all the acceptable models to define a reliable subset of models to propagate the velocity uncertainties to the microseismic locations. The algorithm is illustrated on a real 3D data set in the context of hydraulic fracturing.},
author = {Luu, K. and Noble, M. and Gesret, A. and Belayouni, N. and Roux, P.F.},
booktitle = {79th EAGE Conference and Exhibition 2017},
isbn = {9789462822177},
title = {{Propagation of velocity uncertainties to Microseismic locations using a Competitive Particle Swarm Optimizer}},
year = {2017}
}
@inproceedings{Bonneau2017,
abstract = {Relationships between fracture growth and seismic emissions are still poorly understood. In this study, we investigate these relationships through an experiment at the laboratory scale. We performed a triaxial compression on an andesite sample while monitoring the microseismic activity. The deformation is imaged by two X-Ray tomogaphic images. The first has been done before and the second after the triaxial test. Finally, we intent to investigate the acoustic emission organization in time and space in the light of the fracture network to find correlations between the fracturing process and the seismic activity. Preliminary results are reported in this paper.},
author = {Bonneau, Francois and Luu, Keurfon and Nicolas, Aurelien and Li, Zhi},
booktitle = {2017 Ring Meeting},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2017 Ring Meeting/Bonneau et al.{\_}2017.pdf:pdf},
keywords = {microseismic},
mendeley-tags = {microseismic},
number = {Figure 1},
pages = {1--10},
title = {{Toward an understanding of the relationship between fracturing process and microseismic activity : study at the laboratory scale}},
year = {2017}
}
@article{Hansen1993,
author = {Hansen, Per Christian and O'Leary, Dianne Prost},
doi = {10.1137/0914086},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
month = {nov},
number = {6},
pages = {1487--1503},
title = {{The Use of the L-Curve in the Regularization of Discrete Ill-Posed Problems}},
url = {http://epubs.siam.org/doi/10.1137/0914086},
volume = {14},
year = {1993}
}
@article{Socco2004,
abstract = {Surface-wave methods (SWMs) are very powerful tools for the near-surface characterization of sites. They can be used to determine the shear-wave velocity and the damping ratio overcoming, in some cases, the limitations of other shallow seismic techniques. The different steps of SWM have to be optimized, taking into consideration the conditions imposed by the small scale of engineering problems. This only allows the acquisition of apparent dispersion characteristics: i.e. the high frequencies and short distances involved make robust modelling algorithms necessary in order to take modal superposition into account. The acquisition has to be properly planned to obtain quality data over an adequate frequency range. Processing and inversion should enable the interpretation of the apparent dispersion characteristics, i.e. evaluating the local quality of the data, filtering coherent noise due to other seismic events and determining energy distribution, higher modes and attenuation. The different approaches that are used to estimate and interpret the dispersion characteristics are considered. Their potential and limits with regard to sensitivity to noise, reliability and capability of extracting significant information present in surface waves are discussed. The theory and modelling algorithms, and the acquisition, processing and inversion procedures suitable for providing stiffness and damping ratio profiles are illustrated, with particular attention to reliability and resolution.},
author = {Socco, L.V. and Strobbia, C.},
doi = {10.3997/1873-0604.2004015},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Near Surface Geophysics/Socco, Strobbia{\_}2004.pdf:pdf},
issn = {18730604},
journal = {Near Surface Geophysics},
number = {22},
pages = {165--185},
title = {{Surface-wave method for near-surface characterization: a tutorial}},
url = {http://nsg.eage.org/publication/publicationdetails/?publication=8126},
volume = {2},
year = {2004}
}
@article{Corciulo2012,
abstract = {In the study, we investigate the use of ambient-noise data to locate microseismic sources at the exploration scale. We develop a multiscale matched-field processing (MFP) approach to localize seismic sources at frequencies below 10 Hz. An application to an actual data set acquired over a hydrocarbon field is presented to determine the reliability of the MFP procedure. The data used were continuously recorded over five days, at a total of 397 stations on a 1-km-per-side square seismic network. The MFP results show: (1) a dominant and stable surface source associated with human activities (road and exploration platforms) around the reservoir; and (2) weaker sources at depth below the seismic network that are related to the injection/extraction process.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Corciulo, Margherita and Roux, Philippe and Campillo, Michel and Dubucq, Dominique and Kuperman, W A},
doi = {10.1190/geo2011-0438.1},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Corciulo et al.{\_}2012.pdf:pdf},
isbn = {9788578110796},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {KS33--KS41},
pmid = {25246403},
title = {{Multiscale matched-field processing for noise-source localization in exploration geophysics}},
url = {http://library.seg.org/doi/10.1190/geo2011-0438.1},
volume = {77},
year = {2012}
}
@article{Poliannikov2013,
abstract = {We study the problem of determining an unknown microseismic event location relative to previously located events using a single monitoring array in a monitoring well. We show that using the available information about the previously located events for locating new events is advantageous compared to locating each event independently. By analysing confidence regions, we compare the performance of two previously proposed location methods, double-difference and interferometry, for varying signal-to-noise ratio and uncertainty in the velocity model. We show that one method may have an advantage over another depending on the experiment geometry, assumptions about uncertainty in velocity and recorded signal, etc. We propose a unified approach to relative event location that includes double-difference and interferometry as special cases, and is applicable to velocity models and well geometries of arbitrary complexity, producing location estimators that are superior to those of double-difference and interferometry.},
author = {Poliannikov, Oleg V. and Prange, Michael and Malcolm, Alison and Djikpesse, Hugues},
doi = {10.1093/gji/ggt119},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Poliannikov et al.{\_}2013.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Instability analysis,Interferometry,Numerical solutions,Theoretical seismology,bayesian},
mendeley-tags = {bayesian},
number = {1},
pages = {557--571},
title = {{A unified Bayesian framework for relative microseismic location}},
volume = {194},
year = {2013}
}
@article{Pan2003,
author = {Pan, Lehua},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/PROCEEDINGS, TOUGH Symposium 2003/Pan{\_}2003.pdf:pdf},
journal = {PROCEEDINGS, TOUGH Symposium 2003},
keywords = {tough2},
mendeley-tags = {tough2},
number = {2},
pages = {1--6},
title = {{WinGridder - An interactive grid generator for TOUGH2}},
year = {2003}
}
@inproceedings{Veezhinathan1990,
abstract = {First-break picking is an extremely time-consuming task for manual$\backslash$noperation since seismic data are so voluminous (e.g. a typical 3-D survey consists of more than half a million traces to be picked). Previous attempts to automate first-arrival picking have achieved only limited success. The authors describe a neural network (NN) solution to this problem using a back-propagation network. The NN-based application system achieved above 95{\%} accuracy on picking several seismic lines based on a single training using only a few seismic records. The level of performance exceeded that achieved by an existing automatic picking program. Job turnaround time (compared to manual picking) improved by 88{\%}. The approach appears robust and shows promise for automating other event-picking tasks in seismic velocity analysis and seismic tomography.},
author = {Veezhinathan, J. and Wagner, Don},
booktitle = {1990 IJCNN International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.1990.137575},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/1990 IJCNN International Joint Conference on Neural Networks/Veezhinathan, Wagner{\_}1990.pdf:pdf},
keywords = {neural network,picking},
mendeley-tags = {neural network,picking},
pages = {235--240 vol.1},
publisher = {IEEE},
title = {{A neural network approach to first break picking}},
url = {http://ieeexplore.ieee.org/document/5726535/},
year = {1990}
}
@article{Malinverno2004,
abstract = {A common way to account for uncertainty in inverse problems is to apply Bayes' rule and obtain a posterior distribution of the quantities of interest given a set of measurements. A conventional Bayesian treatment, however, requires assuming specific values for parameters of the prior distribution and of the distribution of the measurement errors (e.g., the standard deviation of the errors). In practice, these parameters are often poorly known a priori, and choosing a particular value is often problematic. Moreover, the posterior uncertainty is computed assuming that these parameters are fixed; if they are not well known a priori, the posterior uncertainties have dubious value.This paper describes extensions to the conventional Bayesian treatment that assign uncertainty to the parameters defining the prior distribution and the distribution of the measurement errors. These extensions are known in the statistical literature as “empirical Bayes” and “hierarchical Bayes.” We demonstrate the practical application of these approaches to a simple linear inverse problem: using seismic traveltimes measured by a receiver in a well to infer compressional wave slowness in a 1D earth model. These procedures do not require choosing fixed values for poorly known parameters and, at most, need a realistic range (e.g., a minimum and maximum value for the standard deviation of the measurement errors). Inversion is thus made easier for general users, who are not required to set parameters they know little about.},
author = {Malinverno, A. and Briggs, V. A.},
doi = {10.1190/1.1778243},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Malinverno, Briggs{\_}2004.pdf:pdf},
isbn = {00168033},
issn = {00168033},
journal = {Geophysics},
keywords = {bayesian},
mendeley-tags = {bayesian},
number = {4},
pages = {1005--10016},
title = {{Expanded uncertainty quantification in inverse problems: Hierarchical Bayes and empirical Bayes}},
volume = {69},
year = {2004}
}
@article{Mohamed2012,
abstract = {This paper presents the application of a population Markov Chain Monte Carlo (MCMC) technique to generate history-matched models. The technique has been developed and successfully adopted in challenging domains such as computational biology but has not yet seen application in reservoir modelling. In population MCMC, multiple Markov chains are run on a set of response surfaces that form a bridge from the prior to posterior. These response surfaces are formed from the product of the prior with the likelihood raised to a varying power less than one. The chains exchange positions, with the probability of a swap being governed by a standard Metropolis accept/reject step, which allows for large steps to be taken with high probability. We show results of Population MCMC on the IC Fault Model-a simple three-parameter model that is known to have a highly irregular misfit surface and hence be difficult to match. Our results show that population MCMC is able to generate samples from the complex, multi-modal posterior probability distribution of the IC Fault model very effectively. By comparison, previous results from stochastic sampling algorithms often focus on only part of the region of high posterior probability depending on algorithm settings and starting points. {\textcopyright} 2011 Springer Science+Business Media B.V.},
author = {Mohamed, Linah and Calderhead, Ben and Filippone, Maurizio and Christie, Mike and Girolami, Mark},
doi = {10.1007/s10596-011-9232-8},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computational Geosciences/Mohamed et al.{\_}2012.pdf:pdf},
isbn = {1059601192},
issn = {1420-0597},
journal = {Computational Geosciences},
keywords = {History matching,MCMC methods,Population MCMC,Uncertainty quantification,mcmc,uncertainty quantification},
mendeley-tags = {mcmc,uncertainty quantification},
month = {mar},
number = {2},
pages = {423--436},
title = {{Population MCMC methods for history matching and uncertainty quantification}},
url = {http://link.springer.com/10.1007/s10596-011-9232-8},
volume = {16},
year = {2012}
}
@article{Li2013,
abstract = {An improved particle swarm optimization (PSO) algorithm with a neighborhood-redispatch (NR) technique is presented to design an ultrawideband (UWB) antenna in this letter. Based on the conventional version of PSO algorithm, some new parameters are introduced in the NR-PSO algorithm to improve optimization performance. In order to illustrate the effectiveness of the algorithm in representative topology problems, several benchmark functions are used to test the performance of the improved PSO algorithm. The results show that it is better than the conventional PSO algorithm for multimodal and unimodal functions.Combined with theHFSS solver, one UWBantenna with a respective notched band is designed by using the proposed NR-PSO algorithm. It not only covers theUWBband (3.1–10.6GHz) and Bluetooth passband (2.40–2.484 GHz), but also realizes a stopband (5.15–5.825 GHz) to avoid potential interferences.},
author = {Li, Yan-liang and Shao, Wei and You, Long and Wang, Bing-zhong},
doi = {10.1109/LAWP.2013.2283375},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Antennas and Wireless Propagation Letters/Li et al.{\_}2013.pdf:pdf},
issn = {1536-1225},
journal = {IEEE Antennas and Wireless Propagation Letters},
keywords = {Neighborhood redispatch,particle swarm optimization,pso,slot antenna,ultrawideband.},
mendeley-tags = {pso},
number = {3},
pages = {1236--1239},
title = {{An Improved PSO Algorithm and Its Application to UWB Antenna Design}},
url = {http://ieeexplore.ieee.org/document/6607154/},
volume = {12},
year = {2013}
}
@article{Vidale1990,
author = {Vidale, John},
doi = {10.1190/1.1442863},
issn = {0016-8033},
journal = {Geophysics},
month = {may},
number = {5},
pages = {521--526},
title = {{Finite‐difference calculation of traveltimes in three dimensions}},
url = {http://library.seg.org/doi/10.1190/1.1442863},
volume = {55},
year = {1990}
}
@article{Patterson2012a,
author = {Patterson, Christopher G. and Falta, Ronald W.},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/PROCEEDINGS, TOUGH Symposium 2012/Patterson, Falta{\_}2012.pdf:pdf},
journal = {PROCEEDINGS, TOUGH Symposium 2012},
keywords = {tough2},
mendeley-tags = {tough2},
pages = {1--5},
title = {{mView: A powerful pre- and post-processor for TOUGH2}},
year = {2012}
}
@article{Napoles2012,
abstract = {Particle Swarm Optimization (PSO) is a bioinspired meta–heuristic for solving complex global optimization problems. In standard PSO, the particle swarm frequently gets attracted by suboptimal solutions, causing premature convergence of the algorithm and swarm stagnation. Once the particles have been attracted to a local optimum, they continue the search process within a minuscule region of the solution space, and escaping from this local optimum may be difficult. This paper presents a modified variant of constricted PSO that uses random samples in variable neighborhoods for dispersing the swarm whenever a premature convergence (or stagnation) state is detected, offering an escaping alternative from local optima. The performance of the proposed algorithm is discussed and experimental results show its ability to approximate to the global minimum in each of the nine well–known studied benchmark functions.},
author = {N{\'{a}}poles, Gonzalo and Grau, Isel and Bello, Rafael},
doi = {10.17562/PB-46-1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Polibits/N{\'{a}}poles, Grau, Bello{\_}2012.pdf:pdf},
isbn = {1870-9044},
issn = {2395-8618},
journal = {Polibits},
keywords = {global optima,local optima,particle swarm optimization,premature convergence,pso,random samples,variable neighborhoods},
mendeley-tags = {pso},
month = {dec},
number = {46},
pages = {5--11},
title = {{Constricted Particle Swarm Optimization based Algorithm for Global Optimization}},
url = {http://www.polibits.cidetec.ipn.mx/ojs/index.php/polibits/article/view/1788},
volume = {46},
year = {2012}
}
@article{Mosegaard1995,
abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analysing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyse and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
author = {Mosegaard, Klaus and Tarantola, Albert},
doi = {10.1029/94JB03097},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Mosegaard, Tarantola{\_}1995.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {doi:10.1029/94JB03097,http://dx.doi.org/10.1029/94JB03097,mcmc},
mendeley-tags = {mcmc},
month = {jul},
number = {B7},
pages = {12431--12447},
title = {{Monte Carlo sampling of solutions to inverse problems}},
url = {http://doi.wiley.com/10.1029/94JB03097},
volume = {100},
year = {1995}
}
@phdthesis{Luu2015,
author = {Luu, Keurfon},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Luu{\_}2015.pdf:pdf},
keywords = {ambient noise,interferometry,monte-carlo inversion,passive seismic,raphy,seismic processing,shear wave velocity,surface waves,tomog-},
pages = {1--30},
title = {{Passive seismic tomography: feasibility on onshore domain}},
year = {2015}
}
@inproceedings{Rada-Vilela2011,
abstract = {This work provides a further study on the difference between synchronous and asynchronous updates in Particle Swarm Optimization with different neighborhood sizes ranging from local best to global best. Ten well-known functions are used as benchmarks on both variants. Statistical tests performed on the results provide strong evidence to claim that syn-chronous updates yield in general better results with similar or even faster speed of convergence than its asynchronous counterpart, contrary to observations and conclusions of pre-vious studies based solely on descriptive statistics.},
address = {New York, New York, USA},
author = {Rada-Vilela, Juan and Zhang, Mengjie and Seah, Winston},
booktitle = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
doi = {10.1145/2001576.2001581},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11/Rada-Vilela, Zhang, Seah{\_}2011.pdf:pdf},
isbn = {9781450305570},
keywords = {Particle swarm optimization,particle swarm optimization,pso,speed of convergence,synchronous and async,synchronous and asynchronous,updates},
mendeley-tags = {pso},
pages = {21},
publisher = {ACM Press},
title = {{A performance study on synchronous and asynchronous updates in particle swarm optimization}},
url = {http://portal.acm.org/citation.cfm?doid=2001576.2001581},
year = {2011}
}
@article{Rechenberg1973,
author = {Rechenberg, Ingo},
keywords = {cmaes},
mendeley-tags = {cmaes},
title = {{Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution}},
year = {1973}
}
@inproceedings{Gesret2011,
abstract = {Among many factors that contribute to microseismic location errors, the largest contribution is due to the lack of knowledge of the wave-propagation medium. In spite of efforts to build the " best " velocity model derived from surface seismic and/or logging data, these models are very often not adapted to the microseismic context and are characterized by numerous uncertainties. These uncertainties are often enhanced due to the poor aperture of the microseismic monitoring networks. Precise location of hypocenters requires deriving a very accurate velocity model using calibration shots; the inversion to obtain this model is a difficult task but cannot be neglected. We propose a tomography algorithm using calibrations shots that does not produce only a unique " best " velocity model but all velocity models that explain the observed data within the traveltime picking uncertainties. This approach allows deriving location uncertainties associated to velocity model uncertainties. These maps show that the commonly used probability associated to the picking uncertainties must not be used to represent the probability associated to the velocity model uncertainties.},
author = {Gesret, A. and Noble, M. and Desassis, N. and Romary, T.},
booktitle = {Third Passive Seismic Workshop – Actively Passive!},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Third Passive Seismic Workshop – Actively Passive!/Gesret et al.{\_}2011.pdf:pdf},
isbn = {9781629937915},
number = {March 2011},
pages = {PAS31},
title = {{Microseismic Monitoring : Consequences of Velocity Model Uncertainties on Location Uncertainties}},
year = {2011}
}
@article{Lee2014,
abstract = {The Illinois Basin - Decatur Project (IBDP) is a carbon dioxide (CO2) storage project that has a goal to inject 1 million tonnes of CO2 over a three-year period. As a part of managing the CO2 storage, several measurement, monitoring, characterization, data integration, and modelling technologies originally developed for hydrocarbon exploration and production applications were adapted for use on this project. Real-time continuous measurement of microseismicity in the project area showed that these events consistently cluster instead of being randomly located, suggesting the pre-existence of rock imperfections in the injection site. Geomechanics and finite element models that duplicate the location of observed small amplitude microseismic clusters with injection show a high correlation with measured events locations. This work advances the direct use of surface seismic data to constrain mechanical model assumptions pertaining to features associated with injection-induced microseismicity.},
author = {Lee, Donald W. and Mohamed, Farid and Will, Robert and Bauer, Robert and Shelander, Dianna},
doi = {10.1016/j.egypro.2014.11.363},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Lee et al.{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Carbon capture,Finite element model,Mechanical earth model,Microseismic events,decatur},
mendeley-tags = {decatur},
number = {281},
pages = {3347--3356},
publisher = {Elsevier B.V.},
title = {{Integrating Mechanical Earth Models, Surface Seismic, and Microseismic Field Observations at the Illinois Basin - Decatur Project}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.363 https://linkinghub.elsevier.com/retrieve/pii/S187661021402178X},
volume = {63},
year = {2014}
}
@book{Tikhonov2013,
author = {Tikhonov, Andreĭ Nikolaevich and Goncharsky, A V and Stepanov, V V and Yagola, Anatoly G},
isbn = {940158480X},
publisher = {Springer Science {\&} Business Media},
title = {{Numerical methods for the solution of ill-posed problems}},
volume = {328},
year = {2013}
}
@article{Powell1964,
author = {Powell, M. J. D.},
doi = {10.1093/comjnl/7.2.155},
issn = {0010-4620},
journal = {The Computer Journal},
month = {feb},
number = {2},
pages = {155--162},
title = {{An efficient method for finding the minimum of a function of several variables without calculating derivatives}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/7.2.155},
volume = {7},
year = {1964}
}
@article{Aki1957,
author = {Aki, Keiiti},
journal = {Bull. Earth. Res. Inst.},
pages = {415--456},
title = {{Space and time spectra of stationary stochastic waves, with special reference to microtremors}},
volume = {35},
year = {1957}
}
@article{Li2011,
abstract = {A new, relatively high frequency, full waveform matching method was used to study the focal mechanisms of small, local earthquakes induced in an oil field, which are monitored by a sparse near-surface network and a deep borehole network. The determined source properties are helpful for understanding the local stress regime in this field. During the waveform inversion, we maximize both the phase and amplitude matching between the observed and modeled waveforms. We also use the polarities of the first P-wave arrivals and the average S/P amplitude ratios to better constrain the matching. An objective function is constructed to include all four criteria. For different hypocenters and source types, comprehensive synthetic tests showed that our method is robust enough to determine the focal mechanisms under the current array geometries, even when there is considerable velocity inaccuracy. The application to several tens of induced microseismic events showed satisfactory waveform matching between modeled and observed seismograms. Most of the events have a strike direction parallel with the major northeast-southwest faults in the region, and some events trend parallel with the northwest-southeast conjugate faults. The results are consistent with the in situ well breakout measurements and the current knowledge on the stress direction of this region. The source mechanisms of the studied events, together with the hypocenter distribution, indicate that the microearthquakes are caused by the reactivation of preexisting faults. We observed that the faulting mechanism varies with depth, from strike-slip dominance at shallower depth to normal faulting dominance at greater depth.},
author = {Li, Junlun and {Sadi Kuleli}, H. and Zhang, Haijiang and {Nafi Toks{\"{o}}z}, M.},
doi = {10.1190/geo2011-0030.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Li et al.{\_}2011.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {moment tensor},
mendeley-tags = {moment tensor},
month = {nov},
number = {6},
pages = {WC87--WC101},
title = {{Focal mechanism determination of induced microearthquakes in an oil field using full waveforms from shallow and deep seismic networks}},
url = {http://library.seg.org/doi/10.1190/geo2011-0030.1},
volume = {76},
year = {2011}
}
@article{Pruess2005,
author = {Pruess, Karsten},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Pruess{\_}2005.pdf:pdf},
keywords = {tough2},
mendeley-tags = {tough2},
number = {August},
title = {{ECO2N: A TOUGH2 Fluid Property Module for Mixtures of Water, NaCl, and CO2}},
year = {2005}
}
@article{Hsiung2005,
abstract = {Results from the 4-year-long heating phase of the Drift-Scale Heater Test at the Exploratory Studies Facility at Yucca Mountain, Nevada, USA, provide a basis to evaluate conceptual and numerical models used to simulate thermal-mechanical coupled processes expected to occur at the potential geologic repository at Yucca Mountain. The objectives of the evaluation were to investigate coupled processes associated with (i) temperature effects on mechanical deformation and (ii) effect of thermal-mechanical processes on rock-mass permeability. Two-dimensional numerical models were built to perform the thermal-mechanical analyses. Thermal-mechanical simulations were predicated on a continuum representation of a deformation-permeability relationship based on fracture normal stress. The estimated trend of permeability responses using a normal stress-based deformation-permeability relationship compared reasonably to that measured in the coupled thermal-mechanical analyses.},
author = {Hsiung, S.M. and Chowdhury, A.H. and Nataraja, M.S.},
doi = {10.1016/j.ijrmms.2005.03.006},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Rock Mechanics and Mining Sciences/Hsiung, Chowdhury, Nataraja{\_}2005.pdf:pdf},
issn = {13651609},
journal = {International Journal of Rock Mechanics and Mining Sciences},
keywords = {Fracture closure,Fracture normal stress,Fracture permeability,Heater Test,Rock-mass permeability,Thermal-mechanical process,Thermally induced mechanical deformation,flac,tough2},
mendeley-tags = {flac,tough2},
month = {jul},
number = {5-6},
pages = {652--666},
title = {{Numerical simulation of thermal–mechanical processes observed at the Drift-Scale Heater Test at Yucca Mountain, Nevada, USA}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S136516090500033X},
volume = {42},
year = {2005}
}
@inproceedings{Price1996,
abstract = {Differential evolution (DE) is a powerful yet simple evolutionary algorithm for optimizing real-valued multi-modal functions. Function parameters are encoded as floating-point variables and mutated with a simple arithmetic operation. During mutation, a variable-length, one-way crossover operation splices perturbed best-so-far parameter values into existing population vectors. A novel sampling technique adaptively scales the step-size of perturbations as the population evolves. DE's selection criterion demands that improved vectors always be accepted. The performance of DE on a testbed of 15 functions is compared with a variety of recently published results encompassing many different methods. DE converged for all 15 functions and was the fastest method for solving 11 of them. DE's performance on the remaining 4 functions was competitive ER -},
author = {Price, K.V.},
booktitle = {Proceedings of North American Fuzzy Information Processing},
doi = {10.1109/NAFIPS.1996.534790},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of North American Fuzzy Information Processing/Price{\_}1996.pdf:pdf},
isbn = {0-7803-3225-3},
keywords = {de},
mendeley-tags = {de},
pages = {524--527},
publisher = {IEEE},
title = {{Differential evolution: a fast and simple numerical optimizer}},
url = {papers2://publication/uuid/CC402C6E-3DE4-444F-B7E5-48EAC30623DB http://ieeexplore.ieee.org/document/534790/},
year = {1996}
}
@article{Riahi2013,
abstract = {We perform a time-lapse analysis of Rayleigh and Love wave anisotropy above an underground gas storage facility in the Paris Basin. The data were acquired with a three-component seismic array deployed during several days in April and November 2010. Phase velocity and back azimuth of Rayleigh and Love waves are measured in the frequency range 0.2–1.1 Hz using a three-component beamforming algorithm. In both snapshots, higher-surface wave modes start dominating the signal above 0.4 Hz with a concurrent increase in back azimuth ranges. We fit anisotropy parameters to the array detections above 0.4 Hz using a bootstrap approach which also provides estimation uncertainty and enables significance testing. The isotropic phase velocity dispersion for Love and Rayleigh waves match for both snapshots. We also observe a stable fast direction of NNW-SSE for Love and Rayleigh waves which is aligned with the preferred orientation of known shallow ({\textless}300 m) and deeper (∼1000 m) fault systems in the area, as well as the maximum horizontal stress orientation. At lower frequencies corresponding to deeper parts of the basin, the anisotropic parameters exhibit higher magnitude in the November data. This may perhaps be caused by the higher pore pressure changes in the gas reservoir in that depth range.},
author = {Riahi, Nima and Bokelmann, G{\"{o}}tz and Sala, Paola and Saenger, Erik H.},
doi = {10.1002/jgrb.50375},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Riahi et al.{\_}2013.pdf:pdf},
issn = {21699313},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {anisotropy,array processing,hypothesis testing,seismic noise,surface waves,time-lapse},
month = {oct},
number = {10},
pages = {5339--5351},
title = {{Time-lapse analysis of ambient surface wave anisotropy: A three-component array study above an underground gas storage}},
url = {http://doi.wiley.com/10.1002/jgrb.50375},
volume = {118},
year = {2013}
}
@article{Maxwell2010,
author = {Maxwell, S. C. and Rutledge, J. and Jones, R. and Fehler, M.},
doi = {10.1190/1.3477966},
issn = {0016-8033},
journal = {Geophysics},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {sep},
number = {5},
pages = {75A129--75A137},
title = {{Petroleum reservoir characterization using downhole microseismic monitoring}},
url = {http://library.seg.org/doi/10.1190/1.3477966},
volume = {75},
year = {2010}
}
@article{Fogel1993,
author = {Fogel, David B},
issn = {0196-9722},
journal = {Cybernetics and systems},
number = {1},
pages = {27--36},
publisher = {Taylor {\&} Francis},
title = {{Applying evolutionary programming to selected traveling salesman problems}},
volume = {24},
year = {1993}
}
@inproceedings{Luu2016,
abstract = {Seismic traveltime tomography is an optimization problem that requires large computational efforts. Therefore, linearized techniques are commonly used for their low computational cost. These local optimization methods are likely to get trapped in a local minimum as they critically depend on the initial model. On the other hand, common global optimization techniques such as Genetic Algorithm (GA) or Simulated Annealing (SA) are insensitive to the initial model but are computationally expensive and require many controlling parameters. Particle Swarm Optimization (PSO) is a rather new global optimization approach with few parameters that has shown excellent convergence rates and is straightforwardly parallelizable, allowing a good distribution of the workload. However, while it can traverse several local minima of the evaluated misfit function, classical implementation of PSO can get trapped in local minima at later iterations as particles inertia dim. We propose a Competitive PSO (CPSO) to allow "worst" particles to explore the model parameter space and eventually find a better minimum. A tomography algorithm based on CPSO is successfully applied on a 3D synthetic case corresponding to a typical calibration shot geometry in a hydraulic fracturing context.},
author = {Luu, Keurfon and Noble, Mark and Gesret, Alexandrine},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
doi = {10.1190/segam2016-13840267.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Luu, Noble, Gesret{\_}2016.pdf:pdf},
keywords = {cpso,inversion,microseismic,pso},
mendeley-tags = {cpso,inversion,microseismic,pso},
month = {sep},
pages = {2740--2744},
publisher = {Society of Exploration Geophysicists},
title = {{A competitive particle swarm optimization for nonlinear first arrival traveltime tomography}},
url = {http://library.seg.org/doi/10.1190/segam2016-13840267.1},
year = {2016}
}
@article{Maity2014,
abstract = {Microseismic monitoring is an increasingly common geophysical tool to monitor the changes in the subsurface. Autopicking involving phase arrival detection is a common element in microseismic data processing schemes and is necessary for accurate estimation of event locations as well as other workflows such as tomographic or moment tensor inversion, etc. The quality of first arrival picking is dependent on the actual seismic waveform, which in turn is related to the near surface and subsurface structure, source type, noise conditions, environmental factors, and monitoring array design, etc. We have developed a new hybrid autopicking workflow which makes use of multiple derived attributes from the seismic data and combines them within an artificial neural network framework. An evolutionary algorithm scheme is used as the network training algorithm. The autopicker has been tested and its applicability has been validated using a synthetically modelled seismic source, with promising results. In this work, we share the basic workflow and different attributes that have been tested with this algorithm for a synthetic data set to provide a framework for independent implementation, use and validation. We also compare the results obtained using the new neural network based autopicking routine with very robust contemporary autopicking algorithms in use within the industry.},
author = {Maity, Debotyam and Aminzadeh, Fred and Karrenbach, Martin},
doi = {10.1111/1365-2478.12125},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Maity, Aminzadeh, Karrenbach{\_}2014.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Automatic picking,Data processing,Neural network},
month = {jul},
number = {4},
pages = {834--847},
title = {{Novel hybrid artificial neural network based autopicking workflow for passive seismic data}},
url = {http://doi.wiley.com/10.1111/1365-2478.12125},
volume = {62},
year = {2014}
}
@article{Zhang2016,
abstract = {For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods.},
archivePrefix = {arXiv},
arxivId = {1506.05555},
author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
doi = {10.1007/s11222-016-9699-1},
eprint = {1506.05555},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Statistics and Computing/Zhang, Shahbaba, Zhao{\_}2016.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Hamiltonian dynamics,Markov chain Monte Carlo,Random bases,Surrogate method,mcmc},
mendeley-tags = {mcmc},
month = {sep},
pages = {1--18},
title = {{Hamiltonian Monte Carlo acceleration using surrogate functions with random bases}},
url = {http://link.springer.com/10.1007/s11222-016-9699-1},
year = {2016}
}
@article{Got2003,
abstract = {We investigated the microseismicity recorded in an active volcano to infer information concerning the volcano structure and long-term dynamics, by using relative relocations and focal mechanisms of microearthquakes. There were 32,000 earthquakes of the Mauna Loa and Kilauea volcanoes recorded by more than eight stations of the Hawaiian Volcano Observatory seismic network between 1988 and 1999. We studied 17,000 of these events and relocated more than 70{\{}{\%}{\}}, with an accuracy ranging from 10 to 500 m. About 75{\{}{\%}{\}} of these relocated events are located in the vicinity of subhorizontal decollement planes, at a depth of 8--11 km. However, the striking features revealed by these relocation results are steep southeast dipping fault planes working as reverse faults, clearly located below the decollement plane and which intersect it. If this decollement plane coincides with the pre-Mauna Loa seafloor, as hypothesized by numerous authors, such reverse faults rupture the pre-Mauna Loa oceanic crust. The weight of the volcano and pressure in the magma storage system are possible causes of these ruptures, fully compatible with the local stress tensor computed by Gillard et al. [1996]. Reverse faults are suspected of producing scarps revealed by kilometer-long horizontal slip-perpendicular lineations along the decollement surface and therefore large-scale roughness, asperities, and normal stress variations. These are capable of generating stick-slip, large--magnitude earthquakes, the spatial microseismic pattern observed in the south flank of Kilauea volcano, and Hilina-type instabilities. Rupture intersecting the decollement surface, causing its large-scale roughness, may be an important parameter controlling the growth of Hawaiian volcanoes.},
author = {Got, J.-L. and Okubo, P.G.},
doi = {10.1029/2002JB002060},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Got, Okubo{\_}2003.pdf:pdf},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
keywords = {doi:10.102,http://dx.doi.org/10.1029/2002JB002060},
number = {B7},
pages = {1--13},
title = {{New insights into Kilauea's volcano dynamics brought by large-scale relative relocation of microearthquakes}},
volume = {108},
year = {2003}
}
@inproceedings{Ji2011,
abstract = {Finding the least‐absolute (ℓ1 norm) error solution to solve an optimized problem is known to give a better answer than the classical least‐squares (ℓ2 norm) method does. It is because the robust property of the median value is not much affected by outlier values and the solution of the least ℓ1‐norm error corresponds to the solution of minimum median error. Several variants of the ℓ1 norm such as the Huber norm and the Hybrid norm have the same robust properties. These ℓ1‐norm‐based optimization methods obtain their robustness by reducing the influence of outliers significanly, although never ignoring it. Therefore, if the proportion of outliers increases, most of the ℓ1 ‐norm‐based methods may begin to be affected by the outliers. In such a case, other types of robust measures such as Tukey's Biweight (Bisquare weight) norm, which excludes outliers in computing the misfit measure, could perform better. This paper describes the application of the Biweight norm using the IRLS (iteratively reweighted least squares) method as a robust inversion and shows its possible improvement in robustness when dealing with data having complex outliers.},
author = {Ji, Jun},
booktitle = {SEG Technical Program Expanded Abstracts 2011},
doi = {10.1190/1.3627758},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2011/Ji{\_}2011.pdf:pdf},
keywords = {inversion,least squares,noise,nonlinear},
month = {jan},
pages = {2717--2721},
publisher = {Society of Exploration Geophysicists},
title = {{Robust inversion using Biweight norm}},
url = {http://library.seg.org/doi/abs/10.1190/1.3627758},
year = {2011}
}
@inproceedings{Benaichouche2015,
abstract = {Classical algorithms used for first arrival traveltime tomography are not necessarily well-suited for handling very large seismic data sets, densely parameterized velocity models and for taking advantage of current supercomputers architecture. Several authors have recently revisited the classical approach of first arrival traveltime tomography by proposing to use a gradient-type approach based on the adjoint-state of the eikonal equation. With the adjoint-state technique the gradient of the objective function can be obtained without the explicit estimation of the Fr{\'{e}}chet derivative matrix, which is computationally prohibitive for large-scale problems. We propose a new robust, efficient numerical implementation of the adjoint state technique combined with the fast marching method. This scheme allows us to compute the gradient of the misfit function with respect to velocity, it also enable us to build up an approximation of the Hessian. This approach is a very effective solution for processing huge and dense surveys with no compromise in terms of amount of data and model description.},
author = {Benaichouche, Abed and Noble, Mark and Gesret, Alexandrine},
booktitle = {77th EAGE Conference {\&} Exhibition},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/77th EAGE Conference {\&} Exhibition/Benaichouche, Noble, Gesret{\_}2015.pdf:pdf},
pages = {1--4},
title = {{First Arrival Traveltime Tomography Using the Fast Marching Method and the Adjoint State Technique}},
year = {2015}
}
@inproceedings{Souche2013,
abstract = {A new technology for creating, reliably and automatically, structural models from interpretation data is presented. The main idea behind this technique is to model directly volumes (the geological layers) rather than surfaces (horizons that are bounding these layers). In order to enforce the geological consistency of the created models another key element is built into this technology: it guarantees that the variations of dip and thickness of the created geological layers are minimized, while all seismic and well data are properly honored. The proposed method enables the construction of very complex structural models, independently from the geological settings, and even when such models have to be built from sparse or noisy data. The full automation of the model construction process allows to rapidly update the model, to efficiently identify the most uncertain parameters, to understand their impact, and to iteratively optimize the model until it fits all available data. To demonstrate the advantages of this technique the construction of a complex exploration-scale structural model of a prospect located offshore Australia is detailed.},
author = {Souche, L. and Lepage, F. and Iskenova, G.},
booktitle = {London 2013, 75th eage conference en exhibition incorporating SPE Europec},
doi = {10.3997/2214-4609.20130037},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/London 2013, 75th eage conference en exhibition incorporating SPE Europec/Souche, Lepage, Iskenova{\_}2013.pdf:pdf},
keywords = {complex structural models,mesh,volume based modeling},
mendeley-tags = {mesh},
number = {June 2013},
pages = {10--13},
title = {{Volume Based Modeling - Automated Construction of Complex Structural Models}},
url = {http://www.earthdoc.org/publication/publicationdetails/?publication=68318},
year = {2013}
}
@article{VanTrier1991,
author = {van Trier, J. and Symes, W. W.},
doi = {10.1190/1.1443099},
issn = {0016-8033},
journal = {Geophysics},
month = {jun},
number = {6},
pages = {812--821},
title = {{Upwind finite‐difference calculation of traveltimes}},
url = {http://library.seg.org/doi/10.1190/1.1443099},
volume = {56},
year = {1991}
}
@incollection{LeCun1998,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"{u}}ller, Klaus -Robert},
doi = {10.1007/3-540-49430-8_2},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/LeCun et al.{\_}1998.pdf:pdf},
keywords = {machine learning,neural network},
mendeley-tags = {machine learning,neural network},
pages = {9--50},
title = {{Efficient BackProp}},
url = {http://link.springer.com/10.1007/3-540-49430-8{\_}2},
volume = {75},
year = {1998}
}
@article{Jeanne2014,
abstract = {In this study, we performed in situ multidisciplinary analyses of two different fault zones in carbonate formations. One is a seismically active fault zone several kilometers long (the Roccasseira Fault Zone); the other is a small fault zone a few hundred meters long (the GAS Fault Zone). The smaller, "immature" fault zone displays a discontinuous damage zone, because tectonic deformations have been accommodated differently according to the initial properties of the host rock. The larger, "mature" fault zone displays a continuous damage zone caused by the presence of secondary fault cores embedded in a heavily fractured area inside the damage zone. These markedly different fault-zone architectures were reflected in two hydraulic and geomechanical fault models, both generated from a coupled fluid-flow and geomechanical simulator, to examine the impact of hydromechanical property distribution on fault stability when the faults are reactivated by CO2 injection. In the smaller fault zone, marked differences in hydromechanical properties (Young's modulus and permeability) favor fluid accumulation, inducing high pressurization in parts of the damage zone, potentially resulting in small seismic events. On the other hand in the mature fault zone, fluid flows more easily and thus fluid-induced earthquakes may not readily occur, because the fault-zone pressurization is much lower, insufficient for triggering a seismic event. {\textcopyright} 2014 Elsevier Ltd.},
author = {Jeanne, Pierre and Guglielmi, Yves and Cappa, Fr{\'{e}}d{\'{e}}ric and Rinaldi, Antonio P. and Rutqvist, Jonny},
doi = {10.1016/j.jsg.2014.01.017},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Structural Geology/Jeanne et al.{\_}2014.pdf:pdf},
issn = {01918141},
journal = {Journal of Structural Geology},
keywords = {Coupled hydromechanical processes,Damage zone continuity,Fault reactivation,Fault zone architecture,flac,tough2},
mendeley-tags = {flac,tough2},
month = {may},
pages = {97--108},
publisher = {Elsevier Ltd},
title = {{The effects of lateral property variations on fault-zone reactivation by fluid pressurization: Application to CO2 pressurization effects within major and undetected fault zones}},
url = {http://dx.doi.org/10.1016/j.jsg.2014.01.017 https://linkinghub.elsevier.com/retrieve/pii/S0191814114000303},
volume = {62},
year = {2014}
}
@article{Nakaten2014,
abstract = {We discuss a workflow implemented for coupling arbitrary numerical simulators considering complex geological models with discrete faults. This includes grid conversion of geological model grids generated with the Petrel software package to different simulator input formats within a few minutes for multi-million element models. We introduce the conceptual workflow design and tools required for the workflow realization. In this context, different fault representations can be realized including discrete fault planes supported by a virtual element concept in the multiphase flow simulator or an equivalent porous media approach using the mechanical ubiquitous joint model.},
author = {Nakaten, Benjamin and Kempka, Thomas},
doi = {10.1016/j.egypro.2014.11.387},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Nakaten, Kempka{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Faults,Grid conversion,Numerical simulation,Workflow,mesh,tough2},
mendeley-tags = {mesh,tough2},
pages = {3576--3581},
publisher = {Elsevier B.V.},
title = {{Workflow for fast and efficient integration of Petrel-based fault models into coupled hydro-mechanical TOUGH2-MP - FLAC3D simulations of CO2 storage}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.387 https://linkinghub.elsevier.com/retrieve/pii/S1876610214022024},
volume = {63},
year = {2014}
}
@article{Zoback2012,
abstract = {Despite its enormous cost, large-scale carbon capture and storage (CCS) is considered a viable strategy for significantly reducing CO 2 emissions associated with coal-based electrical power generation and other industrial sources of CO 2 [Intergovernmental Panel on Climate Change (2005) IPCC Special Report on Carbon Dioxide Capture and Storage. Prepared by Working Group III of the Intergovernmental Panel on Climate Change, eds Metz B, et al. (Cambridge Univ Press, Cambridge, UK); Szulczewski ML, et al. (2012) Proc Natl Acad Sci USA 109:5185-5189]. We argue here that there is a high probability that earthquakes will be triggered by injection of large volumes of CO2 into the brittle rocks commonly found in continental interiors. Because even small- to moderate-sized earthquakes threaten the seal integrity of CO 2 repositories, in this context, large-scale CCS is a risky, and likely unsuccessful, strategy for significantly reducing greenhouse gas emissions.},
author = {Zoback, M. D. and Gorelick, S. M.},
doi = {10.1073/pnas.1202473109},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the National Academy of Sciences/Zoback, Gorelick{\_}2012.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {jun},
number = {26},
pages = {10164--10168},
title = {{Earthquake triggering and large-scale geologic storage of carbon dioxide}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1202473109},
volume = {109},
year = {2012}
}
@article{Jones1983,
author = {Jones, A G and Olafsdottir, B and Tiikkainen, J},
issn = {0340-062X},
journal = {Journal of Geophysics Zeitschrift Geophysik},
keywords = {mcmc},
mendeley-tags = {mcmc},
pages = {35--50},
title = {{Geomagnetic induction studies in Scandinavia. III Magnetotelluric observations}},
volume = {54},
year = {1983}
}
@inproceedings{Erbas2007,
abstract = {Generating multiple history-matched reservoir models by stochastic sampling to quantify the uncertainty in oil recovery predictions has recently aroused interest in the industry. Coupling a stochastic sampling algorithm with a Bayesian analysis potentially allows incorporation of all sources of uncertainties including data, simulation and interpolation errors. However, the accuracy of the uncertainty estimations strongly depends on the sampling performance. In order to improve the robustness of the coupled Bayesian methodology, the factors that affect the accuracy of the estimations must be examined. This paper investigates how different sampling strategies affect the estimation of uncertainty in prediction of reservoir production. The sampling strategy involves the choice of algorithm and selection of algorithm parameters in sampling the high-dimensional parameter space. We present examples of using both the Neighbourhood Algorithm (NA) and a Genetic Algorithm (GA) to generate history-matched reservoir models for a real field case from the North Sea.},
author = {Erbas, Demet and Christie, Michael A.},
booktitle = {SPE Reservoir Simulation Symposium},
doi = {10.2118/106229-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Reservoir Simulation Symposium/Erbas, Christie{\_}2007.pdf:pdf},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Effect of Sampling Strategies on Prediction Uncertainty Estimation}},
url = {http://www.onepetro.org/doi/10.2118/106229-MS},
year = {2007}
}
@inproceedings{Rastogi2009,
abstract = {Seismic traveltime tomography provides method for direct estimation of velocity distribution in the subsurface from the P or S-wave first arrival traveltime data. Some of the seismic tomographic algorithms like - SIRT, ART and LSQR are widely used for their low compute intensiveness. But they may not yield the better image of the complex subsurface and especially with poor initial model. Also, these methods belong to the category of local optimization techniques, which are likely to get stuck in local minima and are derivative based. Genetic Algorithms (GA), in turn, one of the most popular global optimization method which works even with poor or no initial guess and without derivatives too. It can work well in various operator and coding modifications, e.g., binary and real number coding etc, with any global optimization problem. Any inversion algorithm consists of two important modules; one is the optimization technique and other is the forward modeler scheme. We have to start with certain model, which is nothing but the set of parameter values to be inverted, do a forward modeling, compare the results with the experimental data, update the guess using the optimization method and repeat the process until the match is satisfactory. In current investigation, we are dealing with a problem of inverting the first arrival traveltime data to reconstruct the subsurface velocity model which is nothing but a tomographic inversion problem. Our aim of the present study, is to develop and demonstrate an efficient and robust tool for traveltime tomographic inversion. To achieve this, we have developed an inversion algorithm taking real-coded genetic algorithm (RCGA) as optimization technique and used its multiple realization (run), in order to make sure that we are really near to the global minima. Here, the forward modeler is an efficient finite difference based scheme for 2D first arrival traveltime calculation of a given velocity model. The developed algorithm is highly compute intensive, hence parallelized using Hybrid Island model. The results and the performance of the algorithm are presented and discussed here for a complex synthetic velocity model.},
author = {Rastogi, Richa and Srivastava, Abhishek and Majumder, Satyajit and Gholap, Sachin},
booktitle = {SEG Technical Program Expanded Abstracts 2009},
doi = {10.1190/1.3255362},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2009/Rastogi et al.{\_}2009.pdf:pdf},
isbn = {10523812 (ISSN)},
issn = {10523812},
keywords = {genetic algorithm,inversion},
mendeley-tags = {genetic algorithm,inversion},
month = {jan},
pages = {2491--2495},
publisher = {Society of Exploration Geophysicists},
title = {{Multiple realisation of real‐coded genetic algorithm: A tool for 2D traveltime tomographic inversion}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84857240174{\&}partnerID=40{\&}md5=b0207329e9c7173d6f5a865b42ac96af http://library.seg.org/doi/abs/10.1190/1.3255362},
volume = {28},
year = {2009}
}
@article{Weyland2010,
abstract = {In recent years a lot of novel (mostly naturally inspired) search heuristics have been proposed. Among those approaches is Harmony Search. After its introduction in 2000, positive results and improvements over existing approaches have been reported. In this paper, the authors give a review of the developments of Harmony Search during the past decade and perform a rigorous analysis of this approach. This paper compares Harmony Search to the well-known search heuristic called Evolution Strategies. Harmony Search is a special case of Evolution Strategies in which the authors give compelling evidence for the thesis that research in Harmony is fundamentally misguided. The overarching question is how such a method could be inaccurately portrayed as a significant innovation without confronting a respectable challenge of its content or credentials. The authors examine possible answers to this question, and implications for evaluating other procedures by disclosing the way in which limitations of the method have been systematically overlooked.},
author = {Weyland, Dennis},
doi = {10.4018/jamc.2010040104},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Applied Metaheuristic Computing/Weyland{\_}2010.pdf:pdf},
isbn = {2010040104},
issn = {1947-8283},
journal = {International Journal of Applied Metaheuristic Computing},
month = {apr},
number = {2},
pages = {50--60},
pmid = {44954},
title = {{A Rigorous Analysis of the Harmony Search Algorithm}},
url = {http://www.igi-global.com/article/rigorous-analysis-harmony-search-algorithm/44954 http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jamc.2010040104},
volume = {1},
year = {2010}
}
@incollection{Si2015,
abstract = {TetGen is a C++ program for generating quality tetrahedral meshes aimed to support numerical methods and scientific computing. It is also a research project for studying the underlying mathematical problems and evaluating algorithms. This paper presents the essential meshing components developed in TetGen for robust and efficient software implementation. And it highlights the state-of-the-art algorithms and technologies currently implemented and developed in TetGen for automatic quality tetrahedral mesh generation.},
address = {Berlin/Heidelberg},
author = {Si, Hang and G{\"{a}}rtner, Klaus},
booktitle = {Proceedings of the 14th International Meshing Roundtable},
doi = {10.1007/3-540-29090-7_9},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 14th International Meshing Roundtable/Si, G{\"{a}}rtner{\_}2015.pdf:pdf},
issn = {00983500},
keywords = {delaunay tetrahedralization,mesh,mesh generation,mesh quality,strained delaunay tetrahedralization,tetrahedral mesh},
mendeley-tags = {mesh},
number = {2},
pages = {147--163},
publisher = {Springer-Verlag},
title = {{Meshing Piecewise Linear Complexes by Constrained Delaunay Tetrahedralizations}},
url = {http://link.springer.com/10.1007/3-540-29090-7{\_}9},
volume = {41},
year = {2015}
}
@inproceedings{Rabenseifner2009,
abstract = {Today most systems in high-performance computing (HPC) feature a hierarchical hardware design: Shared memory nodes with several multi-core CPUs are connected via a network infrastructure. Parallel programming must combine distributed memory parallelization on the node interconnect with shared memory parallelization inside each node. We describe potentials and challenges of the dominant programming models on hierarchically structured hardware: Pure MPI (message passing interface), pure OpenMP (with distributed shared memory extensions) and hybrid MPI+OpenMP in several flavors. We pinpoint cases where a hybrid programming model can indeed be the superior solution because of reduced communication needs and memory consumption, or improved load balance. Furthermore we show that machine topology has a significant impact on performance for all parallelization strategies and that topology awareness should be built into all applications in the future. Finally we give an outlook on possible standardization goals and extensions that could make hybrid programming easier to do with performance in mind.},
author = {Rabenseifner, Rolf and Hager, Georg and Jost, Gabriele},
booktitle = {2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
doi = {10.1109/PDP.2009.43},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing/Rabenseifner, Hager, Jost{\_}2009.pdf:pdf},
isbn = {978-0-7695-3544-9},
issn = {1066-6192},
number = {c},
pages = {427--436},
publisher = {IEEE},
title = {{Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes}},
url = {http://ieeexplore.ieee.org/document/4912964/},
year = {2009}
}
@article{Mehnert2013,
abstract = {Numerical modeling of geologic carbon sequestration (GCS) is needed to guide efficient development and to understand potential consequences of its development. The Illinois Basin is a major geologic basin in north-central United States. To evaluate the feasibility of future, commercial-scale GCS within this basin, a basin-scale, flow model is being developed using TOUGH2-MP/ECO2N and refined as new geologic data become available. Current models include the Mt. Simon Sandstone and its caprock (Eau Claire Shale). The project goals are to evaluate the migration of injected CO2 and assess the pressure changes in this open reservoir in response to future GCS developments.},
author = {Mehnert, Edward and Damico, James and Frailey, Scott and Leetaru, Hannes and Lin, Yu-Feng and Okwen, Roland and Adams, Nathaniel and Storsved, Brynne and Valocchi, Albert},
doi = {10.1016/j.egypro.2013.06.282},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Mehnert et al.{\_}2013.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Geologic carbon sequestration,Illinois basin,Model development,Mt. simon sandstone,Saline reservoir,TOUGH2-MP,decatur},
mendeley-tags = {decatur},
pages = {3850--3858},
publisher = {Elsevier B.V.},
title = {{Development of a Basin-scale Model for CO2 Sequestration in the Basal Sandstone Reservoir of the Illinois Basin-issues, Approach and Preliminary Results}},
url = {http://dx.doi.org/10.1016/j.egypro.2013.06.282 https://linkinghub.elsevier.com/retrieve/pii/S1876610213005250},
volume = {37},
year = {2013}
}
@article{Urpi2016,
abstract = {Poro-elastic stress and effective stress reduction associated with deep underground fluid injection can potentially trigger shear rupture along pre-existing faults. We modeled an idealized CO2 injection scenario, to assess the effects on faults in the first phase of a generic CO2 aquifer storage operation. We used coupled multiphase fluid flow and geomechanical numerical modeling to evaluate the stress and pressure perturbations induced by fluid injection and the response of a nearby normal fault. Slip-rate dependent friction and inertial effects have been taken into account during rupture. Contact elements have been used to take into account the frictional behavior of the rupture plane. We investigated different scenarios of injection rate to induce rupture on the fault, employing various fault rheologies. Published laboratory data on CO2-saturated intact and crushed rock samples, representative of a potential target aquifer, sealing formation and fault gouge, have been used to define a scenario where different fault rheologies apply at different depths. Nucleation of fault rupture takes place at the bottom of the reservoir, in agreement with analytical poro-elastic stress calculations, depending on injection-induced reservoir inflation and the tectonic stress scenario. For the stress state considered here, the first triggered rupture always produces the largest rupture length and slip magnitude, both of which correlate with the fault rheology. Velocity weakening produces larger ruptures and generates larger magnitude seismic events. Heterogeneous faults have been considered including velocity-weakening or velocity strengthening sections inside and below the aquifer, with the upper sections being velocity-neutral. Nucleation of rupture in a velocity-strengthening section results in a limited rupture extension, both in terms of maximum slip and rupture length. For a heterogeneous fault with nucleation in a velocity-weakening section, the rupture may propagate into the overlying velocity-neutral section, if the extent of velocity-weakening and associated friction drop are large enough.},
author = {Urpi, Luca and Rinaldi, Antonio P. and Rutqvist, Jonny and Cappa, Fr{\'{e}}d{\'{e}}ric and Spiers, Christopher J.},
doi = {10.1016/j.gete.2016.04.003},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geomechanics for Energy and the Environment/Urpi et al.{\_}2016.pdf:pdf},
issn = {23523808},
journal = {Geomechanics for Energy and the Environment},
keywords = {Carbon sequestration,Fault reactivation,Geomechanics,Seismicity,Velocity-dependent friction,flac,tough2},
mendeley-tags = {flac,tough2},
month = {sep},
pages = {47--65},
publisher = {Elsevier Ltd},
title = {{Dynamic simulation of CO2-injection-induced fault rupture with slip-rate dependent friction coefficient}},
url = {http://dx.doi.org/10.1016/j.gete.2016.04.003 https://linkinghub.elsevier.com/retrieve/pii/S2352380816300284},
volume = {7},
year = {2016}
}
@article{Rumpf2015,
abstract = {To analyse and invert refraction seismic travel time data, different approaches and techniques have been proposed. One common approach is to invert first-break travel times employing local optimization approaches. However, these approaches result in a single velocity model, and it is difficult to assess the quality and to quantify uncertainties and non-uniqueness of the found solution. To address these problems, we propose an inversion strategy relying on a global optimization approach known as particle swarm optimization. With this approach we generate an ensemble of ac- ceptable velocity models, i.e., models explaining our data equally well. We test and evaluate our approach using synthetic seismic travel times and field data collect- ed across a creeping hillslope in the Austrian Alps. Our synthetic study mimics a layered near-surface environment, including a sharp velocity increase with depth and complex refractor topography. Analysing the generated ensemble of acceptable so- lutions using different statistical measures demonstrates that our inversion strategy is able to reconstruct the input velocity model, including reasonable, quantitative estimates of uncertainty. Our field data set is inverted, employing the same strategy, and we further compare our results with the velocity model obtained by a stan- dard local optimization approach and the information from a nearby borehole. This comparison shows that both inversion strategies result in geologically reasonable models (in agreement with the borehole information). However, analysing the model variability of the ensemble generated using our global approach indicates that the result of the local optimization approach is part of this model ensemble. Our results show the benefit of employing a global inversion strategy to generate near-surface velocity models from refraction seismic data sets, especially in cases where no de- tailed apriori information regarding subsurface structures and velocity variations is available.},
author = {Rumpf, Michael and Tronicke, Jens},
doi = {10.1111/1365-2478.12240},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Rumpf, Tronicke{\_}2015.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Inversion,Seismic refraction,Uncertainty,pso},
mendeley-tags = {pso},
month = {sep},
number = {5},
pages = {1188--1197},
title = {{Assessing uncertainty in refraction seismic traveltime inversion using a global inversion strategy}},
url = {http://doi.wiley.com/10.1111/1365-2478.12240},
volume = {63},
year = {2015}
}
@article{Zhou2010,
abstract = {Integrated modeling of basin- and plume-scale processes induced by full-scale deployment of CO 2 storage was applied to the Mt. Simon Aquifer in the Illinois Basin. A three-dimensional mesh was generated with local refinement around 20 injection sites, with approximately 30 km spacing. A total annual injection rate of 100 Mt CO 2 over 50 years was used. The CO 2 -brine flow at the plume scale and the single-phase flow at the basin scale were simulated. Simulation results show the overall shape of a CO 2 plume consisting of a typical gravity-override subplume in the bottom injection zone of high injectivity and a pyramid-shaped subplume in the overlying multilayered Mt. Simon, indicating the important role of a secondary seal with relatively low-permeability and high-entry capillary pressure. The secondary-seal effect is manifested by retarded upward CO 2 migration as a result of multiple secondary seals, coupled with lateral preferential CO 2 viscous fingering through high-permeability layers. The plume width varies from 9.0 to 13.5 km at 200 years, indicating the slow CO 2 migration and no plume interference between storage sites. On the basin scale, pressure perturbations propagate quickly away from injection centers, interfere after less than 1 year, and eventually reach basin margins. The simulated pressure buildup of 35 bar in the injection area is not expected to affect caprock geomechanical integrity. Moderate pressure buildup is observed in Mt. Simon in northern Illinois. However, its impact on groundwater resources is less than the hydraulic drawdown induced by long-term extensive pumping from overlying freshwater aquifers. Copyright {\textcopyright} 2009 The Author(s). Journal compilation {\textcopyright} 2009 National Ground Water Association.},
author = {Zhou, Quanlin and Birkholzer, Jens T. and Mehnert, Edward and Lin, Yu-Feng and Zhang, Keni},
doi = {10.1111/j.1745-6584.2009.00657.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Ground Water/Zhou et al.{\_}2009.pdf:pdf},
issn = {0017467X},
journal = {Ground Water},
keywords = {decatur},
mendeley-tags = {decatur},
month = {dec},
number = {4},
pages = {494--514},
title = {{Modeling Basin- and Plume-Scale Processes of CO2 Storage for Full-Scale Deployment}},
url = {http://doi.wiley.com/10.1111/j.1745-6584.2009.00657.x},
volume = {48},
year = {2009}
}
@article{Saragiotis2013,
abstract = {Event picking is used in many steps of seismic processing. We present an automatic event picking method that is based on a new attribute of seismic signals, instantaneous travel-time. The calculation of the instantaneous traveltime con-sists of two separate but interrelated stages. First, a trace is mapped onto the time-frequency domain. Then the time-frequency representation is mapped back onto the time domain by an appropriate operation. The computed instan-taneous traveltime equals the recording time at those in-stances at which there is a seismic event, a feature that is used to pick the events. We analyzed the concept of the in-stantaneous traveltime and demonstrated the application of our automatic picking method on dynamite and Vibroseis field data.},
author = {Saragiotis, Christos and Alkhalifah, Tariq and Fomel, Sergey},
doi = {10.1190/geo2012-0026.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Saragiotis, Alkhalifah, Fomel{\_}2013.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {mar},
number = {2},
pages = {T53--T58},
title = {{Automatic traveltime picking using instantaneous traveltime}},
url = {http://library.seg.org/doi/10.1190/geo2012-0026.1},
volume = {78},
year = {2013}
}
@article{Drosinos2004,
abstract = {Summary form only given. We compare the performance of three programming paradigms for the parallelization of nested loop algorithms onto SMP clusters. More specifically, we propose three alternative models for tiled nested loop algorithms, namely a pure message passing paradigm, as well as two hybrid ones, that implement communication both through message passing and shared memory access. The hybrid models adopt an advanced hyperplane scheduling scheme, that allows both for minimal thread synchronization, as well as for pipelined execution with overlapping of computation and communication phases. We focus on the experimental evaluation of all three models, and test their performance against several iteration spaces and parallelization grains with the aid of a typical microkernel benchmark. We conclude that the hybrid models can in some cases be more beneficial compared to the monolithic pure message passing model, as they exploit better the configuration characteristics of an hierarchical parallel platform, such as an SMP cluster.},
author = {Drosinos, N. and Koziris, N.},
doi = {10.1109/IPDPS.2004.1302919},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/18th International Parallel and Distributed Processing Symposium, 2004. Proceedings/Drosinos, Koziris{\_}2004.pdf:pdf},
isbn = {0-7695-2132-0},
journal = {18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.},
keywords = {Clustering algorithms,Concurrent computing,Electronic mail,Laboratories,MPI,Message passing,OpenMP parallelization,Parallel programming,Processor scheduling,SMP clusters,Systems engineering and theory,Testing,Yarn,distributed shared memory systems,hybrid model,hyperplane scheduling scheme,message passing,message passing paradigm,microkernel benchmark,nested loop algorithm,open systems,pipeline processing,pipelined execution,shared memory access,synchronisation,thread synchronization,workstation clusters},
number = {C},
pages = {15--24},
title = {{Performance comparison of pure MPI vs hybrid MPI-OpenMP parallelization models on SMP clusters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302919},
volume = {00},
year = {2004}
}
@article{Such2017,
abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.$\backslash$ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}4 hours on one desktop or {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
archivePrefix = {arXiv},
arxivId = {1712.06567},
author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1712.06567},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Such et al.{\_}2017.pdf:pdf},
month = {dec},
title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.06567},
year = {2017}
}
@inproceedings{Lee2015,
abstract = {A novel particle swarm optimization (PSO) method for discrete parameters and its hybridized algorithm with multi-point geostatistics are presented. This stochastic algorithm is designed for complex geological models, which often require discrete facies modeling before simulating continuous reservoir properties. In this paper, we first develop a new PSO method for discrete parameters (Pro-DPSO) where particles move in the probability mass function (pmf) space instead of the parameter space. Then Pro-DPSO is hybridized with the single normal equation simulation algorithm (SNESIM), one of the popular multi-point geostatistics algorithms, to ensure the prior geological features. This hybridized algorithm (Pro-DPSO-SNESIM) is evaluated on a synthetic example of seismic inversion, and compared with a Markov chain Monte Carlo (McMC) method. The results show that the new algorithm generates multiple optimized models with the convergence rate much faster than the McMC method.},
author = {Lee, Jaehoon and Mukerji, Tapan},
booktitle = {SEG Technical Program Expanded Abstracts 2015},
doi = {10.1190/segam2015-5929157.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2015/Lee, Mukerji{\_}2015.pdf:pdf},
keywords = {algorithm,inversion,pso,reservoir chara,statistical},
mendeley-tags = {pso},
month = {aug},
pages = {2754--2759},
publisher = {Society of Exploration Geophysicists},
title = {{Probabilistic particle swarm optimization for discrete parameters (Pro-DPSO) hybridized with multi-point geostatistics: a new stochastic algorithm for complex geological models}},
url = {http://library.seg.org/doi/10.1190/segam2015-5929157.1{\%}5Cnhttp://library.seg.org/doi/pdf/10.1190/segam2015-5929157.1 http://library.seg.org/doi/10.1190/segam2015-5929157.1},
year = {2015}
}
@inproceedings{Kennedy1997,
abstract = {The particle swarm algorithm adjusts the trajectories of a population of "particles" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.},
author = {Kennedy, J. and Eberhart, R.C.},
booktitle = {1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation},
doi = {10.1109/ICSMC.1997.637339},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation/Kennedy, Eberhart{\_}1997.pdf:pdf},
isbn = {0-7803-4053-1},
issn = {1062-922X},
keywords = {pso},
mendeley-tags = {pso},
pages = {4104--4108},
publisher = {IEEE},
title = {{A discrete binary version of the particle swarm algorithm}},
url = {http://ieeexplore.ieee.org/document/637339/},
volume = {5},
year = {1997}
}
@article{Smyth1996,
author = {Smyth, Padhraic},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/KDD/Smyth{\_}1996.pdf:pdf},
isbn = {1-57735-004-9},
journal = {KDD},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
pages = {126--133},
title = {{Clustering Using Monte Carlo Cross-Validation.}},
url = {http://www.aaai.org/Papers/KDD/1996/KDD96-021.pdf},
year = {1996}
}
@article{Improta2002,
abstract = {We have used a dense wide-angle data set to test a two-step procedure for the separate inversion of first-arrival and reflection traveltimes. Data were collected in a complex thrust belt environment (southern Italy) along a 14-km line, with closely spaced sources (60 m) and receivers (90 m). We have applied a fully non-linear tomographic technique, specially designed to image complex structures, to over 6400 first-arrival traveltimes in order to determine a detailed velocity model. A bi-cubic spline velocity model parametrization is used. The inversion strategy follows a multiscale approach, and employs a non-linear velocity optimization scheme. The tomographic velocity model is adopted as the background reference medium for a subsequent interface inversion aimed at imaging a target upper-crust reflector. The interface inversion method is also based on a multiscale approach and uses a non-linear technique for model parameters (interface position nodes) optimization. We have applied the interface inversion method to over 1600 reflection traveltimes of a target event picked both in the near- and in the wide-angle offset range. The retrieved interface is well resolved in the central part of the model, where ray coverage mainly includes clear post-critical reflections and the background velocity model is accurate in depth thanks to large offset deep turning rays. The velocity and interface models thus determined are consistent with Vertical Seismic Profiling data and correlate well with the geometry of known geological structures. This study shows that the used inversion approach is efficient for target-orientated investigations in complex geological environments.},
author = {Improta, L and Zollo, A and Herrero, A and Frattini, R and Virieux, J and Dell'Aversana, P.},
doi = {10.1046/j.1365-246X.2002.01768.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Improta et al.{\_}2002.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {complex geological environment,dense wide-angle data,interface inversion,non-linear inversion,transmission tomography},
month = {oct},
number = {1},
pages = {264--278},
title = {{Seismic imaging of complex structures by non-linear traveltime inversion of dense wide-angle data: application to a thrust belt}},
url = {/home/francesco/Francesco/pdfs/reflection{\_}seismic/improta{\_}etal{\_}2002{\_}JGI.pdf https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.2002.01768.x},
volume = {151},
year = {2002}
}
@article{Kempka2013,
author = {Kempka, Thomas and Class, Holger and G{\"{o}}rke, Uwe-Jens and Norden, Ben and Kolditz, Olaf and K{\"{u}}hn, Michael and Walter, Lena and Wang, Wenqing and Zehner, Bj{\"{o}}rn},
doi = {10.1016/j.egypro.2013.08.048},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Kempka et al.{\_}2013.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {co2 storage,dumu x,eclipse,ketzin pilot site,numerical modelling,opengeosys,tough2},
mendeley-tags = {eclipse,tough2},
number = {June 2008},
pages = {418--427},
publisher = {Elsevier B.V.},
title = {{A Dynamic Flow Simulation Code Intercomparison based on the Revised Static Model of the Ketzin Pilot Site}},
url = {http://dx.doi.org/10.1016/j.egypro.2013.08.048 https://linkinghub.elsevier.com/retrieve/pii/S1876610213016421},
volume = {40},
year = {2013}
}
@book{Haykin1996,
author = {Haykin, Simon},
isbn = {013322760X},
pages = {989},
publisher = {Prentice Hall},
title = {{Adaptive filter theory}},
year = {1996}
}
@article{Maraschini2010a,
abstract = {Higher-mode contribution is important in surface-wave inversion because it allows more information to be exploited, increases investigation depth, and improves model resolution. A new misfit function for multimodal inversion of surface waves, based on the Haskell-Thomson matrix method, allows higher modes to be taken into account without the need to associate experimental data points to a specific mode, thus avoiding mode-misidentification errors in the retrieved velocity profiles. Computing cost is reduced by avoiding the need for calculating synthetic apparent or modal dispersion curves. Based on several synthetic and real examples with inversion results from the classical and the proposed methods, we find that correct velocity models can be retrieved through the multimodal inversion when higher modes are superimposed in the apparent dispersion-curve or when it is not trivial to determine a priori to which mode each data point of the experimental dispersion curve belongs. The main drawback of the method is related to the presence of several local minima in the misfit function. This feature makes the choice of a consistent initial model very important.},
author = {Maraschini, Margherita and Ernst, Fabian and Foti, Sebastiano and Socco, Laura Valentina},
doi = {10.1190/1.3436539},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Maraschini et al.{\_}2010.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {field transformation,matrices,modes,rayleigh-waves,velocity},
month = {jul},
number = {4},
pages = {G31--G43},
title = {{A new misfit function for multimodal inversion of surface waves}},
url = {http://library.seg.org/doi/10.1190/1.3436539},
volume = {75},
year = {2010}
}
@article{Li2018,
abstract = {Accurate and dependable picking of the first arrival time for microseismic data is an important part in microseismic monitoring, which directly affects analysis results of post-processing. This paper presents a new method based on approximate negentropy (AN) theory for microseismic arrival time picking in condition of much lower signal-to-noise ratio (SNR). According to the differences in information characteristics between microseismic data and random noise, an appropriate approximation of negentropy function is selected to minimize the effect of SNR. At the same time, a weighted function of the differences between maximum and minimum value of AN spectrum curve is designed to obtain a proper threshold function. In this way, the region of signal and noise is distinguished to pick the first arrival time accurately. To demonstrate the effectiveness of AN method, we make many experiments on a series of synthetic data with different SNR from −1 dB to −12 dB and compare it with previously published Akaike information criterion (AIC) and short/long time average ratio (STA/LTA) methods. Experimental results indicate that these three methods can achieve well picking effect when SNR is from −1 dB to −8 dB. However, when SNR is as low as −8 dB to −12 dB, the proposed AN method yields more accurate and stable picking result than AIC and STA/LTA methods. Furthermore, the application results of real three-component microseismic data also show that the new method is superior to the other two methods in accuracy and stability.},
author = {Li, Yue and Ni, Zhuo and Tian, Yanan},
doi = {10.1016/j.jappgeo.2018.03.012},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Li, Ni, Tian{\_}2018.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Approximate negentropy,Arrival-time picking,Low SNR,Microseismic data},
month = {may},
pages = {100--109},
publisher = {Elsevier B.V.},
title = {{Arrival-time picking method based on approximate negentropy for microseismic data}},
url = {https://doi.org/10.1016/j.jappgeo.2018.03.012 http://linkinghub.elsevier.com/retrieve/pii/S0926985117300708},
volume = {152},
year = {2018}
}
@article{Lomax2009,
abstract = {Article Outline Glossary I. Definition of the Subject and Its Importance II. Introduction III. The Earthquake Location Problem An inherently nonlinear problem The observed data The velocity or slowness model The travel-time calculation A complete solution -probabilistic location Experimental design methods – choosing receiver locations IV. Location methods Linearized location methods Direct-search location methods V. Illustrative Examples Example 1: An ideal location Examples 2-5: Station distribution Example 6: Incorrect picks and phase identification -outlier data Example 7: Earthquake early-warning scenario Example 8: Incorrect velocity model VI. Future directions VII. Bibliography Books and Reviews Primary Literature Figures Glossary Arrival time The time of the first measurable energy of a seismic phase on a seismogram. Centroid The coordinates of the spatial or temporal average of some characteristic of an earthquake, such as surface shaking intensity or moment release. 26/10/2007 Page 2 Data space If the data are described by a vector d, then the data space D is the set of all possible values of d. Direct search A search or inversion technique that does not explicitly use derivatives. Earthquake early-warning The goal of earthquake early-warning is to estimate the shaking hazard of a large earthquake at a nearby population centre or other critical site before destructive S and surface waves have reached the site. This requires that useful, probabilistic constraint on the location and size of an earthquake is obtained very rapidly. Earthquake location An earthquake location specifies a spatial position and time of occurrence for an earthquake. The location may refer to the earthquake hypocentre and corresponding origin time, a mean or centroid of some spatial or temporal characteristic of the earthquake, or another property of the earthquake that can be spatially and temporally localized. This term also refers to the process of locating an earthquake.},
author = {Lomax, A. and Michelini, A. and Curtis, A.},
doi = {10.1007/978-0-387-30440-3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Lomax, Michelini, Curtis{\_}2009.pdf:pdf},
isbn = {978-0-387-75888-6},
keywords = {location},
mendeley-tags = {location},
pages = {2--449},
title = {{Earthquake location, direct, global-search methods}},
url = {http://geosys.usc.edu/projects/seatree/export/604/trunk/modules/seismo/nonlinloc/nll/doc/EarthqkLoc-Direct-Search{\_}v2.0.pdf},
year = {2009}
}
@article{Pallero2015,
abstract = {Gravity inversion is a classical tool in applied geophysics that corresponds, both, to a linear (density unknown) or nonlinear (geometry unknown) inverse problem depending on the model parameters. Inversion of basement relief of sedimentary basins is an important application among the nonlinear techniques. A common way to approach this problem consists in discretizing the basin using polygons (or other geometries), and iteratively solving the nonlinear inverse problem by local optimization. Nevertheless, this kind of approach is highly dependent of the prior information that is used and lacks from a correct solution appraisal (nonlinear uncertainty analysis). In this paper, we present the application of a full family Particle Swarm Optimizers (PSO) to the 2D gravity inversion and model appraisal (uncertainty assessment) of basement relief in sedimentary basins. The application of these algorithms to synthetic and real cases (a gravimetric profile from Atacama Desert in north Chile) shows that it is possible to perform a fast inversion and uncertainty assessment of the gravimetric model using a sampling while optimizing procedure. Besides, the parameters of these exploratory PSO optimizers are automatically tuned and selected based on stability criteria. We also show that the result is robust to the presence of noise in data. The fact that these algorithms do not require large computational resources makes them very attractive to solve this kind of gravity inversion problems.},
author = {Pallero, J.L.G. and Fern{\'{a}}ndez-Mart{\'{i}}nez, J.L. and Bonvalot, S. and Fudym, O.},
doi = {10.1016/j.jappgeo.2015.03.008},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Pallero et al.{\_}2015.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Chaos,Hysteresis,Quasiperiodicity,pso},
mendeley-tags = {pso},
month = {may},
number = {1-2},
pages = {180--191},
publisher = {Elsevier B.V.},
title = {{Gravity inversion and uncertainty assessment of basement relief via Particle Swarm Optimization}},
url = {http://dx.doi.org/10.1016/j.jappgeo.2015.03.008 https://linkinghub.elsevier.com/retrieve/pii/S0926985115000944},
volume = {116},
year = {2015}
}
@article{Tarantola1982,
abstract = {We examine the general non-linear inverse problem with a finite number of parameters. In order to permit the incorporation of any a priori information about parameters and any distribution of data (not only of gaussian type) we propose to formulate the problem not using single quantities (such as bounds, means, etc.) but using probability density functions for data and parameters. We also want our formulation to allow for the incorporation of theoretical errors, i.e. non-exact theoretical relationships between data and parameters (due to discretization, or incomplete theoretical knowledge); to do that in a natural way we propose to define general theoretical relationships also as probability density functions. We show then that the inverse problem may be formulated as a problem of combination of information: the experimental information about data, the a priori information about parameters, and the theoretical information. With this approach, the general solution of the non-linear inverse problem is unique and consistent (solving the same problem, with the same data, but with a different system of parameters does not change the solution).},
author = {Tarantola, Albert and Valette, Bernard},
doi = {10.1038/nrn1011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysics/Tarantola, Valette{\_}1982.pdf:pdf},
isbn = {0340-062X},
issn = {0340062X},
journal = {Journal of Geophysics},
keywords = {Information,Inverse problems,Pattern recognition,Probability,inversion,location},
mendeley-tags = {inversion,location},
number = {3},
pages = {159--170},
pmid = {12511864},
title = {{Inverse Problems = Quest for Information}},
url = {http://www.ipgp.fr/{~}tarantola/Files/Professional/Papers{\_}PDF/IP{\_}QI{\_}latex.pdf},
volume = {50},
year = {1982}
}
@article{Back1997,
abstract = {Evolutionary computation has started to receive significant attention during the last decade, although the origins can be traced back to the late 1950's. This article surveys the history as well as the current state of this rapidly growing field. We describe the purpose, the general structure, and the working principles of different approaches, including genetic algorithms (GA) (with links to genetic programming (GP) and classifier systems (CS)), evolution strategies (ES), and evolutionary programming (EP) by analysis and comparison of their most important constituents (i.e. representations, variation operators, reproduction, and selection mechanism). Finally, we give a brief overview on the manifold of application domains, although this necessarily must remain incomplete},
author = {Back, T. and Hammel, U. and Schwefel, H.-P.},
doi = {10.1109/4235.585888},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Back, Hammel, Schwefel{\_}1997.pdf:pdf},
isbn = {1089-778X VO - 1},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
number = {1},
pages = {3--17},
title = {{Evolutionary computation: comments on the history and current state}},
url = {http://ieeexplore.ieee.org/document/585888/},
volume = {1},
year = {1997}
}
@article{Bazin2010,
abstract = {On November 21, 2004 an Mw6.3 intraplate earthquake occurred at sea in the French Caribbean. The aftershock sequence continues to this day and is the most extensive sequence in a French territory in more than a century. We recorded aftershocks from day 25 to day 66 of this sequence, using a rapidly-deployed temporary array of ocean bottom seismometers (OBS). We invert P- and S-wave arrivals for a tomographic velocity model and improve aftershock locations. The velocity model shows anomalies related to tectonic and geologic structures beneath the Les Saintes graben. 3D relocated aftershocks outline faults whose scarps were identified as active in recent high-resolution marine data. The aftershocks distribution suggests that both the main November 21 event and its principal aftershock, on February 14, 2005, ruptured Roseau fault, which is the largest of the graben, extending from Dominica Island to the Les Saintes archipelago. Aftershocks cluster in the lower part of the Roseau fault plane (between 8 and 12.6km depth) that did not rupture during the main event. Shallower aftershocks occur in the Roseau fault footwall, probably along smaller antithetic faults. We calculate a strong negative Vp anomaly, between 4 and 8km depth, within the graben, along the Roseau fault plane. This low Vp anomaly is associated with a high Vp/Vs ratio and may reflect a strongly fracturated body filled with fluids. We infer from several types of observation that fault lubrication is the driving mechanism for this long-lasting aftershock sequence. {\textcopyright} 2010 Elsevier B.V.},
author = {Bazin, S. and Feuillet, N. and Duclos, C. and Crawford, W. and Nercessian, A. and Bengoubou-Val{\'{e}}rius, M. and Beauducel, F. and Singh, S. C.},
doi = {10.1016/j.tecto.2010.04.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Tectonophysics/Bazin et al.{\_}2010.pdf:pdf},
isbn = {doi:10.1016/j.tecto.2010.04.005},
issn = {00401951},
journal = {Tectonophysics},
keywords = {Aftershock sequence,Fault-fluid interactions,Les Saintes,Lesser Antilles,Local earthquake tomography,Normal-fault earthquake,Ocean bottom seismometer},
number = {1-4},
pages = {91--103},
publisher = {Elsevier B.V.},
title = {{The 2004-2005 Les Saintes (French West Indies) seismic aftershock sequence observed with ocean bottom seismometers}},
url = {http://dx.doi.org/10.1016/j.tecto.2010.04.005},
volume = {489},
year = {2010}
}
@article{FlorianWellmann2012,
abstract = {Numerical simulations of subsurface fluid and heat flow are commonly controlled manually via input files or from graphical user interfaces (GUIs). Manual editing of input files is often tedious and error-prone, while GUIs typically limit the full capability of the simulator. Neither approach lends itself to automation, which is desirable for more complex simulations. We propose an alternative approach based on the use of scripting. To this end we have developed Python libraries for scripting subsurface simulations using the SHEMAT and TOUGH2 simulators. For many problems the entire modeling process including grid generation, model setup, execution, post-processing and analysis of results can be carried out from a single Python script. Through example problems we demonstrate some of the potential power of the scripting approach, which does not only make model setup simpler and less error-prone, but also facilitates more complex simulations involving, for example, multiple model runs with varying parameters (e.g. permeabilities, heat inputs, and the level of grid refinement). It is also possible to apply the developed methods for extending the functionality of graphical user interfaces. Basing our approach on the Python language makes it simple to take advantage of other libraries available for scientific computation, with sophisticated analysis of results often a matter of a single function call. We envisage many other possible applications of the approach, including linking with geological modeling software, running stochastic ensembles of models and hybrid modeling using multiple interacting simulators. {\textcopyright} 2011 Elsevier Ltd.},
author = {{Florian Wellmann}, J. and Croucher, Adrian and Regenauer-Lieb, Klaus},
doi = {10.1016/j.cageo.2011.10.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Florian Wellmann, Croucher, Regenauer-Lieb{\_}2012.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Automation,Flow simulation,Python,SHEMAT,Scripting,TOUGH2,tough2},
mendeley-tags = {tough2},
month = {jun},
pages = {197--206},
publisher = {Elsevier},
title = {{Python scripting libraries for subsurface fluid and heat flow simulations with TOUGH2 and SHEMAT}},
url = {http://dx.doi.org/10.1016/j.cageo.2011.10.011 https://linkinghub.elsevier.com/retrieve/pii/S0098300411003426},
volume = {43},
year = {2012}
}
@misc{Jones2001,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
keywords = {scipy},
mendeley-tags = {scipy},
title = {{SciPy: Open Source Scientific Tools for Python}},
url = {http://www.scipy.org/},
year = {2001}
}
@article{Park1999,
author = {Park, Choon B and Miller, Richard D and Xia, Jianghai},
issn = {0016-8033},
journal = {Geophysics},
number = {3},
pages = {800--808},
publisher = {Society of Exploration Geophysicists},
title = {{Multichannel analysis of surface waves}},
volume = {64},
year = {1999}
}
@inproceedings{Dong2008,
abstract = {The particle swarm optimization algorithm (PSO) has successfully been applied to many engineering optimization problems. However, the most of existing improved PSO algorithms work well only for small-scale problems on low-dimensional space. In this new self-adaptive PSO, a special function, which is defined in terms of the particle fitness, swarm size and the dimension size of solution space, is introduced to adjust the inertia weight adaptively. In a given generation, the inertia weight for particles with good fitness is decreased to accelerate the convergence rate, whereas the inertia weight for particles with inferior fitness is increased to enhance the global exploration abilities. When the swarm size is large, a smaller inertia weight is utilized to enhance the local search capability for fast convergence rate. If the swarm size is small, a larger inertia weight is employed to improve the global search capability for finding the global optimum. For an optimization problem on multi-dimension complex solution space, a larger inertia weight is employed to strengthen the ability to escape from local optima. In case of small dimension size of solution space, a smaller inertia weight is used for reinforcing the local search capability. This novel self-adaptive PSO can greatly accelerate the convergence rate and improve the capability to reach the global minimum for large-scale problems. Moreover, this new self-adaptive PSO exhibits a consistent methodology: a larger swarm size leads to a better performance.},
author = {Dong, Chen and Wang, Gaofeng and Chen, Zhenyi and Yu, Zuqiang},
booktitle = {Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008},
doi = {10.1109/CSSE.2008.295},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008/Dong et al.{\_}2008.pdf:pdf},
isbn = {9780769533360},
keywords = {Dimension size,Fitness value,Inertia weight,PSO,Self-adaptive,Swarm size,pso},
mendeley-tags = {pso},
pages = {1195--1198},
title = {{A method of self-adaptive inertia weight for PSO}},
volume = {1},
year = {2008}
}
@inproceedings{Hajizadeh2010,
author = {Hajizadeh, Yasin and Christie, Michael A. and Demyanov, Vasily},
booktitle = {SPE EUROPEC/EAGE Annual Conference and Exhibition},
doi = {10.2118/130253-MS},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{History matching with differential evolution approach; a look at new search strategies}},
url = {http://www.onepetro.org/doi/10.2118/130253-MS},
year = {2010}
}
@article{Poliannikov2014,
abstract = {The locations of seismic events are used to infer reservoir properties and to guide future production activity, as well as to determine and understand the stress field. Thus, locating seismic events with uncertainty quantification remains an important problem. Using Bayesian analysis, a joint probability density function of all event locations was constructed from prior information about picking errors in kinematic data and explicitly quantified velocity model uncertainty. Simultaneous location of all seismic events captured the absolute event locations and the relative locations of some events with respect to others, along with their associated uncertainties. We found that the influence of an uncertain velocity model on location uncertainty under many realistic scenarios can be significantly reduced by jointly locating events. Many quantities of interest that are estimated from multiple event locations, such as fault sizes and fracture spacing or orientation, can be better estimated in practice using the proposed approach.},
author = {Poliannikov, Oleg V and Prange, Michael and Malcolm, Alison E and Djikpesse, Hugues},
doi = {10.1190/geo2013-0390.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Poliannikov et al.{\_}2014.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {earthquake,fractures,location,microseismic,traveltime,uncertainty quantification},
mendeley-tags = {location,uncertainty quantification},
month = {nov},
number = {6},
pages = {KS51--KS60},
title = {{Joint location of microseismic events in the presence of velocity uncertainty}},
url = {http://library.seg.org/doi/abs/10.1190/geo2013-0390.1 http://library.seg.org/doi/10.1190/geo2013-0390.1},
volume = {79},
year = {2014}
}
@article{Rutqvist2014,
abstract = {We summarize recent modeling studies of injection-induced fault reactivation, seismicity, and its potential impact on surface structures and nuisance to the local human population. We used coupled multiphase fluid flow and geomechanical numerical modeling, dynamic wave propagation modeling, seismology theories, and empirical vibration criteria from mining and construction industries. We first simulated injection-induced fault reactivation, including dynamic fault slip, seismic source, wave propagation, and ground vibrations. From co-seismic average shear displacement and rupture area, we determined the moment magnitude to about Mw = 3 for an injection-induced fault reactivation at a depth of about 1000 m. We then analyzed the ground vibration results in terms of peak ground acceleration (PGA), peak ground velocity (PGV), and frequency content, with comparison to the U.S. Bureau of Mines' vibration criteria for cosmetic damage to buildings, as well as human-perception vibration limits. For the considered synthetic Mw = 3 event, our analysis showed that the short duration, high frequency ground motion may not cause any significant damage to surface structures, and would not cause, in this particular case, upward CO2 leakage, but would certainly be felt by the local population.},
author = {Rutqvist, Jonny and Cappa, Frederic and Rinaldi, Antonio P. and Godano, Maxime},
doi = {10.1016/j.egypro.2014.11.367},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Rutqvist et al.{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Building damage,Carbon sequestration,Fault reactivation,Ground vibration,Human perception,Induced seismicity,Modeling,flac,tough2},
mendeley-tags = {flac,tough2},
pages = {3379--3389},
publisher = {Elsevier B.V.},
title = {{Dynamic modeling of injection-induced fault reactivation and ground motion and impact on surface structures and human perception}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.367 https://linkinghub.elsevier.com/retrieve/pii/S1876610214021821},
volume = {63},
year = {2014}
}
@article{Kirkpatrick1983,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kirkpatrick, S and Gelatt, C. D. and Vecchi, M. P.},
doi = {10.1126/science.220.4598.671},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Science/Kirkpatrick, Gelatt, Vecchi{\_}1983.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
keywords = {sa},
mendeley-tags = {sa},
month = {may},
number = {4598},
pages = {671--680},
pmid = {17813860},
title = {{Optimization by Simulated Annealing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@article{Delbos2006,
author = {Delbos, F. and Gilbert, J. Ch. and Glowinski, R. and Sinoquet, D.},
doi = {10.1111/j.1365-246X.2005.02729.x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {gauss-newton},
mendeley-tags = {gauss-newton},
month = {mar},
number = {3},
pages = {670--684},
title = {{Constrained optimization in seismic reflection tomography: a Gauss-Newton augmented Lagrangian approach}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2005.02729.x},
volume = {164},
year = {2006}
}
@article{Cheng2015,
abstract = {We proposed a new passive seismic method (PSM) based on seismic interferometry and multichannel analysis of surface waves (MASW) to meet the demand for increasing investigation depth by acquiring surface-wave data at a low-frequency range (1. Hz. ≤. f. ≤. 10. Hz). We utilize seismic interferometry to sort common virtual source gathers (CVSGs) from ambient noise and analyze obtained CVSGs to construct 2D shear-wave velocity (Vs) map using the MASW. Standard ambient noise processing procedures were applied to the computation of cross-correlations. To enhance signal to noise ratio (SNR) of the empirical Green's functions, a new weighted stacking method was implemented. In addition, we proposed a bidirectional shot mode based on the virtual source method to sort CVSGs repeatedly. The PSM was applied to two field data examples. For the test along Han River levee, the results of PSM were compared with the improved roadside passive MASW and spatial autocorrelation method (SPAC). For test in the Western Junggar Basin, PSM was applied to a 70. km long linear survey array with a prominent directional urban noise source and a 60. km-long Vs profile with 1.5. km in depth was mapped. Further, a comparison about the dispersion measurements was made between PSM and frequency-time analysis (FTAN) technique to assess the accuracy of PSM. These examples and comparisons demonstrated that this new method is efficient, flexible, and capable to study near-surface velocity structures based on seismic ambient noise.},
author = {Cheng, Feng and Xia, Jianghai and Xu, Yixian and Xu, Zongbo and Pan, Yudi},
doi = {10.1016/j.jappgeo.2015.04.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Cheng et al.{\_}2015.pdf:pdf},
isbn = {09269851},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {MASW,Passive seismic method,Seismic interferometry,Virtual source method},
month = {jun},
pages = {126--135},
publisher = {Elsevier B.V.},
title = {{A new passive seismic method based on seismic interferometry and multichannel analysis of surface waves}},
url = {http://dx.doi.org/10.1016/j.jappgeo.2015.04.005 http://linkinghub.elsevier.com/retrieve/pii/S0926985115001329},
volume = {117},
year = {2015}
}
@article{Sedlak2008,
abstract = {The information of first-arrival time of acoustic emission (AE) signal is important in event location, event identification and source mechanism analysis. Manual picks are time-consuming and sometimes subjective. Several approaches are used in practice. New first arrival automatic determination technique of AE signals in thin metal plates is presented. Based on Akaike infor-mation criterion (AIC), proposed algorithm of the first-arrival detection uses the specific charac-teristic function, which is sensitive to change of frequency in contrast to others such as envelope of the signal. The approach was tested on real AE data recorded by a four-channel recording system. The results were compared to manual picks and to other AIC approach. It is shown that our two-step AIC picker is a reliable tool to identify the arrival time for AE signals.},
author = {Sedlak, Petr and Hirose, Yuichiro and Enoki, Manabu and Sikula, Josef},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Acoustic Emission/Sedlak et al.{\_}2008.pdf:pdf},
journal = {Journal of Acoustic Emission},
keywords = {akaike information criterion,arrival time detection,multi-layer plate,picking},
mendeley-tags = {picking},
pages = {182--188},
title = {{Arrival Time Detection in Thin Multilayer Plates on the Basis of Akaike Information Criterion}},
volume = {26},
year = {2008}
}
@article{Norbeck2018,
abstract = {The earthquake activity in Oklahoma and Kansas that began in 2008 reflects the most widespread instance of induced seismicity observed to date. We develop a reservoir model to calculate the hydrologic conditions associated with the activity of 902 saltwater disposal wells injecting into the Arbuckle aquifer. Estimates of basement fault stressing conditions inform a rate-and-state friction earthquake nucleation model to forecast the seismic response to injection. Our model replicates many salient features of the induced earthquake sequence, including the onset of seismicity, the timing of the peak seismicity rate, and the reduction in seismicity following decreased disposal activity. We present evidence for variable time lags between changes in injection and seismicity rates, consistent with the prediction from rate-and-state theory that seismicity rate transients occur over timescales inversely proportional to stressing rate. Given the efficacy of the hydromechanical model, as confirmed through a likelihood statistical test, the results of this study support broader integration of earthquake physics within seismic hazard analysis.},
author = {Norbeck, J. H. and Rubinstein, J. L.},
doi = {10.1002/2017GL076562},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Research Letters/Norbeck, Rubinstein{\_}2018.pdf:pdf},
issn = {0094-8276},
journal = {Geophysical Research Letters},
keywords = {decatur,earthquake hazard,geomechanics,hydrology,induced seismicity,rate-and-state friction},
mendeley-tags = {decatur},
month = {apr},
number = {7},
pages = {2963--2975},
title = {{Hydromechanical Earthquake Nucleation Model Forecasts Onset, Peak, and Falling Rates of Induced Seismicity in Oklahoma and Kansas}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2017GL076562},
volume = {45},
year = {2018}
}
@article{Cappa2011,
abstract = {Can CO2 storage cause earthquakes? What is the maximum possible earthquake magnitude resulting from CO2 injection? Here, as a theoretical case study we investigate these questions using coupled hydromechanical modeling with multiphase flow and seismological variables for quantifying earthquake magnitude and energy. Our simulations consider transient fluid flow and stress coupling, and the evolution of fault properties. We simulate CO2 injection into a reservoir-caprock system bounded by a subvertical normal fault subjected to different extensional stress regimes and over a range of initial fault permeability values. For our assumed system and injection rate, the simulation results show that sudden stress drop and fault slip primarily initiated along the fault portion intersecting the storage reservoir after a few months of injection when a sufficiently high reservoir pressure has been reached. The size of the rupture area, and consequently, the earthquake magnitude and energy, depends on initial horizontal-to-vertical stress ratio and fault permeability, which strongly influences the size of the pressurized area, and subsequent stress variations.},
author = {Cappa, Fr{\'{e}}d{\'{e}}ric and Rutqvist, Jonny},
doi = {10.1029/2011GL048487},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Research Letters/Cappa, Rutqvist{\_}2011.pdf:pdf},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {flac,tough2},
mendeley-tags = {flac,tough2},
month = {sep},
number = {17},
title = {{Impact of CO2 geological sequestration on the nucleation of earthquakes}},
url = {http://doi.wiley.com/10.1029/2011GL048487},
volume = {38},
year = {2011}
}
@article{Hansen2010,
abstract = {We present a novel method for handling uncertainty in evolutionary optimization. The method entails quantification and treatment of uncertainty and relies on the rank based selection operator of evolutionary algorithms. The proposed uncertainty handling is implemented in the context of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and verified on test functions. The present method is independent of the uncertainty distribution, prevents premature convergence of the evolution strategy and is well suited for online optimization as it requires only a small number of additional function evaluations. The algorithm is applied in an experimental set-up to the online optimization of feedback controllers of thermoacoustic instabilities of gas turbine combustors. In order to mitigate these instabilities, gain-delay or model-based Hinfty controllers sense the pressure and command secondary fuel injectors. The parameters of these controllers are usually specified via a trial and error procedure. We demonstrate that their online optimization with the proposed methodology enhances, in an automated fashion, the online performance of the controllers, even under highly unsteady operating conditions, and it also compensates for uncertainties in the model-building and design process.},
author = {Hansen, Nikolaus},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Hansen{\_}2010.pdf:pdf},
isbn = {0999999982},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {cmaes},
mendeley-tags = {cmaes},
number = {2},
pages = {180--197},
title = {{Errata / Addenda for A Method for Handling Uncertainty in Evolutionary Optimization With an Application to Feedback Control of Combustion}},
volume = {13},
year = {2010}
}
@article{Schutte2004,
abstract = {Present day engineering optimization problems often impose large computational demands, resulting in long solution times even on a modern high-end processor. To obtain enhanced computational throughput and global search capability, we detail the coarse-grained parallelization of an increasingly popular global search method, the particle swarm optimization (PSO) algorithm. Parallel PSO performance was evaluated using two categories of optimization problems possessing multiple local minima-large-scale analytical test problems with computationally cheap function evaluations and medium-scale biomechanical system identification problems with computationally expensive function evaluations. For load-balanced analytical test problems formulated using 128 design variables, speedup was close to ideal and parallel efficiency above 95{\%} for up to 32 nodes on a Beowulf cluster. In contrast, for load-imbalanced biomechanical system identification problems with 12 design variables, speedup plateaued and parallel efficiency decreased almost linearly with increasing number of nodes. The primary factor affecting parallel performance was the synchronization requirement of the parallel algorithm, which dictated that each iteration must wait for completion of the slowest fitness evaluation. When the analytical problems were solved using a fixed number of swarm iterations, a single population of 128 particles produced a better convergence rate than did multiple independent runs performed using sub-populations (8 runs with 16 particles, 4 runs with 32 particles, or 2 runs with 64 particles). These results suggest that (1) parallel PSO exhibits excellent parallel performance under load-balanced conditions, (2) an asynchronous implementation would be valuable for real-life problems subject to load imbalance, and (3) larger population sizes should be considered when multiple processors are available.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Schutte, J. F. and Reinbolt, J. A. and Fregly, B. J. and Haftka, R. T. and George, A. D.},
doi = {10.1002/nme.1149},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal for Numerical Methods in Engineering/Schutte et al.{\_}2004.pdf:pdf},
isbn = {3143627344},
issn = {00295981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {Cluster computing,Parallel global optimization,Particle swarm},
number = {13},
pages = {2296--2315},
pmid = {17891226},
title = {{Parallel global optimization with the particle swarm algorithm}},
volume = {61},
year = {2004}
}
@article{Diouane2016,
abstract = {In this paper we propose a new way to compute a rough approximation solution, to be later used as a warm starting point in a more refined optimization process, for a challenging global optimization problem related to Earth imaging in geophysics. The warm start con- sists of a velocity model that approximately solves a full-waveform inverse problem at low frequency. Our motivation arises from the availability of massively parallel computing plat- forms and the natural parallelization of evolution strategies as global optimization methods for continuous variables. Our first contribution consists of developing a new and efficient parametrization of the velocity models to significantly reduce the dimension of the original optimization space. Our second contribution is to adapt a class of evolution strategies to the specificity of the physical problem at hands where the objective function evaluation is known to be the most expen- sive computational part. A third contribution is the development of a parallel evolution strategy solver, taking advantage of a recently proposed modification of these class of evolu- tionary methods that ensures convergence and promotes better performance under moderate budgets. The numerical results presented demonstrate the effectiveness of the algorithm on a realistic 3D full-waveform inverse problem in geophysics. The developed numerical approach allows us to successfully solve an acoustic full-waveform inversion problem at low frequencies on a reasonable number of cores of a distributed memory computer.},
author = {Diouane, Y. and Gratton, S. and Vasseur, X. and Vicente, L. N. and Calandra, H.},
doi = {10.1007/s11081-015-9296-8},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Optimization and Engineering/Diouane et al.{\_}2016.pdf:pdf},
issn = {1389-4420},
journal = {Optimization and Engineering},
keywords = {Earth imaging,Evolution strategy,Full-waveform inversion,Global convergence,High performance computing (HPC),Inverse problem,Search space reduction,cmaes,inversion},
mendeley-tags = {cmaes,inversion},
month = {mar},
number = {1},
pages = {3--26},
title = {{A parallel evolution strategy for an earth imaging problem in geophysics}},
url = {http://link.springer.com/10.1007/s11081-015-9296-8},
volume = {17},
year = {2016}
}
@article{Calvez2007,
author = {Calvez, J. H. L. and Craven, M. E. and Klem, R. C. and Baihly, J. D. and Bennett, L. A. and Brook, K.},
doi = {10.2118/106159-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Hydraulic Fracturing Technology Conference/Calvez et al.{\_}2007.pdf:pdf},
isbn = {9781555631611},
journal = {SPE Hydraulic Fracturing Technology Conference},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
number = {SPE 106159},
pages = {7},
title = {{Real-Time Microseismic Monitoring of Hydraulic Fracture Treatment: A Tool To Improve Completion and Reservoir Management}},
year = {2007}
}
@article{Hooke1961,
author = {Hooke, Robert and Jeeves, T. A.},
doi = {10.1145/321062.321069},
issn = {00045411},
journal = {Journal of the ACM},
month = {apr},
number = {2},
pages = {212--229},
title = {{"Direct Search'' Solution of Numerical and Statistical Problems}},
url = {http://portal.acm.org/citation.cfm?doid=321062.321069},
volume = {8},
year = {1961}
}
@inproceedings{Ronald1994,
author = {Ronald, Edmund and Schoenauer, Marc},
booktitle = {International Conference on Parallel Problem Solving from Nature},
pages = {452--461},
publisher = {Springer},
title = {{Genetic Lander: An experiment in accurate neuro-genetic control}},
year = {1994}
}
@article{Sculley2010,
abstract = {We present two modifications to the popular k-means clus- tering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ϵ- accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml},
author = {Sculley, D},
doi = {10.1145/1772690.1772862},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 19th international conference on World wide web WWW 10/Sculley{\_}2010.pdf:pdf},
isbn = {9781605587998},
issn = {1605587990},
journal = {Proceedings of the 19th international conference on World wide web WWW 10},
pages = {1177},
title = {{Web-scale k-means clustering}},
url = {http://portal.acm.org/citation.cfm?doid=1772690.1772862},
year = {2010}
}
@article{Galetti2015,
abstract = {Estimating image uncertainty is fundamental to guiding the interpretation of geoscientific tomo-graphic maps. We reveal novel uncertainty topologies (loops) which indicate that while the speeds of both low-and high-velocity anomalies may be well constrained, their locations tend to remain uncertain. The effect is widespread: loops dominate around a third of UK Love wave tomographic uncertainties, changing the nature of interpretation of the observed anomalies. Loops exist due to 2 nd and higher order aspects of wave physics; hence, although such structures must exist in many tomographic studies in the physical sciences and medicine, they are unobservable using standard linearized methods. Higher order methods might fruitfully be adopted.},
author = {Galetti, Erica and Curtis, Andrew and Meles, Giovanni Angelo and Baptie, Brian},
doi = {10.1103/PhysRevLett.114.148501},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physical Review Letters/Galetti et al.{\_}2015.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {apr},
number = {14},
pages = {148501},
title = {{Uncertainty Loops in Travel-Time Tomography from Nonlinear Wave Physics}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.114.148501},
volume = {114},
year = {2015}
}
@article{Poliannikov2012,
abstract = {With high-permeability hydrocarbon reservoirs exhausting their potential, developing low-permeability reservoirs is becoming of increasing importance. In order to be produced economically, these reservoirs need to be stimulated to increase their permeability. Hydraulic fracturing is a technique used to do this. A mixture of water, additives, and proppants is injected under high pressure into the subsurface; this fluid fractures the rock, creating additional pathways for the oil or gas. Understanding the nature of the resulting fracture system, including the geometry, size, and orientation of individual fractures, as well as the distance from one fracture to the next, is key to answering important practical questions such as: What is the affected reservoir volume? Where should we fracture next? What are the optimal locations for future production wells?},
author = {Poliannikov, Oleg V. and Malcolm, Alison E. and Prange, Michael and Djikpesse, Hugues},
doi = {10.1190/tle31121490.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The Leading Edge/Poliannikov et al.{\_}2012.pdf:pdf},
issn = {1070-485X,},
journal = {The Leading Edge},
number = {12},
pages = {1490--1494},
title = {{Checking up on the neighbors: Quantifying uncertainty in relative event location}},
url = {http://tle.geoscienceworld.org/content/31/12/1490{\%}5Cnhttp://tle.geoscienceworld.org/content/31/12/1490.full.pdf{\%}5Cnhttp://tle.geoscienceworld.org/content/31/12/1490.abstract},
volume = {31},
year = {2012}
}
@article{Ryberg2018,
author = {Ryberg, T and Haberland, Ch},
doi = {10.1093/gji/ggx500},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Ryberg, Haberland{\_}2018.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {mcmc,refraction tomography},
mendeley-tags = {mcmc,refraction tomography},
month = {mar},
number = {3},
pages = {1645--1656},
title = {{Bayesian inversion of refraction seismic traveltime data}},
url = {http://academic.oup.com/gji/article/212/3/1645/4644833},
volume = {212},
year = {2018}
}
@article{VanderBaan2000,
abstract = {Neural networks are increasingly popular in geophysics. Because they are universal approximators, these tools can approximate any continuous function with an arbitrary precision. Hence, they may yield important contributions to finding solutions to a variety of geo physical applications. However, knowledge of many methods and techniques recently developed to increase the performance and to facilitate the use of neural networks does not seem to be widespread in the geophysical community. Therefore, the power of these tools has not yet been explored to their full extent. In this paper, techniques are described for faster training, better overall performance, i.e., generalization, and the automatic estimation of network size and architecture.},
author = {van der Baan, Mirko and Jutten, Christian},
doi = {10.1190/1.1444797},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/van der Baan, Jutten{\_}2000.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
keywords = {machine learning,neural network},
mendeley-tags = {machine learning,neural network},
month = {jul},
number = {4},
pages = {1032--1047},
title = {{Neural networks in geophysical applications}},
url = {http://library.seg.org/doi/10.1190/1.1444797},
volume = {65},
year = {2000}
}
@article{Sabbione2013,
abstract = {We present a robust method for the automatic detection and picking of microseismic events that consists of two steps. The first step provides accurate single-trace picks using three automatic phase pickers adapted from earthquake seismology. In the second step, a multi-channel strategy is implemented to associate (or not) the previous picks with actual microseismic signals by taking into account their expected alignment in all the available channels, thus reducing the false positive rate. As a result, the method provides the number of declared microseismic events, a confidence indicator associated with each of them, and the corresponding traveltime picks. Results using two field noisy data records demonstrate that the automatic detection and picking of microseismic events can be carried out with a relatively high confidence level and accuracy. ?? 2013.},
author = {Sabbione, Juan I. and Velis, Danilo R.},
doi = {10.1016/j.jappgeo.2013.07.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Sabbione, Velis{\_}2013.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Automatic detection,Microseism,Picking,picking},
mendeley-tags = {picking},
month = {dec},
pages = {42--50},
title = {{A robust method for microseismic event detection based on automatic phase pickers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092698511300150X},
volume = {99},
year = {2013}
}
@article{Bhattacharya2019,
abstract = {Earthquake swarms attributed to subsurface fluid injection are usually assumed to occur on faults destabilized by increased pore-fluid pressures. However, fluid injection could also activate aseismic slip, which might outpace pore-fluid migration and transmit earthquake-triggering stress changes beyond the fluid-pressurized region. We tested this theoretical prediction against data derived from fluid-injection experiments that activated and measured slow, aseismic slip on preexisting, shallow faults. We found that the pore pressure and slip history imply a fault whose strength is the product of a slip-weakening friction coefficient and the local effective normal stress. Using a coupled shear-rupture model, we derived constraints on the hydromechanical parameters of the actively deforming fault. The inferred aseismic rupture front propagates faster and to larger distances than the diffusion of pressurized pore fluid.},
author = {Bhattacharya, Pathikrit and Viesca, Robert C.},
doi = {10.1126/science.aaw7354},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Science/Bhattacharya, Viesca{\_}2019.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {may},
number = {6439},
pages = {464--468},
title = {{Fluid-induced aseismic fault slip outpaces pore-fluid migration}},
url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aaw7354},
volume = {364},
year = {2019}
}
@article{Chawla2011,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Artificial Intelligence Research/Chawla et al.{\_}2011.pdf:pdf},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {321--357},
pmid = {18190633},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
url = {http://arxiv.org/abs/1106.1813 http://dx.doi.org/10.1613/jair.953},
volume = {16},
year = {2011}
}
@incollection{Sambridge2011,
abstract = {Monte Carlo method. A computational technique making use of random numbers to solve problems that are either probabilistic or deterministic in nature. Named after the famous Casino in Monaco. Monte Carlo inversion method. A method for sampling a parameter space of variables representing unknowns, governed by probabilistic rules. Markov chain Monte Carlo (McMC). A probabilistic method for generating vectors or parameter variables whose values follow a prescribed density function.},
author = {Sambridge, Malcolm and Gallagher, Kerry},
booktitle = {Encyclopedia of Solid Earth Geophysics},
doi = {10.1007/978-90-481-8702-7_192},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Encyclopedia of Solid Earth Geophysics/Sambridge, Gallagher{\_}2011.pdf:pdf},
isbn = {978-90-481-8701-0},
pages = {639--644},
title = {{Inverse Theory, Monte Carlo Method}},
url = {http://link.springer.com/10.1007/978-90-481-8702-7{\_}192},
volume = {1},
year = {2011}
}
@article{Leonard2000,
author = {Leonard, M.},
doi = {10.1785/0120000026},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
month = {dec},
number = {6},
pages = {1384--1390},
title = {{Comparison of Manual and Automatic Onset Time Picking}},
url = {https://pubs.geoscienceworld.org/bssa/article/90/6/1384-1390/120571},
volume = {90},
year = {2000}
}
@article{Waldhauser2000,
abstract = {We have developed an efficient method to determine high-resolution hypocenter locations over large distances. The location method incorporates ordinary absolute travel-time measurements and/or cross-correlation P-and S-wave differential travel-time measurements. Residuals between observed and theoretical travel-time differences (or double-differences) are minimized for pairs of earthquakes at each station while linking together all observed event-station pairs. A least-squares solu- tion is found by iteratively adjusting the vector difference between hypocentral pairs. The double-difference algorithm minimizes errors due to unmodeled velocity struc- ture without the use of station corrections. Because catalog and cross-correlation data are combined into one system of equations, interevent distances within multiplets are determined to the accuracy of the cross-correlation data, while the relative lo- cations between multiplets and uncorrelated events are simultaneously determined to the accuracy of the absolute travel-time data. Statistical resampling methods are used to estimate data accuracy and location errors. Uncertainties in double-difference locations are improved by more than an order of magnitude compared to catalog locations. The algorithm is tested, and its performance is demonstrated on two clus- ters of earthquakes located on the northern Hayward fault, California. There it col- lapses the diffuse catalog locations into sharp images of seismicity and reveals hor- izontal lineations of hypocenters that define the narrow regions on the fault where stress is released by brittle failure.},
author = {Waldhauser, F. and Ellsworth, W. L.},
doi = {10.1785/0120000006},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Waldhauser, Ellsworth{\_}2000.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {double-difference,location},
mendeley-tags = {double-difference,location},
number = {6},
pages = {1353--1368},
title = {{A Double-difference Earthquake location algorithm: Method and application to the Northern Hayward Fault, California}},
volume = {90},
year = {2000}
}
@inproceedings{Aminzadeh2011,
abstract = {Earthquake data collected from the Geysers geothermal field from the period 2006 to 2010 was studied. An artificial neural network (ANN) based autopicker was developed to study and compare the picks with available autopickers in use at Lawrence Berkeley National Lab (LBNL) as well as in house autopicker at USC. The results indicate the following: 1. The ANN autopicker is able to generate good picks in most situations where other autopickers work. 2. The ANN autopicker is able to pick in situations where the other autopickers fail to generate good picks or do not generate any picks at all due to noise. 3. STA/LTA ratio and frequency based attributes show highest weight in the trained ANN. Despite the low misclassification error observed in the final results (3.7{\%}), the ANN autopicker failed in some situations. The next step is to incorporate improved attribute sets and look at a hybrid neuro-fuzzy approach to tide over the limitations of the present network.},
author = {Aminzadeh, Fred and Maity, Debotyam and Tafti, Tayeb A and Brouwer, Friso},
booktitle = {SEG Technical Program Expanded Abstracts 2011},
doi = {10.1190/1.3627514},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2011/Aminzadeh et al.{\_}2011.pdf:pdf},
issn = {10523812},
keywords = {attributes,geothermal,machine learning,microseismic,neural network,neural networks,noise,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
pages = {1623--1626},
publisher = {Society of Exploration Geophysicists},
title = {{Artificial neural network based autopicker for micro‐earthquake data}},
url = {http://library.seg.org/doi/abs/10.1190/1.3627514},
year = {2011}
}
@article{Got1994,
abstract = {Dense microearthquake swarms occur in the upper south flank of Kilauea, providing multiplets composed of hundreds of events. The similarity of their waveforms and the quality of the data have been sufficient to provide accurate relative relocations of their hypocenters. A simple and efficient method has been developed which allowed the relative relocation of more than 250 events with an average precision of about 50 m horizontally and 75 m vertically. Relocation of these events greatly improves the definition of the seismic image of the fault that generates them. Indeed, relative relocations define a plane dipping about 6° northward, although corresponding absolute locations are widely dispersed in the swarm. A composite focal mechanism, built from events providing a correct spatial sampling of the multiplet, also gives a well-constrained northward dip of about 5° to the near-horizontal plane. This technique thus collapses the clouds of hypocenters of single-event locations to a plane coinciding with the slip plane revealed by previous focal mechanism studies. We cannot conclude that all south flank earthquakes collapse to a single plane. There may locally be several planes, perhaps with different dips and depths throughout the south flank volume. The 6° northward-dipping plane we found is too steep to represent the overall flexure of the oceanic crust under the load of the island of Hawaii. This plane is probably an important feature that characterizes the basal slip layer below the upper south flank of Kilauea volcano. Differences in seismicity rate and surface deformations between the upper and lower south flank could be related to the geometry of this deep fault plane. The present work illustrates how high precision relative relocations of similar events in dense swarms, combined with the analysis of geodetic measurements, can help to describe deep fault plane geometry. Systematic selection and extensive relative relocation of similar earthquakes could be attempted in other well-instrumented, highly seismic areas to provide reliable basic information, especially useful for understanding of earthquake generation processes.},
author = {Got, J.-L. and Fr{\'{e}}chet, J. and Klein, Fred W.},
doi = {10.1029/94JB00577},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Got, Fr{\'{e}}chet, Klein{\_}1994.pdf:pdf},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
number = {B8},
pages = {15375--15386},
title = {{Deep fault plane geometry inferred from multiplet relative relocation beneath the south flank of Kilauea}},
volume = {99},
year = {1994}
}
@article{PianaAgostinetti2015,
abstract = {Local earthquake tomography is a non-linear and non-unique inverse problem that uses event arrival times to solve for the spatial distribution of elastic properties. The typical approach is to apply iterative linearization and derive a preferred solution, but such solutions are biased by a number of subjective choices: the starting model that is iteratively adjusted, the degree of regularization used to obtain a smooth solution, and the assumed noise level in the arrival time data. These subjective choices also affect the estimation of the uncertainties in the inverted parameters. The method presented here is developed in a Bayesian framework where a priori information and measurements are combined to define a posterior probability density of the parameters of interest: elastic properties in a subsurface 3-D model, hypocentre coordinates and noise level in the data. We apply a trans-dimensional Markov chain Monte Carlo algorithm that asymptotically samples the posterior distribution of the investigated parameters. This approach allows us to overcome the issues raised above. First, starting a number of sampling chains from random samples of the prior probability distribution lessens the dependence of the solution from the starting point. Secondly, the number of elastic parameters in the 3-D subsurface model is one of the unknowns in the inversion, and the parsimony of Bayesian inference ensures that the degree of detail in the solution is controlled by the information in the data, given realistic assumptions for the error statistics. Finally, the noise level in the data, which controls the uncertainties of the solution, is also one of the inverted parameters, providing a first-order estimate of the data errors. We apply our method to both synthetic and field arrival time data. The synthetic data inversion successfully recovers velocity anomalies, hypocentre coordinates and the level of noise in the data. The Bayesian inversion of field measurements gives results comparable to those obtained independently by linearized inversion, reconstructing the geometry of the main seismic velocity anomalies. The quantification of the posterior uncertainties, a crucial output of Bayesian inversion, allows for visualizing regions where elastic properties are closely constrained by the data and is used here to directly compare our results to the ones obtained with the linearized inversion. In the case we examined the results of two inversion techniques are not significantly different.},
author = {{Piana Agostinetti}, Nicola and Giacomuzzi, Genny and Malinverno, Alberto},
doi = {10.1093/gji/ggv084},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Piana Agostinetti, Giacomuzzi, Malinverno{\_}2015.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Crustal structure,Seismic tomography,location,mcmc},
mendeley-tags = {location,mcmc},
month = {apr},
number = {3},
pages = {1598--1617},
title = {{Local three-dimensional earthquake tomography by trans-dimensional Monte Carlo sampling}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggv084},
volume = {201},
year = {2015}
}
@article{Chen2016,
abstract = {We developed a Python phase identification package: the PhasePApy for earthquake data processing and near-real-time monitoring. The package takes advantage of the growing number of Python libraries including Obspy. All the data formats supported by Obspy can be supported within the PhasePApy. The PhasePApy has two subpackages: the PhasePicker and the Associator, aiming to identify phase arrival onsets and associate them to phase types, respectively. The PhasePicker and the As- sociator can work jointly or separately. Three autopickers are implemented in the PhasePicker subpackage: the frequency- band picker, the Akaike information criteria function derivative picker, and the kurtosis picker. All three autopickers identify picks with the same processing methods but different characteristic functions. The PhasePicker triggers the pick with a dynamic threshold and can declare a pick with false-pick filtering. Also, the PhasePicker identifies a pick polarity and uncertainty for further seismological analysis, such as focal mechanism determination. Two associators are included in the Associator subpackage: the 1D Associator and 3D Associator, which assign phase types to picks that can best fit potential earthquakes by minimizing root mean square (rms) residuals of the misfits in distance and time, respectively. The Associator processes multiple picks from all channels at a seismic station and aggregates them to increase computational efficiencies. Both associators use travel-time look up tables to determine the best estimation of the earthquake location and evaluate the phase type for picks. The PhasePApy package has been used extensively for local and regional earthquakes and can work for active source experiments as well.},
author = {Chen, Chen and Holland, Austin A.},
doi = {10.1785/0220160019},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Seismological Research Letters/Chen, Holland{\_}2016.pdf:pdf},
issn = {0895-0695},
journal = {Seismological Research Letters},
keywords = {picking},
mendeley-tags = {picking},
month = {nov},
number = {6},
pages = {1384--1396},
title = {{PhasePApy: A Robust Pure Python Package for Automatic Identification of Seismic Phases}},
url = {http://srl.geoscienceworld.org/lookup/doi/10.1785/0220160019 https://pubs.geoscienceworld.org/srl/article/87/6/1384-1396/314181},
volume = {87},
year = {2016}
}
@article{Brantut2011,
abstract = {Triaxial tests on gypsum polycrystal samples are performed at confining pressure (Pc) ranging from 2 to 95 MPa and temperatures up to 70°C. During the tests, stress, strain, elastic wave velocities, and acoustic emissions are recorded. At Pc {\textless}= 10 MPa, the macroscopic behavior is brittle, and above 20 MPa the macroscopic behavior becomes ductile. Ductile deformation is cataclastic, as shown by the continuous decrease of elastic wave velocities interpreted in terms of microcrack accumulation. Surprisingly, ductile deformation and strain hardening are also accompanied by small stress drops from 0.5 to 6 MPa in amplitude. Microstructural observations of the deformed samples suggest that each stress drop corresponds to the generation of a single shear band, formed by microcracks and kinked grains. At room temperature, the stress drops are not correlated to acoustic emssions (AEs). At 70°C, the stress drops are larger and systematically associated with a low-frequency AE (LFAE). Rupture velocities can be inferred from the LFAE high-frequency content and range from 50 to 200 m/s. The LFAE amplitude also increases with increasing rupture speed and is not correlated with the amplitude of the macroscopic stress drops. LFAEs are thus attributed to dynamic propagation of shear bands. In Volterra gypsum, the result of the competition between microcracking and plasticity is counterintuitive: Dynamic instalibilities at 70°C may arise from the thermal activation of mineral kinking.},
author = {Brantut, N. and Schubnel, A. and Gu{\'{e}}guen, Y.},
doi = {10.1029/2010JB007675},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Brantut, Schubnel, Gu{\'{e}}guen{\_}2011.pdf:pdf},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
month = {jan},
number = {B1},
pages = {B01404},
title = {{Damage and rupture dynamics at the brittle-ductile transition: The case of gypsum}},
url = {http://doi.wiley.com/10.1029/2010JB007675},
volume = {116},
year = {2011}
}
@article{Eberhart2001a,
abstract = {A fuzzy system is implemented to dynamically adapt the inertia weight of the particle swarm optimization algorithm (PSO). Three benchmark functions with asymmetric initial range settings are selected as the test functions. The same fuzzy system has been applied to all three test functions with different dimensions. The experimental results illustrate that the fuzzy adaptive PSO is a promising optimization method, which is especially useful for optimization problems with a dynamic environment},
author = {Eberhart, R.C.},
doi = {10.1109/CEC.2001.934377},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)/Eberhart{\_}2001.pdf:pdf},
isbn = {0-7803-6657-3},
issn = {1879-2472},
journal = {Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)},
keywords = {pso},
mendeley-tags = {pso},
pages = {101--106},
pmid = {22192158},
title = {{Fuzzy adaptive particle swarm optimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=934377},
volume = {1},
year = {2001}
}
@article{Akram2016,
abstract = {We have evaluated arrival-time picking algorithms for downhole microseismic data. The picking algorithms that we considered may be classified as window-based single-level methods (e.g., energy-ratio [ER] methods), nonwindow-based single-level methods (e.g., Akaike information criterion), multilevel-or array-based methods (e.g., crosscorrelation ap-proaches), and hybrid methods that combine a number of single-level methods (e.g., Akazawa's method). We have determined the key parameters for each algorithm and devel-oped recommendations for optimal parameter selection based on our analysis and experience. We evaluated the performance of these algorithms with the use of field examples from a downhole microseismic data set recorded in western Canada as well as with pseudo-synthetic microseismic data generated by adding 100 realizations of Gaussian noise to high signal-to-noise ratio microseismic waveforms. ER-based algorithms were found to be more efficient in terms of computational speed and were therefore recommended for real-time micro-seismic data processing. Based on the performance on pseudo-synthetic and field data sets, we found statistical, hybrid, and multilevel crosscorrelation methods to be more efficient in terms of accuracy and precision. Pick errors for S-waves are reduced significantly when data are preconditioned by ap-plying a transformation into ray-centered coordinates.},
author = {Akram, Jubran and Eaton, David W},
doi = {10.1190/geo2014-0500.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Akram, Eaton{\_}2016.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {picking},
mendeley-tags = {picking},
month = {mar},
number = {2},
pages = {KS71--KS91},
title = {{A review and appraisal of arrival-time picking methods for downhole microseismic data}},
url = {http://library.seg.org/doi/10.1190/geo2014-0500.1},
volume = {81},
year = {2016}
}
@inproceedings{Shahzad2009,
abstract = {This paper presents an Opposition-based PSO(OVCPSO) which uses Velocity Clamping to accelerate its convergence speed and to avoid premature convergence of algorithm. Probabilistic opposition-based learning for particles has been used in the proposed method which uses velocity clamping to control the speed and direction of particles. Experiments have been performed upon various well known benchmark optimization problems and results have shown that OVCPSO can deal with difficult unimodal and multimodal optimization problems efficiently and effectively. The numbers of function calls (NFC) are significantly less than other PSO variants i.e. basic PSO with inertia weight, PSO with inertia weight and velocity clamping (VCPSO) and opposition based PSO with Cauchy Mutation (OPSOCM).},
author = {Shahzad, Farrukh and Baig, A. Rauf and Masood, Sohail and Kamran, Muhammad and Naveed, Nawazish},
booktitle = {Advances in Computational Intelligence},
doi = {10.1007/978-3-642-03156-4},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Advances in Computational Intelligence/Shahzad et al.{\_}2009.pdf:pdf},
isbn = {978-3-642-03156-4},
keywords = {opposite number,optimization,pso,swarm intelligence},
number = {1},
pages = {339--348},
title = {{Opposition-Based Particle Swarm Optimization with Velocity Clamping (OVCPSO)}},
url = {http://link.springer.com/10.1007/978-3-642-03156-4{\_}34},
year = {2009}
}
@article{Socco2008,
abstract = {Inversion of surface wave data suffers from solution non-uniqueness and is hence strongly biased by the initial model. The Monte Carlo approach can handle this non-uniqueness by evidencing the local minima but it is inefficient for high dimensionality problems and makes use of subjective criteria, such as misfit thresholds, to interpret the results. If a smart sampling of the model parameter space, which exploits scale properties of the modal curves, is introduced the method becomes more efficient and with respect to traditional global search methods it avoids the subjective use of control parameters that are barely related to the physical problem. The results are interpreted drawing inference by means of a statistical test that selects an ensemble of feasible shear wave velocity models according to data quality and model parameterization. Tests on synthetic data demonstrate that the application of scale properties concentrates the sampling of model parameter space in high probability density zones and makes it poorly sensitive to the initial boundary of the model parameters. Tests on synthetic and field data, where boreholes are available, prove that the statistical test selects final results that are consistent with the true model and which are sensitive to data quality. The implemented strategies make the Monte Carlo inversion efficient for practical applications and able to effectively retrieve subsoil models even in complex and challenging situations such as velocity inversions.},
author = {Socco, Laura Valentina and Boiero, Daniele},
doi = {10.1111/j.1365-2478.2007.00678.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Socco, Boiero{\_}2008.pdf:pdf},
isbn = {00168025},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {mcmc},
mendeley-tags = {mcmc},
number = {3},
pages = {357--371},
title = {{Improved Monte Carlo inversion of surface wave data}},
volume = {56},
year = {2008}
}
@article{Schmidt1986,
author = {Schmidt, Ralph},
issn = {0018-926X},
journal = {IEEE transactions on antennas and propagation},
number = {3},
pages = {276--280},
publisher = {IEEE},
title = {{Multiple emitter location and signal parameter estimation}},
volume = {34},
year = {1986}
}
@article{Hu2016,
abstract = {As a powerful simulator with input files in fixed-format formats, the capabilities of TOUGH2 simulator urge programmers to develop the pre- and post-processing programs. A new program (IGMESH) with Graphical User Interface (GUI) is introduced. The elements for spatial discrezation are classified into domain bound, boundary for refinement, well, fault, drift and free point, which will be discreted into a series of points. The Voronoi tessellation method is employed to generate Voronoi diagrams in the plane and the relation of neighbor points in a polygon is obtained from the geometric relationship of Voronoi diagrams. Three-dimensional mesh is built based on top elevation and thickness of each model layer. IGMESH provides functions for rock type assignment, boundary conditions, interpolation method of elevation and thickness, simulation results conversion and visualization with TECPLOT software. The case studies in the Beishan area demonstrate the applicability of the approach. IGMESH software has shown to be adequate to build quasi-3D unstructured grids from the beginning of numerical model build to the results analysis, and thus will facilitate the application of TOUGH2 simulator.},
author = {Hu, Litang and Zhang, Keni and Cao, Xiaoyuan and Li, Yi and Guo, Chaobin},
doi = {10.1016/j.cageo.2016.06.014},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Hu et al.{\_}2016.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Irregular grid,Pre- and post processing,TOUGH2,The Beishan area,meshmaker,tough2},
mendeley-tags = {meshmaker,tough2},
month = {oct},
pages = {11--17},
title = {{IGMESH: A convenient irregular-grid-based pre- and post-processing tool for TOUGH2 simulator}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300416301637},
volume = {95},
year = {2016}
}
@article{Zhang1998,
abstract = {A few important issues for performing nonlinear refraction traveltime tomography have been identified. They include the accuracy of the traveltime and raypath calculations for refraction, the physical information in the refraction traveltime curves, and the characteristics of the refraction traveltime errors. Consequently, we develop a shortest path ray-tracing method with an optimized node distribution that can calculate refraction traveltimes and raypaths accurately in any velocity model. We find that structure ambiguity caused by short and long rays in the seismic refraction method may influence the inversion solution significantly. Therefore, we pose a nonlinear inverse problem that explicitly minimizes the misfits of the average slownesses (ratios of traveltimes to the corresponding ray lengths) and the apparent slownesses (derivatives of traveltimes with respect to distance). As a result, we enhance the resolution as well as the convergence speed. To regularize our inverse problem, we use the Tikhonov method to avoid solving an ill-posed inverse problem. Errors in refraction traveltimes are characterized in terms of a common-shot error, a constant deviation for recordings from the same shot, and a relative traveltime-gradient error with zero mean with respect to the true gradient of the traveltime curve. Therefore, we measure the uncertainty of our tomography solution using a nonlinear Monte Carlo approach and compute the posterior model covariance associated with two different types of random data vectors and one random model vector. The nonlinear uncertainty analysis indicates that the resolution of a tomography solution may not correspond to the ray coverage. We apply this tomography technique to image the shallow velocity structure at a coastal site near Boston, Massachusetts. The results are consistent with a subsequent drilling survey.},
author = {Zhang, Jie and Toks{\"{o}}z, M. Nafi},
doi = {10.1190/1.1444468},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Zhang, Toks{\"{o}}z{\_}1998.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
month = {sep},
number = {5},
pages = {1726--1737},
title = {{Nonlinear refraction traveltime tomography}},
url = {https://library.seg.org/doi/10.1190/1.1444468},
volume = {63},
year = {1998}
}
@inproceedings{Piccand2008,
author = {Piccand, Sebastien and O'Neill, Michael and Walker, Jacqueline},
booktitle = {2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)},
doi = {10.1109/CEC.2008.4631134},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)/Piccand, O'Neill, Walker{\_}2008.pdf:pdf},
isbn = {978-1-4244-1822-0},
keywords = {pso},
mendeley-tags = {pso},
month = {jun},
pages = {2505--2512},
publisher = {IEEE},
title = {{On the scalability of particle swarm optimisation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4631134 http://ieeexplore.ieee.org/document/4631134/},
year = {2008}
}
@article{Maeda1985,
author = {Maeda, Naoki},
doi = {10.4294/zisin1948.38.3_365},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Zisin (Journal of the Seismological Society of Japan. 2nd ser.)/Maeda{\_}1985.pdf:pdf},
issn = {0037-1114},
journal = {Zisin (Journal of the Seismological Society of Japan. 2nd ser.)},
number = {3},
pages = {365--379},
title = {{A Method for Reading and Checking Phase Time in Auto-Processing System of Seismic Wave Data}},
url = {https://scholar.google.com/scholar{\_}lookup?title=A method for reading and checking phase times in auto-processing system of seismic data{\&}author=N. Maeda{\&}publication{\_}year=1985{\&}journal=Zisin{\%}3DJishin{\&}volume=38{\&}pages=365-379 https://www.jstage.jst.go.jp/artic},
volume = {38},
year = {1985}
}
@article{Durand2018a,
abstract = {We present SeisTomoPy, a new Python tool that facilitates the use of a suite of tomographic models available to the public, with a single program. We placed particular emphasis on providing a tool that will be freely available on a GitHub platform and that is based on free software, Python and Fortran. SeisTomoPy provides six tools that allow the user to visualize tomographic models, compare them, and extract information for further scientific purposes. The tool comes with a graphical interface with intuitive buttons and simple parameters but the same information can also be gained using the Python classes that can be run routinely in Python scripts. This tool is suitable for global and spherical tomographic models and is provided with a default list of eight recent tomographic models. However, the users can also upload their own model if desired. By facilitating the wider use of tomographic models, SeisTomoPy aims at encouraging a wider community of geophysicists to explore tomographic models in more detail.},
author = {Durand, S. and Abreu, R. and Thomas, C.},
doi = {10.1785/0220170142},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Seismological Research Letters/Durand, Abreu, Thomas{\_}2018(3).pdf:pdf},
isbn = {0220170142},
issn = {0895-0695},
journal = {Seismological Research Letters},
month = {mar},
number = {2A},
pages = {658--667},
title = {{SeisTomoPy: Fast Visualization, Comparison, and Calculations in Global Tomographic Models}},
url = {https://pubs.geoscienceworld.org/srl/article-lookup?doi=10.1785/0220170142 https://pubs.geoscienceworld.org/ssa/srl/article/89/2A/658/525825/SeisTomoPy-Fast-Visualization-Comparison-and},
volume = {89},
year = {2018}
}
@incollection{Yang2010,
author = {Yang, Xin-She},
booktitle = {Nature inspired cooperative strategies for optimization (NICSO 2010)},
pages = {65--74},
publisher = {Springer},
title = {{A new metaheuristic bat-inspired algorithm}},
year = {2010}
}
@article{Hansen2003,
author = {Hansen, Nikolaus and M{\"{u}}ller, Sibylle D and Koumoutsakos, Petros},
issn = {1063-6560},
journal = {Evolutionary computation},
number = {1},
pages = {1--18},
publisher = {MIT Press},
title = {{Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)}},
volume = {11},
year = {2003}
}
@article{Maxwell2009,
abstract = {Microseismic imaging is a common technique for mapping hydraulic fractures. Accurate imaging of the source location of the microseisms is critical for accurate fracture mapping. In this paper, two simple synthetic examples are presented which demonstrate the mislocation that could result from data and velocity model errors. Location errors resulting from intentionally processing with inaccurate velocity models are presented to highlight an important quality control plot of arrival time differences that can be used to identify systematic apparent velocity errors in the arrival time moveout. This provides a simple velocity model quality control for interpretation by non-specialists and offers the possibility of velocity model inversion and quantification of velocity model errors for sensitivity tests. Monte Carlo simu- lations are also used to demonstrate random location uncer- tainties resulting from input data uncertainties in arrival times and raypath polarizations. The results highlight the interaction between mislocations from both input data random uncertainties and systematic velocity model errors. The errors are also shown to significantly change with geophone array geometry, and demonstrate the possibility of optimized sensor placement to improve location accuracy. For example, modeling of the Horn River Basin in NE BC, illustrates significantly improved accuracy with certain geophone locations.},
author = {Maxwell, Shawn C.},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/CSEG Recorder/Maxwell{\_}2009.pdf:pdf},
journal = {CSEG Recorder},
keywords = {location},
mendeley-tags = {location},
number = {4},
pages = {41--46},
title = {{Microseismic Location Uncertainty}},
volume = {34},
year = {2009}
}
@article{Trojanowski2017,
abstract = {Microseismic monitoring in the oil and gas industry commonly uses migration-based methods to locate very weak microseismic events. The objective of this study is to compare the most popular migration-based methods on a synthetic dataset that simulates a strike-slip source mechanism event with a low signal-to-noise ratio recorded by surface receivers (vertical components). The results show the significance of accounting for the known source mechanism in the event detection and location procedures. For detection and location without such a correction, the ability to detect weak events is reduced. We show both numerically and theoretically that neglecting the source mechanism by using only absolute values of the amplitudes reduces noise suppression during stacking and, consequently, limits the possibility to retrieve weak microseismic events. On the other hand, even a simple correction to the data polarization used with otherwise ineffective methods can significantly improve detections and locations. A simple stacking of the data with a polarization correction provided clear event detection and location, but even better results were obtained for those data combined with methods that are based on semblance and cross-correlation.},
author = {Trojanowski, Jacek and Eisner, Leo},
doi = {10.1111/1365-2478.12366},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Trojanowski, Eisner{\_}2017.pdf:pdf},
isbn = {1365-2478},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Microseismic monitoring,Signal-to-noise ratio,Stacking,location,migration},
mendeley-tags = {location,migration},
month = {jan},
number = {1},
pages = {47--63},
title = {{Comparison of migration-based location and detection methods for microseismic events}},
url = {http://doi.wiley.com/10.1111/1365-2478.12366},
volume = {65},
year = {2017}
}
@article{Beleites2013,
abstract = {In biospectroscopy, suitably annotated and statistically independent samples (e.g. patients, batches, etc.) for classifier training and testing are scarce and costly. Learning curves show the model performance as function of the training sample size and can help to determine the sample size needed to train good classifiers. However, building a good model is actually not enough: the performance must also be proven. We discuss learning curves for typical small sample size situations with 5-25 independent samples per class. Although the classification models achieve acceptable performance, the learning curve can be completely masked by the random testing uncertainty due to the equally limited test sample size. In consequence, we determine test sample sizes necessary to achieve reasonable precision in the validation and find that 75-100 samples will usually be needed to test a good but not perfect classifier. Such a data set will then allow refined sample size planning on the basis of the achieved performance. We also demonstrate how to calculate necessary sample sizes in order to show the superiority of one classifier over another: this often requires hundreds of statistically independent test samples or is even theoretically impossible. We demonstrate our findings with a data set of ca. 2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and three tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive simulation that allows precise determination of the actual performance of the models in question. {\textcopyright} 2012 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1211.1323},
author = {Beleites, Claudia and Neugebauer, Ute and Bocklitz, Thomas and Krafft, Christoph and Popp, J{\"{u}}rgen},
doi = {10.1016/j.aca.2012.11.007},
eprint = {1211.1323},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Analytica Chimica Acta/Beleites et al.{\_}2013.pdf:pdf},
isbn = {1873-4324 (Electronic)$\backslash$r0003-2670 (Linking)},
issn = {00032670},
journal = {Analytica Chimica Acta},
keywords = {Classification,Design of experiments,Learning curve,Multivariate,Small sample size,Training,Validation,machine learning},
mendeley-tags = {machine learning},
month = {jan},
pages = {25--33},
pmid = {23265730},
title = {{Sample size planning for classification models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0003267012016479},
volume = {760},
year = {2013}
}
@article{Sorensen2015,
abstract = {In recent years, the field of combinatorial optimization has witnessed a true tsunami of " novel " metaheuristic methods, most of them based on a metaphor of some natural or man-made process. The behavior of virtually any species of insects, the flow of water, musicians playing together – it seems that no idea is too far-fetched to serve as inspiration to launch yet another metaheuristic. In this paper, we will argue that this line of research is threatening to lead the area of metaheuristics away from scientific rigor. We will examine the historical context that gave rise to the increasing use of metaphors as inspiration and justification for the development of new methods, discuss the reasons for the vulnerability of the metaheuristics field to this line of research, and point out its fallacies. At the same time, truly innovative research of high quality is being performed as well. We conclude the paper by discussing some of the properties of this research and by pointing out some of the most promising research avenues for the field of metaheuristics.},
author = {S{\"{o}}rensen, Kenneth},
doi = {10.1111/itor.12001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Transactions in Operational Research/S{\"{o}}rensen{\_}2015.pdf:pdf},
isbn = {1475-3995},
issn = {09696016},
journal = {International Transactions in Operational Research},
keywords = {Combinatorial optimization,Heuristics,Metaheuristics,Optimization},
month = {jan},
number = {1},
pages = {3--18},
title = {{Metaheuristics-the metaphor exposed}},
url = {http://doi.wiley.com/10.1111/itor.12001},
volume = {22},
year = {2015}
}
@article{Sigaud2013,
abstract = {La r{\'{e}}solution de probl{\`{e}}mes {\`{a}} {\'{e}}tats et actions continus par l'optimisation de politiques param{\'{e}}triques est un sujet d'int{\'{e}}r{\^{e}}t r{\'{e}}cent en apprentissage par renforcement. L'algorithme PI2 est un exemple de cette approche, qui b{\'{e}}n{\'{e}}ficie de fondements math{\'{e}}matiques solides tir{\'{e}}s de la commande stochastique optimale et des outils de la th{\'{e}}orie de l'estimation statistique. Dans cet article, nous consid{\'{e}}rons PI2 en tant que membre de la famille plus vaste des m{\'{e}}thodes qui partagent le concept de moyenne pond{\'{e}}r{\'{e}}e par les probabilit{\'{e}}s pour mettre {\`{a}} jour it{\'{e}}rativement des param{\`{e}}tres afin d'optimiser une fonction de co{\^{u}}t. Nous comparons PI2 {\`{a}} d'autres membres de la m{\^{e}}me famille - la " m{\'{e}}thode d'entropie crois{\'{e}}e " et CMA-ES 1 - au niveau conceptuel et en termes de performance. La comparaison d{\'{e}}bouche sur la d{\'{e}}rivation d'un nouvel algorithme que nous appelons PI2-CMA pour " Path Integral Policy Improvement with Covariance Matrix Adaptation ". Le principal avantage de PI2-CMA est qu'il d{\'{e}}termine l'amplitude du bruit d'exploration automatiquement.},
author = {Sigaud, Olivier and Stulp, Freek},
doi = {10.3166/ria.27.243-263},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Revue d'intelligence artificielle/Sigaud, Stulp{\_}2013.pdf:pdf},
issn = {0992499X},
journal = {Revue d'intelligence artificielle},
keywords = {Covariance matrix adaptation,Cross-entropy,Policy improvement,cmaes},
mendeley-tags = {cmaes},
month = {apr},
number = {2},
pages = {243--263},
title = {{Adaptation de la matrice de covariance pour l'apprentissage par renforcement direct}},
url = {http://ria.revuesonline.com/article.jsp?articleId=18378},
volume = {27},
year = {2013}
}
@article{Boschetti1996,
author = {Boschetti, Fabio and Dentith, Mike C and List, Ron D},
doi = {10.1190/1.1444089},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Boschetti, Dentith, List{\_}1996.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
month = {nov},
number = {6},
pages = {1715--1727},
title = {{Inversion of seismic refraction data using genetic algorithms}},
url = {http://library.seg.org/doi/10.1190/1.1444089},
volume = {61},
year = {1996}
}
@article{Angeline1998,
abstract = {This paper investigates the philosophical and performance differences of particle swarm and evolutionary optimization. The method of processing employed in each technique are first reviewed followed by a summary of their philosophical differences. Comparison experiments involving four non-linear functions well studied in the evolutionary optimization literature are used to highlight some performance differences between the techniques.},
author = {Angeline, Peter J},
doi = {10.1007/BFb0040811},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Lecture Notes in Computer Science Evolutionary Programming VII/Angeline{\_}1998.pdf:pdf},
isbn = {3540648917},
issn = {16113349},
journal = {Lecture Notes in Computer Science: Evolutionary Programming VII},
keywords = {pso},
mendeley-tags = {pso},
pages = {601--610},
title = {{Evolutionary optimization versus particle swarm optimization: Philosophy and performance differences}},
volume = {1447},
year = {1998}
}
@inproceedings{Eberhart2000,
abstract = {The performance of particle swarm optimization using an inertia weight is compared with performance using a constriction factor. Five benchmark functions are used for the comparison. It is concluded that the best approach is to use the constriction factor while limiting the maximum velocity Vmax to the dynamic range of the variable Xmax on each dimension. This approach provides performance on the benchmark functions superior to any other published results known by the authors},
author = {Eberhart, R.C. and Shi, Y.},
booktitle = {Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)},
doi = {10.1109/CEC.2000.870279},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512)/Eberhart, Shi{\_}2000.pdf:pdf},
isbn = {0-7803-6375-2},
issn = {13272314},
keywords = {pso},
mendeley-tags = {pso},
number = {7},
pages = {84--88},
publisher = {IEEE},
title = {{Comparing inertia weights and constriction factors in particle swarm optimization}},
url = {http://ieeexplore.ieee.org/ielx5/6997/18852/00870279.pdf?tp={\&}arnumber=870279{\&}isnumber=18852{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=870279{\&}tag=1 http://ieeexplore.ieee.org/document/870279/},
volume = {1},
year = {2000}
}
@article{Michelini2004,
abstract = {Michelini and Lomax [2004] (hereinafter ML2004) make some statements regarding the double-difference (DD) tomography method of Zhang and Thurber [2003] (hereinafter ZT2003) that are incorrect or misleading. In this comment, we indicate the ways in which ML2004 misrepresent characteristics of ZT2003's DD tomography algorithm and the associated code tomoDD. In the process, we clarify the ways in which tomoDD differs from the DD location code hypoDD of Waldhauser [2001] and Waldhauser and Ellsworth [2000] (hereinafter WE2000).},
author = {Michelini, A. and Lomax, A.},
doi = {10.1029/2004GL019682},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Research Letters/Michelini, Lomax{\_}2004.pdf:pdf},
isbn = {0094-8276},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {doi:10.102,http://dx.doi.org/10.1029/2004GL019682,location},
mendeley-tags = {location},
number = {9},
pages = {1--4},
title = {{The effect of velocity structure errors on double-difference earthquake location}},
volume = {31},
year = {2004}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is $\Theta$(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
doi = {10.1145/1283383.1283494},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms/Arthur, Vassilvitskii{\_}2007.pdf:pdf},
isbn = {9780898716245},
issn = {978-0-898716-24-5},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
pages = {1027--1025},
pmid = {1000164511},
title = {k-means++: the advantages of careful seeding},
year = {2007}
}
@article{Rabenseifner2003,
abstract = {Most HPC systems are clusters of shared memory nodes. Parallel programming$\backslash$nmust combine the distributed memory parallelization on the node interconnect$\backslash$nwith the hared memory parallelization inside of each node. Various$\backslash$nhybrid MPI+OpenMP programming models are compared with pure MPI.$\backslash$nBenchmark results of several platforms are presented. This paper$\backslash$nanalyzes the strength and weakness of several parallel programming$\backslash$nmodels on clusters of SMP nodes. Benchmark results on a Myrinet cluster$\backslash$nand on reent Cray, NEC, IBM, Hitachi, SUN and SGI platforms how,$\backslash$nthat the hybrid-masteronly programming model can be used more efficiently$\backslash$non some vector-type systems, but also on clusters of dual-CPUs. On$\backslash$nother systems, one CPU is not able to saturate the inter-node network$\backslash$nand he commonly used masteronly programming model suffers rom insufficient$\backslash$ninter-node bandwidth. This paper analyses strategies to overcome$\backslash$ntypical drawbacks of this easily usable programming scheme on systems$\backslash$nwith weaker interconnects. Best performance can be achieved with$\backslash$noverlapping communication and computation, but this scheme is acking$\backslash$nin ease of use.},
author = {Rabenseifner, Rolf},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/5th European Workshop on OpenMP/Rabenseifner{\_}2003.pdf:pdf},
journal = {5th European Workshop on OpenMP},
keywords = {e,efficient way,hpc,hybrid parallel programming,i,induced by the programming,mpi,openmp,performance,scheme,threads and mpi,ware in a most,without any overhead},
pages = {185--194},
title = {{Hybrid Parallel Programming on HPC Platforms}},
year = {2003}
}
@article{Eisner2009,
author = {Eisner, Leo and Duncan, Peter M. and Heigl, Werner M. and Keller, William R.},
doi = {10.1190/1.3148403},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The Leading Edge/Eisner et al.{\_}2009.pdf:pdf},
issn = {1070-485X},
journal = {The Leading Edge},
month = {jun},
number = {6},
pages = {648--655},
title = {{Uncertainties in passive seismic monitoring}},
url = {http://tle.geoscienceworld.org/cgi/content/abstract/28/6/648{\%}5Cnhttp://tle.geoscienceworld.org/content/28/6/648.short http://library.seg.org/doi/10.1190/1.3148403},
volume = {28},
year = {2009}
}
@article{Rutqvist2016,
author = {Rutqvist, Jonny and Jeanne, Pierre and Dobson, Patrick F and Garcia, Julio and Hartline, Craig and Hutchings, Lawrence and Singh, Ankit and Vasco, Donald W and Walters, Mark},
doi = {10.1016/j.geothermics.2015.08.002},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geothermics/Rutqvist et al.{\_}2016.pdf:pdf},
issn = {03756505},
journal = {Geothermics},
keywords = {flac,tough},
mendeley-tags = {flac,tough},
month = {sep},
pages = {120--138},
title = {{The Northwest Geysers EGS Demonstration Project, California – Part 2: Modeling and interpretation}},
url = {http://dx.doi.org/10.1016/j.geothermics.2015.08.002 https://linkinghub.elsevier.com/retrieve/pii/S0375650515001030},
volume = {63},
year = {2016}
}
@article{Sainsbury2017,
abstract = {Anisotropic rock masses, the behavior of which is dominated by closely spaced planes of weakness, present particular difficulties in rock engineering analyses. The orientation of discontinuities relative to an excavation face has a significant influence on the behavioral response. At the present time, discontinuum modeling techniques provide the most rigorous analyses of the deformation and failure processes of anisotropic rock masses. However, due to their computational efficiency continuum analyses are routinely used to represent laminated materials through the implementation of a Ubiquitous-Joint model. The problem with Ubiquitous-Joint models is that they do not consider the effects of joint spacing, length and stiffness. As such, without an understanding of the limitations of the modeling approach and detailed calibration of the material response, simulation results can be misleading. This paper provides a framework to select and validate ubiquitous-joint constitutive properties.},
author = {Sainsbury, B. L. and Sainsbury, D. P.},
doi = {10.1007/s00603-017-1177-3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Rock Mechanics and Rock Engineering/Sainsbury, Sainsbury{\_}2017.pdf:pdf},
issn = {0723-2632},
journal = {Rock Mechanics and Rock Engineering},
keywords = {Anisotropic,Discontinuum,Numerical simulation,Subiquitous,Ubiquitous-joint},
month = {jun},
number = {6},
pages = {1507--1528},
publisher = {Springer Vienna},
title = {{Practical Use of the Ubiquitous-Joint Constitutive Model for the Simulation of Anisotropic Rock Masses}},
url = {http://link.springer.com/10.1007/s00603-017-1177-3},
volume = {50},
year = {2017}
}
@article{Vidale1988,
abstract = {The travel times of the first arriving seismic waves through any velocity structure can be rapidly computed on a two- or three-dimensional numerical grid by finite-difference extrapolation from point to point. Wavefronts, rather than the traditional rays, are tracked. Head waves are properly treated and shadow zones are filled by the appropriate diffractions. Differences of less than 0.03 per cent are found between the results of this technique and raytracing for a complex, two-dimensional model. This scheme is useful for the windowing of finite-difference calculations to increase computing speed, and promises to aid in earthquake location, tomographic inversion, and Kirchhoff migration in structures that have strong lateral velocity gradients.},
author = {Vidale, John},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
month = {dec},
number = {6},
pages = {2062--2076},
title = {{Finite-difference calculation of travel times}},
url = {http://dx.doi.org/},
volume = {78},
year = {1988}
}
@article{Schoberl1997,
abstract = {In this paper, the algorithms of the automatic mesh generator NETGEN are described. The domain is provided by a Constructive Solid Geometry (CSG). The whole task of 3D mesh generation splits into four subproblems of special point calculation, edge following, surface meshing and finally volume mesh generation. Surface and volume mesh generation are based on the advancing front method. Emphasis is given to the abstract structure of the element generation rules. Several techniques of mesh optimization are tested and quality plots are presented.},
author = {Sch{\"{o}}berl, Joachim},
doi = {10.1007/s007910050004},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computing and Visualization in Science/Sch{\"{o}}berl{\_}1997.pdf:pdf},
issn = {1432-9360},
journal = {Computing and Visualization in Science},
keywords = {mesh},
mendeley-tags = {mesh},
month = {jul},
number = {1},
pages = {41--52},
title = {{NETGEN An advancing front 2D/3D-mesh generator based on abstract rules}},
url = {http://link.springer.com/10.1007/s007910050004},
volume = {1},
year = {1997}
}
@book{Neal1996,
address = {New York, NY},
author = {Neal, Radford M},
doi = {10.1007/978-1-4612-0745-0},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Neal{\_}1996.pdf:pdf},
isbn = {978-0-387-94724-2},
pages = {341},
publisher = {Springer New York},
series = {Lecture Notes in Statistics},
title = {{Bayesian Learning for Neural Networks}},
url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
volume = {118},
year = {1996}
}
@article{Neiswanger2013,
abstract = {Communication costs, resulting from synchronization requirements during learning, can greatly slow down many parallel machine learning algorithms. In this paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in which subsets of data are processed independently, with very little communication. First, we arbitrarily partition data onto multiple machines. Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be used to draw samples from a posterior distribution given the data subset. Finally, the samples from each machine are combined to form samples from the full posterior. This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data (without communication) until the final combination stage. We prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models.},
archivePrefix = {arXiv},
arxivId = {1311.4780},
author = {Neiswanger, Willie and Wang, Chong and Xing, Eric},
eprint = {1311.4780},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Neiswanger, Wang, Xing{\_}2013.pdf:pdf},
isbn = {9780974903910},
number = {1},
pages = {1--16},
title = {{Asymptotically Exact, Embarrassingly Parallel MCMC}},
url = {http://arxiv.org/abs/1311.4780},
year = {2013}
}
@inproceedings{Storn2017,
author = {Storn, Rainer},
booktitle = {2017 IEEE Congress on Evolutionary Computation (CEC)},
doi = {10.1109/CEC.2017.7969387},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2017 IEEE Congress on Evolutionary Computation (CEC)/Storn{\_}2017.pdf:pdf},
isbn = {978-1-5090-4601-0},
keywords = {Communications,Differential Evolution,Evolutionary Computation,Measurement,Real-world applications,de},
mendeley-tags = {de},
month = {jun},
number = {3},
pages = {765--772},
publisher = {IEEE},
title = {{Real-world applications in the communications industry - when do we resort to Differential Evolution?}},
url = {http://ieeexplore.ieee.org/document/7969387/},
year = {2017}
}
@article{Ostermeier1994,
abstract = {The performance of Evolution Strategies (ESs) depends on a suitable choice of internal strategy control parameters. Apart from a fixed setting, ESs facilitate an adjustment of such parameters within a self-adaptation process. For step-size control in particular, various adaptation concepts were evolved early in the development of ESs. These algorithms mostly work very efficiently as long as the relative sensitivities of the parameters to be optimized are known. If this scaling is not known, the strategy has to adapt individual step-sizes for the parameters. In general, the number of necessary step-sizes (variances) equals the dimension of the problem. In this case, step-size adaptation proves to be difficult.},
author = {Ostermeier, A and Gawelczyk, A and Hansen, N},
doi = {10.1007/3-540-58484-6_263},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Lecture Notes in Computer Science, Vol. 866/Ostermeier, Gawelczyk, Hansen{\_}1994.pdf:pdf},
isbn = {3-540-58484-6},
journal = {Lecture Notes in Computer Science, Vol. 866},
keywords = {adaptation,control,evolution strategy,individual step-size,introduction,mutative step-size,scaling,self-adaptation,step-size,step-size adaptation in ess},
pages = {282--291},
title = {{Step-size adaptation based on non-local use of selection information}},
year = {1994}
}
@article{Geuzaine2013,
author = {Geuzaine, Christophe and Remacle, Jean-Fran{\c{c}}ois},
doi = {10.1002/nme.2579},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal for Numerical Methods in Engineering/Geuzaine, Remacle{\_}2009.pdf:pdf},
issn = {00295981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {computer-aided design,finite element method,mesh generation,open-,post-processing},
month = {sep},
number = {11},
pages = {1309--1331},
title = {{Gmsh: A 3-D finite element mesh generator with built-in pre- and post-processing facilities}},
url = {http://doi.wiley.com/10.1002/nme.2579},
volume = {79},
year = {2009}
}
@article{Husen2010,
abstract = {Earthquake location catalogs are not an exact representation of the true earthquake locations. They contain random error, for example from errors in the arrival time picks, as well as systematic biases. The most important source of systematic errors in earthquake locations is the inherent dependence of earthquake locations on the assumed seismic velocity structure of the Earth. Random errors may be accounted for in formal uncertainty estimates, but systematic biases are not, and they must be considered based on knowledge about how the earthquakes were located. In this article we discuss earthquake location methods and methods for estimating formal uncertainties; we consider systematic biases in earthquake location catalogs; and we give readers guidance on how to identify good-quality earthquake locations.},
author = {Husen, S and Hardebeck, J},
doi = {10.5078/corssa-55815573},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Community Online Resource for Statistical Seismicity Analysis/Husen, Hardebeck{\_}2010.pdf:pdf},
journal = {Community Online Resource for Statistical Seismicity Analysis},
keywords = {Earthquake location accuracy,location},
mendeley-tags = {location},
number = {September},
pages = {1--35},
title = {{Understanding Seismicity Catalogs and their Problems}},
year = {2010}
}
@article{Tran2016,
abstract = {TOUGH2 and iTOUGH2 are powerful models that simulate the heat and fluid flows in porous and fracture media, and perform parameter estimation, sensitivity analysis and uncertainty propagation analysis. However, setting up the input files is not only tedious, but error prone, and processing output files is time consuming. In this study, we present an open source Matlab-based tool (iMatTOUGH) that supports the generation of all necessary inputs for both TOUGH2 and iTOUGH2 and visualize their outputs. The tool links the inputs of TOUGH2 and iTOUGH2, making sure the two input files are consistent. It supports the generation of rectangular computational mesh, i.e., it automatically generates the elements and connections as well as their properties as required by TOUGH2. The tool also allows the specification of initial and time-dependent boundary conditions for better subsurface heat and water flow simulations. The effectiveness of the tool is illustrated by an example that uses TOUGH2 and iTOUGH2 to estimate soil hydrological and thermal properties from soil temperature data and simulate the heat and water flows at the Rifle site in Colorado.},
author = {Tran, Anh Phuong and Dafflon, Baptiste and Hubbard, Susan},
doi = {10.1016/j.cageo.2016.02.006},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Tran, Dafflon, Hubbard{\_}2016.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {ITOUGH2,Initial and boundary conditions,Matlab-based GUI,Pre- and post-processing,TOUGH2,Visualization,tough2},
mendeley-tags = {tough2},
month = {apr},
pages = {132--143},
title = {{iMatTOUGH: An open-source Matlab-based graphical user interface for pre- and post-processing of TOUGH2 and iTOUGH2 models}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300416300310},
volume = {89},
year = {2016}
}
@inproceedings{McKinney2010,
author = {McKinney, Wes},
booktitle = {Proceedings of the 9th Python in Science Conference},
pages = {51--56},
title = {{Data Structures for Statistical Computing in Python}},
year = {2010}
}
@article{FernandezMartinez2012,
abstract = {History matching provides to reservoir engineers an improved spatial distribution of physical properties to be used in forecasting the reservoir response for field management. The ill-posed character of the history-matching problem yields nonuniqueness and numerical instabilities that increase with the reservoir complexity. These features might cause local optimization methods to provide unpredictable results not being able to discriminate among the multiple models that fit the observed data (production history). Also, the high dimensionality of the inverse problem impedes estimation of uncertainties using classical Markov-chain Monte Carlo methods. We attenuated the ill-conditioned character of this history-matching inverse problem by reducing the model complexity using a spatial principal component basis and by combining as observables flow production measurements and time-lapse seismic crosswell tomographic images. Additionally the inverse problem was solved in a stochastic framework. For this purpose, we used a family of particle swarm optimization (PSO) optimizers that have been deduced from a physical analogy of the swarm system. For a synthetic sand-and-shale reservoir, we analyzed the performance of the different PSO optimizers, both in terms of exploration and convergence rate for two different reservoir models with different complexity and under the presence of different levels of white Gaussian noise added to the synthetic observed data. We demonstrated that PSO optimizers have a very good convergence rate for this example, and provide in addition, approximate measures of uncertainty around the optimum facies model. The PSO algorithms are robust in presence of noise, which is always the case for real data.},
author = {{Fern{\'{a}}ndez Mart{\'{i}}nez}, Juan Luis and Mukerji, Tapan and {Garc{\'{i}}a Gonzalo}, Esperanza and Suman, Amit},
doi = {10.1190/geo2011-0041.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Fern{\'{a}}ndez Mart{\'{i}}nez et al.{\_}2012.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {pso},
mendeley-tags = {pso},
month = {jan},
number = {1},
pages = {M1--M16},
title = {{Reservoir characterization and inversion uncertainty via a family of particle swarm optimizers}},
url = {http://library.seg.org/doi/abs/10.1190/geo2011-0041.1 http://library.seg.org/doi/10.1190/geo2011-0041.1},
volume = {77},
year = {2012}
}
@article{Nemeth1997,
author = {Nemeth, Tamas and Normark, Egon and Qin, Fuhao},
doi = {10.1190/1.1444115},
issn = {0016-8033},
journal = {Geophysics},
keywords = {steepest descent},
mendeley-tags = {steepest descent},
month = {jan},
number = {1},
pages = {168--176},
title = {{Dynamic smoothing in crosswell traveltime tomography}},
url = {http://library.seg.org/doi/10.1190/1.1444115},
volume = {62},
year = {1997}
}
@article{Sambridge1998,
abstract = {A discussion of methodologies for nonlinear geophysical inverse problems is presented. Geophysical inverse problems are often posed as optimization problems in a finite- dimensional parameter space. An Earth model is usually described by a set of parameters representing one or more geophysical properties (e.g. the speed with which seismic waves travel through the Earth's interior). Earth models are sought by minimizing the discrepancies between observation and predictions from the model, possibly, together with some regularizing constraint. The resulting optimization problem is usually nonlinear and often highly so, which may lead to multiple minima in the misfit landscape. Global (stochastic) optimization methods have become popular in the past decade. A discussion of simulated annealing, genetic algorithms and evolutionary programming methods is presented in the geophysical context. Less attention has been paid to assessing how well constrained, or resolved, individual parameters are. Often this problem is poorly posed. A new class of method is presented which offers potential in both the optimization and the ‘error analysis' stage of the inversion. This approach uses concepts from the field of computational geometry. The search algorithm described here does not appear to be practical in problems with dimension much greater than 10.},
author = {Sambridge, Malcolm},
doi = {10.1088/0266-5611/14/3/005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Inverse Problems/Sambridge{\_}1998.pdf:pdf},
isbn = {0266-5611},
issn = {0266-5611},
journal = {Inverse Problems},
keywords = {inversion},
mendeley-tags = {inversion},
month = {jun},
number = {3},
pages = {427--440},
title = {{Exploring multidimensional landscapes without a map}},
url = {http://stacks.iop.org/0266-5611/14/i=3/a=005?key=crossref.bf74b2bb29a53426b1d85fee8e3bccd3},
volume = {14},
year = {1998}
}
@book{Price2006,
author = {Price, Kenneth and Storn, Rainer M and Lampinen, Jouni A},
isbn = {3540313060},
publisher = {Springer Science {\&} Business Media},
title = {{Differential evolution: a practical approach to global optimization}},
year = {2006}
}
@inproceedings{Hansen1996,
abstract = {A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean is presented. This enables the evolution strategy (ES) to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the fitness function (or the coordinate system). Especially rotation invariance, here resulting directly from the coordinate system independent adaptation of the mutation distribution, is an essential feature of the ES with regard to its general applicability to complex fitness functions. Compared to previous work on this subject, the introduced formulation facilitates an interpretation of the resulting mutation distribution, making sensible manipulation by the user possible (if desired). Furthermore it enables a more effective control of the overall mutation variance (expected step length).},
author = {Hansen, N. and Ostermeier, A.},
booktitle = {Proceedings of IEEE International Conference on Evolutionary Computation},
doi = {10.1109/ICEC.1996.542381},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of IEEE International Conference on Evolutionary Computation/Hansen, Ostermeier{\_}1996.pdf:pdf},
isbn = {0-7803-2902-3},
keywords = {adaptation,cmaes,covariance matrix,derandomized,evolution strategy,evolutionary algorithms,mutation distribution,self-adaptation,strategy parameters},
mendeley-tags = {cmaes},
pages = {312--317},
publisher = {IEEE},
title = {{Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation}},
url = {http://ieeexplore.ieee.org/document/542381/},
year = {1996}
}
@article{Shapiro2002,
abstract = {We describe a method to invert surface wave dispersion data for a model of shear velocities with uncertainties in the crust and uppermost mantle. The inversion is a multistep process, constrained by a priori information, that culminates in a Markov-chain Monte-Carlo sampling of model space to yield an ensemble of acceptable models at each spatial node. The model is radially anisotropic in the uppermost mantle to an average depth of about 200 km and is isotropic elsewhere. The method is applied on a 2°× 2° grid globally to a large data set of fundamental mode surface wave group and phase velocities (Rayleigh group velocity, 16–200 s; Love group velocity, 16–150 s; Rayleigh and Love phase velocity, 40–150 s). The middle of the ensemble (Median Model) defines the estimated model and the half-width of the corridor of models provides the uncertainty estimate. Uncertainty estimates allow the identification of the robust features of the model which, typically, persist only to depths of ∼250 km. We refer to the features that appear in every member of the ensemble of acceptable models as ‘persistent'. Persistent features include sharper images of the variation of oceanic lithosphere and asthenosphere with age, continental roots, extensional tectonic features in the upper mantle, the shallow parts of subducted lithosphere, and improved resolution of radial anisotropy. In particular, we find no compelling evidence for ‘negative anisotropy' anywhere in the world's lithosphere.},
author = {Shapiro, N. M. and Ritzwoller, M. H.},
doi = {10.1046/j.1365-246X.2002.01742.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Shapiro, Ritzwoller{\_}2002.pdf:pdf},
isbn = {1365-246X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Lithosphere,Monte-Carlo inversion,Shear velocity,mcmc},
mendeley-tags = {mcmc},
month = {oct},
number = {1},
pages = {88--105},
pmid = {8214931},
title = {{Monte-Carlo inversion for a global shear-velocity model of the crust and upper mantle}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.2002.01742.x},
volume = {151},
year = {2002}
}
@article{PerezSolano2014,
abstract = {In the context of near surface seismic imaging (a few hundreds of metres), we propose an alternative approach for inversion of surface waves in 2-D media with laterally varying velocities. It is based on Full Waveform Inversion (FWI) but using an alternative objective function formulated in the frequency–wavenumber f − k domain. The classical FWI objective function suffers from severe local minima problems in the presence of surface waves. It thus requires a very accurate initial model. The proposed objective function is similar to the one used in classical surface wave analysis. In this approach, the data are first split using sliding windows in the time–space t − x domain. For each window, the amplitude of the f − k spectrum is computed. The objective function measures the least-squares misfit between the amplitude of observed and modelled 2-D Fourier transformed data sets. We call this formulation the windowed-amplitude waveform inversion (w-AWI).The w-AWI objective function reduces some local minima problems as shown here through numerical examples. The global minimum basin is wider in the w-AWI approach than in FWI. Synthetic examples show that w-AWI may achieve convergence if the lowest data frequency content is twice higher than the one needed by FWI. For elastic inversion, w-AWI can be used to reconstruct a velocity model explaining surface waves. This surface wave inversion procedure can be used to retrieve near-surface model parameters in lateral-varying media.},
author = {{Perez Solano}, C. A. and Donno, D. and Chauris, H.},
doi = {10.1093/gji/ggu211},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Perez Solano, Donno, Chauris{\_}2014.pdf:pdf},
isbn = {0956-540X},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Surface waves and free oscillations,Wave propagation,fwi,surface wave},
mendeley-tags = {fwi,surface wave},
month = {jul},
number = {3},
pages = {1359--1372},
title = {{Alternative waveform inversion for surface wave analysis in 2-D media}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggu211},
volume = {198},
year = {2014}
}
@article{Zhou1995,
author = {Zhou, Changxi and Cai, Wenying and Luo, Yi and Schuster, Gerard T. and Hassanzadeh, Sia},
doi = {10.1190/1.1443815},
issn = {0016-8033},
journal = {Geophysics},
keywords = {conjugate gradient},
mendeley-tags = {conjugate gradient},
month = {may},
number = {3},
pages = {765--773},
title = {{Acoustic wave‐equation traveltime and waveform inversion of crosshole seismic data}},
url = {http://library.seg.org/doi/10.1190/1.1443815},
volume = {60},
year = {1995}
}
@article{Lehman2017,
abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
archivePrefix = {arXiv},
arxivId = {1712.06563},
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
doi = {10.1145/3205455},
eprint = {1712.06563},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Lehman et al.{\_}2017.pdf:pdf},
keywords = {2018,acm reference format,and kenneth o,deep learning,jay chen,jeff clune,joel lehman,mutation,neuroevolution,recurrent networks,safe muta-,stanley},
title = {{Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients}},
url = {http://arxiv.org/abs/1712.06563},
year = {2017}
}
@article{Greenberg2017,
abstract = {The Illinois Basin - Decatur Project (IBDP) has demonstrated the safety, effectiveness, and efficiency of the process of isolating the carbon dioxide (CO2) stream from biofuels production and storage in a deep saline reservoir at a depth of more than 2,000 meters. Geologic assessment and controls have proven essential to understanding reservoir conditions and predicting CO2 behavior. The injectivity and storage capacity of part of the lower Mt. Simon Sandstone at IBDP have been confirmed. Modeling, microseismic event analysis, and MVA continue to provide significant insights into reservoir response to stored CO2 and the development of commercial-scale project workflows. Published by Elsevier Ltd.},
author = {Greenberg, Sallie E. and Bauer, Robert and Will, Robert and {Locke II}, Randall and Carney, Michael and Leetaru, Hannes and Medler, John},
doi = {10.1016/j.egypro.2017.03.1913},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Greenberg et al.{\_}2017.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Carbon Storage,IBDP,Microseismic,Sequestration,decatur},
mendeley-tags = {decatur},
month = {jul},
number = {November 2016},
pages = {5529--5539},
publisher = {The Author(s)},
title = {{Geologic Carbon Storage at a One Million Tonne Demonstration Project: Lessons Learned from the Illinois Basin – Decatur Project}},
url = {http://dx.doi.org/10.1016/j.egypro.2017.03.1913 https://linkinghub.elsevier.com/retrieve/pii/S1876610217321215},
volume = {114},
year = {2017}
}
@article{Davis1991,
author = {Davis, Lawrence},
publisher = {CUMINCAD},
title = {{Handbook of genetic algorithms}},
year = {1991}
}
@article{Jeanne2018,
abstract = {We studied the relation between rupture and changes in permeability within a fault zone intersecting the Opalinus Clay formation at 300 m depth in the Mont Terri Underground Research Laboratory (Switzerland). A series of water injection experiments were performed in a borehole straddle interval set within the damage zone of the main fault. A three-component displacement sensor allowed an estimation of the displacement of a minor fault plane reactivated during a succession of step rate pressure tests. The experiment reveals that the fault hydromechanical (HM) behavior is different from one test to the other with varying pressure levels needed to trigger rupture and different slip behavior under similar pressure conditions. Numerical simulations were performed to better understand the reason for such different behavior and to investigate the relation between rupture nucleation, permeability change, pressure diffusion, and rupture propagation. Our main findings are as follows: (i) a rate frictional law and a rate-and-state permeability law can reproduce the first test, but it appears that the rate constitutive parameters must be pressure dependent to reproduce the complex HM behavior observed during the successive injection tests; (ii) almost similar ruptures can create or destroy the fluid diffusion pathways; (iii) a too high or too low diffusivity created by the main rupture prevents secondary rupture events from occurring whereas "intermediate" diffusivity favors the nucleation of a secondary rupture associated with the fluid diffusion. However, because rupture may in certain cases destroy permeability, this succession of ruptures may not necessarily create a continuous hydraulic pathway.},
author = {Jeanne, Pierre and Guglielmi, Yves and Rutqvist, Jonny and Nussbaum, Christophe and Birkholzer, Jens},
doi = {10.1002/2017JB015149},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Jeanne et al.{\_}2018.pdf:pdf},
issn = {21699313},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {changes in permeability,fault reactivation,flac,in situ experiment,numerical simulation,rate and state,role of fluid,tough2},
mendeley-tags = {flac,tough2},
month = {feb},
number = {2},
pages = {1694--1710},
title = {{Permeability Variations Associated With Fault Reactivation in a Claystone Formation Investigated by Field Experiments and Numerical Simulations}},
url = {http://doi.wiley.com/10.1002/2017JB015149},
volume = {123},
year = {2018}
}
@article{Rawlinson2006,
abstract = {We demonstrate the potential of a recently developed grid-based eikonal solver for tracking phases comprising reflection branches, transmission branches, or a combination of these, in 3D heterogeneous layered media. The scheme is based on a multi-stage fast marching approach that reinitialises the wavefront from each interface it encounters as either a reflection or transmission. The use of spherical coordinates allows wavefronts and traveltimes to be computed at local, regional, and semi-global scales. Traveltime datasets for a large variety of seismic experiments can be predicted, including reflection, wide-angle reflection and refraction, local earthquake, and teleseismic. A series of examples are presented to demonstrate potential applications of the method. These include: (1) tracking active and passive source wavefronts in the presence of a complex subduction zone; (2) earthquake hypocentre relocation in a laterally heterogeneous 3D medium; (3) joint inversion of wide-angle and teleseismic datasets for P-wave velocity structure in the crust and upper mantle. Results from these numerical experiments show that the new scheme is highly flexible, robust and efficient, a combination seldom found in either grid- or ray-based traveltime solvers. The ability to track arrivals for multiple data classes such as wide-angle and teleseismic is of particular importance, given the recent momentum in the seismic imaging community towards combining active and passive source datasets in a single tomographic inversion.},
author = {Rawlinson, N. and de Kool, M. and Sambridge, M.},
doi = {10.1071/EG06322},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Exploration Geophysics/Rawlinson, de Kool, Sambridge{\_}2006.pdf:pdf},
issn = {0812-3985},
journal = {Exploration Geophysics},
keywords = {fast marching method,tomography},
number = {4},
pages = {322},
title = {{Seismic wavefront tracking in 3D heterogeneous media: applications with multiple data classes}},
url = {http://www.publish.csiro.au/?paper=EG06322},
volume = {37},
year = {2006}
}
@article{Freeman2014,
abstract = {Few tools exist for creating and visualizing complex three-dimensional simulation meshes, and these have limitations that restrict their application to particular geometries and circumstances. Mesh generation needs to trend toward ever more general applications. To that end, we have developed MeshVoro, a tool that is based on the Voro++ (Chris H. Rycroft, 2009. Chaos 19, 041111) library and is capable of generating complex three-dimensional Voronoi tessellation-based (unstructured) meshes for the solution of problems of flow and transport in subsurface geologic media that are addressed by the TOUGH (Pruess, K., Oldenburg C., Moridis G., 1999. Report LBNL-43134, 582. Lawrence Berkeley National Laboratory, Berkeley, CA) family of codes. MeshVoro, which includes built-in data visualization routines, is a particularly useful tool because it extends the applicability of the TOUGH family of codes by enabling the scientifically robust and relatively easy discretization of systems with challenging 3D geometries.We describe several applications of MeshVoro. We illustrate the ability of the tool to straightforwardly transform a complex geological grid into a simulation mesh that conforms to the specifications of the TOUGH family of codes. We demonstrate how MeshVoro can describe complex system geometries with a relatively small number of grid blocks, and we construct meshes for geometries that would have been practically intractable with a standard Cartesian grid approach. We also discuss the limitations and appropriate applications of this new technology. {\textcopyright} 2014 Elsevier Ltd.},
author = {Freeman, C.M. and Boyle, K.L. and Reagan, M. and Johnson, J. and Rycroft, C. and Moridis, G.J.},
doi = {10.1016/j.cageo.2014.05.002},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Freeman et al.{\_}2014.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Geology,Grid,Hydrology,Mesh,Petroleum,Shale,TOUGH,VisIt,Visualization,Voronoi,meshmaker,tough2},
mendeley-tags = {meshmaker,tough2},
month = {sep},
pages = {26--34},
publisher = {Elsevier},
title = {{MeshVoro: A three-dimensional Voronoi mesh building tool for the TOUGH family of codes}},
url = {http://dx.doi.org/10.1016/j.cageo.2014.05.002 https://linkinghub.elsevier.com/retrieve/pii/S0098300414001046},
volume = {70},
year = {2014}
}
@article{Reynen2017,
author = {Reynen, Andrew and Audet, Pascal},
doi = {10.1093/gji/ggx238},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Reynen, Audet{\_}2017.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Numerical approximations and analysis,Time-series analysis,machine learning},
mendeley-tags = {machine learning},
number = {3},
pages = {1394--1409},
title = {{Supervised machine learning on a network scale: Application to seismic event classification and detection}},
volume = {210},
year = {2017}
}
@article{Rostami2017,
author = {Rostami, Shahin and Neri, Ferrante},
doi = {10.1016/j.swevo.2016.12.002},
issn = {22106502},
journal = {Swarm and Evolutionary Computation},
month = {jun},
pages = {50--67},
title = {{A fast hypervolume driven selection mechanism for many-objective optimisation problems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2210650216301328},
volume = {34},
year = {2017}
}
@article{Billings1994,
abstract = {The Metropolis algorithm for simulated annealing has been applied to earthquake location. The method combines linear and non-linear search techniques, and is able to locate events reliably and efficiently. A separation of the spatial and temporal components of the search improves performance. This arises from decoupling the strong correlation between depth and origin time and by taking advantage of the low computational cost of re-computing the misfit for multiple origin times. In addition, a method of generating new models is applied which progressively concentrates attention on more favourable regions while still allowing the algorithm to avoid local minima in the misfit function. In contrast to other non-linear algorithms there is no requirement to explicitly delineate bounds on the hypocentral coordinates. The simulated-annealing technique is an example of a global optimization routine. Consequently, it does not require the computation of derivatives and so can be used with arrival times of multiple phases, azimuth and slowness information and any type of velocity model, including laterally heterogeneous 3-D models, without modification to the basic algorithm. In addition, robust statistical functions describing the data misfit can be easily incorporated. The performance of the algorithm is illustrated on three events with different types of data, including one event with array information. Traveltimes are calculated relative to the l-D imp91 velocity model. In each case, starting from widely separated initial locations the algorithm converges to a region of a few cubic kilometres. This region represents the neighbourhood of the global minimum of the misfit function. By including the maximum amount of available information and using robust statistical functions, the final locations are more accurate than those obtained using linear methods. In addition, the algorithm is able to locate the immediate region of the global minimum with much less computational effort than standard non-linear algorithms.},
author = {Billings, Stephen D.},
doi = {10.1111/j.1365-246X.1994.tb03993.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Billings{\_}1994.pdf:pdf},
isbn = {0956-540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {earthquake location,non‐linear optimization,sa,simulated annealing},
mendeley-tags = {sa},
month = {sep},
number = {3},
pages = {680--692},
title = {{Simulated annealing for earthquake location}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1994.tb03993.x},
volume = {118},
year = {1994}
}
@article{Garcia-Nieto2011,
abstract = {Large scale continuous optimization problems are more relevant in current benchmarks since they are more representative of real-world problems (bioinformatics, data mining, etc.). Unfortunately, the performance of most of the available optimization algorithms deteriorates rapidly as the dimensionality of the search space increases. In particular, particle swarm optimization is a very simple and effective method for continuous optimization. Nevertheless, this algorithm usually suffers from unsuccessful performance on large dimension problems. In this work, we incorporate two new mechanisms into the particle swarm optimization with the aim of enhancing its scalability. First, a velocity modulation method is applied in the movement of particles in order to guide them within the region of interest. Second, a restarting mechanism avoids the early convergence and redirects the particles to promising areas in the search space. Experiments are carried out within the scope of this Special Issue to test scalability. The results obtained show that our proposal is scalable in all functions of the benchmark used, as well as numerically very competitive with regards to other compared optimizers.},
author = {Garc{\'{i}}a-Nieto, Jos{\'{e}} and Alba, Enrique},
doi = {10.1007/s00500-010-0648-1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Soft Computing/Garc{\'{i}}a-Nieto, Alba{\_}2011.pdf:pdf},
isbn = {1432-7643},
issn = {1432-7643},
journal = {Soft Computing},
keywords = {Continuous optimization,Large scale benchmarking,Particle swarm optimization,Scalability,pso},
mendeley-tags = {pso},
month = {nov},
number = {11},
pages = {2221--2232},
title = {{Restart particle swarm optimization with velocity modulation: a scalability test}},
url = {http://link.springer.com/10.1007/s00500-010-0648-1},
volume = {15},
year = {2011}
}
@article{Cary1988,
abstract = {Full waveform synthetic seismograms are now used as standard in the interpretation of marine refraction data, but only with a laborious trial-and-error fitting procedure. We show how the matching of observed waveforms with WKBJ synthetic seismograms can be efficiently and automatically performed for 1-D earth models. Automation of the inversion is difficult because of the multi-modal, irregular form of the misfit of data and synthetics. A sequence of three steps is required for a complete inversion: location of the deepest valley of the misfit function, descent to the global minimum, and description of the neighbourhood of the minimum for error analysis. The first step is accomplished with a Monte Carlo search through a very large model space defined by poor prior knowledge of velocities and gradients of the solution. Weak traveltime constraints are used to eliminate poorly fitting models from waveform calculations. A comparison of the traveltime and waveform misfits of the Monte Carlo models clearly illustrates that waveforms are providing more information than traveltimes alone. Bayesian statistics are used to construct marginal probability distributions and the covariance matrix, which give a rough preliminary error analysis. In the second step, damped least-squares linearized inversion makes small adjustments to the best-fitting Monte Carlo model as it descends to the global minimum. Finally the immediate neighbourhood of the global minimum is explored with constrained least-squares inversion. Realistic error bounds on each parameter are defined from the resulting slices through the misfit function. These bounds are much narrower than traveltime inversion provides. Correlations between parameters are obtained from the covariance matrix constructed from models examined during this error analysis. The inversion methods are illustrated on the FF2 refraction data set of the Scripps Institution of Oceanography. The Monte Carlo search successfully locates the valley of the global minimum as well as a nearby secondary minimum. The error analysis puts realistic error bounds on the detailed inversion of this data set by Spudich {\&} Orcutt.},
author = {Cary, P. W. and Chapman, C. H.},
doi = {10.1111/j.1365-246X.1988.tb03879.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Cary, Chapman{\_}1988.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {Error analysis,Monte Carlo,inverse theory,mcmc,seismic refraction,waveform inversion},
mendeley-tags = {mcmc},
month = {jun},
number = {3},
pages = {527--546},
title = {{Automatic 1-D waveform inversion of marine seismic refraction data}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1988.tb03879.x},
volume = {93},
year = {1988}
}
@article{Sleeman1999a,
abstract = {The onset of a seismic signal is determined through joint AR modeling of the noise and the seismic signal, and the application of the Akaike Information Criterion (AIC) using the onset time as parameter. This so-called AR-AIC phase picker has been tested successfully and implemented on the Z-component of the broadband station HGN to provide automatic P-phase picks for a rapid warning system. The AR-AIC picker is shown to provide accurate and robust automatic picks on a large experimental database. Out of 1109 P-phase onsets with signal-to-noise ratio (SNR) above 1 from local. regional and teleseismic earthquakes, our implementation detects 71{\%} and gives a mean difference with manual picks of 0.1 s. An optimal version of the well-established picker of Baer and Kradolfer [Baer, M., Kradolfer, U., An automatic phase picker for local and teleseismic events, Bull. Seism. Soc. Am. 77 (1987) 1437-1445] detects less than 41{\%} and gives a mean difference with manual picks of 0.3 s using the same dataset.},
author = {Sleeman, Reinoud and van Eck, Torild},
doi = {10.1016/S0031-9201(99)00007-2},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Sleeman, van Eck{\_}1999.pdf:pdf},
isbn = {0031-9201},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {AR model,Akaike information criterion (AIC),Phase-picker,Seismic warning system,picking},
mendeley-tags = {picking},
month = {jun},
number = {1-4},
pages = {265--275},
title = {{Robust automatic P-phase picking: an on-line implementation in the analysis of broadband seismogram recordings}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031920199000072},
volume = {113},
year = {1999}
}
@article{Taillandier2009,
abstract = {Classical algorithms used for traveltime tomography are not necessarily well suited for handling very large seismic data sets or for taking advantage of current supercomputers. The classical approach of first-arrival traveltime tomography was revisited with the proposal of a simple gradient-based approach that avoids ray tracing and estimation of the Fr{\'{e}}chet derivative matrix. The key point becomes the derivation of the gradient of the misfit function obtained by the adjoint-state technique. The adjoint-state method is very attractive from a numerical point of view because the associated cost is equivalent to the solution of the forward-modeling problem, whatever the size of the input data and the number of unknown velocity parameters. An application on a 2D synthetic data set demonstrated the ability of the algorithm to image near-surface velocities with strong vertical and lateral variations and revealed the potential of the method.},
author = {Taillandier, C{\'{e}}dric and Noble, Mark and Chauris, Herv{\'{e}} and Calandra, Henri},
doi = {10.1190/1.3250266},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Taillandier et al.{\_}2009.pdf:pdf},
isbn = {0016-8033},
issn = {0016-8033},
journal = {Geophysics},
title = {{First-arrival traveltime tomography based on the adjoint-state method}},
year = {2009}
}
@inproceedings{Yang2009,
author = {Yang, Xin-She and Deb, Suash},
booktitle = {Nature {\&} Biologically Inspired Computing, 2009. NaBIC 2009. World Congress on},
isbn = {1424450535},
pages = {210--214},
publisher = {IEEE},
title = {{Cuckoo search via L{\'{e}}vy flights}},
year = {2009}
}
@article{Evans2011,
author = {Rinaldi, Antonio P. and Jeanne, Pierre and Rutqvist, Jonny and Cappa, Fr{\'{e}}d{\'{e}}ric and Guglielmi, Yves},
doi = {10.1002/ghg.1403},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Greenhouse Gases Science and Technology/Rinaldi et al.{\_}2014.pdf:pdf},
issn = {21523878},
journal = {Greenhouse Gases: Science and Technology},
keywords = {carbon sequestration,fault architecture,flac,geomechanics,induced seismicity,leakage,toug2},
mendeley-tags = {flac,toug2},
month = {feb},
number = {1},
pages = {99--120},
title = {{Effects of fault-zone architecture on earthquake magnitude and gas leakage related to CO2 injection in a multi-layered sedimentary system}},
url = {http://doi.wiley.com/10.1002/ghg.1403},
volume = {4},
year = {2014}
}
@article{Wilken2012,
abstract = {We investigate different aspects concerning the application of swarm intelligence optimization to the inversion of Scholte-wave phase-slowness frequency (pf) spectra with respect to shear wave velocity structure. Besides human influence due to the dependence on a priori information for starting models and interpretation of pf spectra as well as noise, the model resolution of the inversion problem is strongly influenced by the multimodality of the misfit function. We thus tested the efficiency of global, stochastic optimization approaches with focus on swarm intelligence methods that can explore the multiple minima of the misfit function. A comparison among different PSO schemes by applying them to synthetic Scholte-wave spectra led to a hybrid of Particle Swarm Optimization and Downhill Simplex providing the best resolution of inverted shear wave velocity depth models. The results showed a very low spread of best fitting solutions of 7 per cent in shear wave velocity and an average of 9 per cent for noisy synthetic data and a very good fit to the true synthetic model used for computation of the input data. To classify this method we also compared the probability of finding a good fit in synthetic spectra inversion with that of Evolutionary Algorithm, Simulated Annealing, Neighbourhood Algorithm and Artificial Bee Colony algorithm. Again the hybrid optimization scheme showed its predominance. The usage of stochastic algorithms furthermore allowed a new way of misfit definition in terms of dispersion curve slowness residuals making the inversion scheme independent on Scholte-wave mode identification and allowing joint inversion of fundamental mode and higher mode information. Finally we used the hybrid optimization scheme and the misfit calculation for the inversion of 2-D shear wave velocity profiles from two locations in the North- and Baltic Sea. The models show acceptable resolution and a very good structural correlation to high resolution reflection seismic data.},
author = {Wilken, D. and Rabbel, W.},
doi = {10.1111/j.1365-246X.2012.05500.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Wilken, Rabbel{\_}2012.pdf:pdf},
isbn = {0956-540X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Europe,Inverse theory,Surface waves and free oscillations,pso},
mendeley-tags = {pso},
month = {jul},
number = {1},
pages = {580--594},
title = {{On the application of Particle Swarm Optimization strategies on Scholte-wave inversion}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2012.05500.x},
volume = {190},
year = {2012}
}
@article{Gandomi2013,
abstract = {In this study, a new metaheuristic optimization algorithm, called cuckoo search (CS), is introduced for solving structural optimization tasks. The new CS algorithm in combination with Le´vy flights is first verified using a$\backslash$r$\backslash$nbenchmark nonlinear constrained optimization problem. For$\backslash$r$\backslash$nthe validation against structural engineering optimization$\backslash$r$\backslash$nproblems, CS is subsequently applied to 13 design problems$\backslash$r$\backslash$nreported in the specialized literature. The performance of the CS algorithm is further compared with various algorithms$\backslash$r$\backslash$nrepresentative of the state of the art in the area. The optimal solutions obtained by CS are mostly far better than the best solutions obtained by the existing methods. The unique search features used in CS and the implications for future research are finally discussed in detail.},
archivePrefix = {arXiv},
arxivId = {1003.1594},
author = {Gandomi, Amir Hossein and Yang, Xin-She and Alavi, Amir Hossein},
doi = {10.1007/s00366-011-0241-y},
eprint = {1003.1594},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Engineering with Computers/Gandomi, Yang, Alavi{\_}2013.pdf:pdf},
isbn = {0177-0667},
issn = {0177-0667},
journal = {Engineering with Computers},
keywords = {Cuckoo search,Engineering design,Metaheuristic algorithm,Structural optimization},
month = {jan},
number = {1},
pages = {17--35},
pmid = {19782018},
title = {{Cuckoo search algorithm: a metaheuristic approach to solve structural optimization problems}},
url = {http://link.springer.com/10.1007/s00366-011-0241-y},
volume = {29},
year = {2013}
}
@article{Sambridge2002,
abstract = {Monte Carlo inversion techniques were first used by Earth scientists more than 30 years ago. Since that time they have been applied to a wide range of problems, from the inversion of free oscillation data for whole Earth seismic structure to studies at the meter-scale lengths encountered in exploration seismology. This paper traces the development and application of Monte Carlo methods for inverse problems in the Earth sciences and in particular geophysics. The major developments in theory and application are traced from the earliest work of the Russian school and the pioneering studies in the west by Press 1968 to modern importance sampling and ensemble inference methods. The paper is divided into two parts. The first is a literature review, and the second is a summary of Monte Carlo techniques that are currently popular in geophysics. These include simulated annealing, genetic algorithms, and other importance sampling approaches. The objective is to act as both an introduction for newcomers to the field and a comprehensive reference source for researchers already familiar with Monte Carlo inversion. It is our hope that the paper will serve as a timely summary of an expanding and versatile methodology and also encourage applications to new areas of the Earth sciences.},
author = {Sambridge, Malcolm and Mosegaard, Klaus},
doi = {10.1029/2000RG00089},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Reviews of Geophysics/Sambridge, Mosegaard{\_}2002.pdf:pdf},
isbn = {2002101029},
issn = {87551209},
journal = {Reviews of Geophysics},
number = {3},
pages = {1009},
title = {{Monte Carlo Methods in Geophysical Inverse Problems}},
url = {http://www.gps.caltech.edu/classes/ge193/papers{\_}books/sambridge2002.pdf},
volume = {40},
year = {2002}
}
@article{Rutqvist2016a,
abstract = {In the light of current concerns related to induced seismicity associated with geological carbon sequestration (GCS), this paper summarizes lessons learned from recent modeling studies on fault activation, induced seismicity, and potential for leakage associated with deep underground carbon dioxide (CO2) injection. Model simulations demonstrate that seismic events large enough to be felt by humans require brittle fault properties and continuous fault permeability allowing pressure to be distributed over a large fault patch to be ruptured at once. Heterogeneous fault properties, which are commonly encountered in faults intersecting multilayered shale/sandstone sequences, effectively reduce the likelihood of inducing felt seismicity and also effectively impede upward CO2 leakage. A number of simulations show that even a sizable seismic event that could be felt may not be capable of opening a new flow path across the entire thickness of an overlying caprock and it is very unlikely to cross a system of multiple overlying caprock units. Site-specific model simulations of the In Salah CO2 storage demonstration site showed that deep fractured zone responses and associated microseismicity occurred in the brittle fractured sandstone reservoir, but at a very substantial reservoir overpressure close to the magnitude of the least principal stress. We conclude by emphasizing the importance of site investigation to characterize rock properties and if at all possible to avoid brittle rock such as proximity of crystalline basement or sites in hard and brittle sedimentary sequences that are more prone to injection-induced seismicity and permanent damage.},
author = {Rutqvist, Jonny and Rinaldi, Antonio P. and Cappa, Frederic and Jeanne, Pierre and Mazzoldi, Alberto and Urpi, Luca and Guglielmi, Yves and Vilarrasa, Victor},
doi = {10.1016/j.jrmge.2016.09.001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Rock Mechanics and Geotechnical Engineering/Rutqvist et al.{\_}2016.pdf:pdf},
issn = {16747755},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
keywords = {Carbon dioxide (CO2) injection,Fault rupture,Ground motion,Induced seismicity,Leakage,Modeling,flac,tough2},
mendeley-tags = {flac,tough2},
month = {dec},
number = {6},
pages = {789--804},
publisher = {Elsevier Ltd},
title = {{Fault activation and induced seismicity in geological carbon storage – Lessons learned from recent modeling studies}},
url = {http://dx.doi.org/10.1016/j.jrmge.2016.09.001 https://linkinghub.elsevier.com/retrieve/pii/S167477551630049X},
volume = {8},
year = {2016}
}
@inproceedings{Malinverno2000a,
author = {Malinverno, Alberto and Leaney, Scott},
booktitle = {SEG Technical Program Expanded Abstracts 2000},
doi = {10.1190/1.1815943},
month = {jan},
pages = {2393--2396},
publisher = {Society of Exploration Geophysicists},
title = {{A Monte Carlo method to quantify uncertainty in the inversion of zero‐offset VSP data}},
url = {http://library.seg.org/doi/abs/10.1190/1.1815943},
year = {2000}
}
@book{Chavent1974,
author = {Chavent, Guy},
booktitle = {[No source information available]},
keywords = {adjoint state},
mendeley-tags = {adjoint state},
month = {jun},
title = {{Identification of Functional Parameters in Partial Differential Equations}},
year = {1974}
}
@article{Jones1979a,
author = {Jones, Alan G and Hutton, Rosemary},
doi = {10.1111/j.1365-246X.1979.tb00169.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Jones, Hutton{\_}1979.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {mcmc},
mendeley-tags = {mcmc},
month = {feb},
number = {2},
pages = {351--368},
title = {{A multi-station magnetotelluric study in southern Scotland -- II. Monte-Carlo inversion of the data and its geophysical and tectonic implications}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1979.tb00169.x},
volume = {56},
year = {1979}
}
@article{Vassallo2012,
abstract = {In this paper we introduce an optimization scheme for choosing the most appropriate set of parameters for a picking algorithm by using real picks and data acquired by a specific seismic network. The optimal model is chosen through searching in the global parameter space of the maximum of an objective function that depends on the comparison between automatic picks and manual picks performed on a dataset representative for a seismic network. We show applications to two STA/LTA algorithms: the Allen (1978) picker and the new FilterPicker algorithm (Lomax et al. 2012). Automatic Picker Developments and Optimization: A Strategy for Improving the Performances of Automatic Phase Pickers (PDF Download Available).},
author = {Vassallo, Maurizio and Satriano, Claudio and Lomax, Anthony},
doi = {10.1785/gssrl.83.3.541},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Seismological Research Letters/Vassallo, Satriano, Lomax{\_}2012.pdf:pdf},
issn = {0895-0695},
journal = {Seismological Research Letters},
keywords = {picking},
mendeley-tags = {picking},
month = {may},
number = {3},
pages = {541--554},
title = {{Automatic Picker Developments and Optimization: A Strategy for Improving the Performances of Automatic Phase Pickers}},
url = {http://srl.geoscienceworld.org/cgi/doi/10.1785/gssrl.83.3.541},
volume = {83},
year = {2012}
}
@article{Rothert2003,
author = {Rothert, Elmar and Shapiro, Serge A.},
doi = {10.1190/1.1567239},
issn = {0016-8033},
journal = {Geophysics},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {mar},
number = {2},
pages = {685--689},
title = {{Microseismic monitoring of borehole fluid injections: Data modeling and inversion for hydraulic properties of rocks}},
url = {https://library.seg.org/doi/10.1190/1.1567239},
volume = {68},
year = {2003}
}
@article{Sambridge1999,
abstract = {This paper presents a new derivative-free search method for finding models of acceptable data fit in a multidimensional parameter space. It falls into the same class of method as simulated annealing and genetic algorithms, which are commonly used for global optimization problems. The objective here is to find an ensemble of models that preferentially sample the good data-fitting regions of parameter space, rather than seeking a single optimal model. (A related paper deals with the quantitative appraisal of the ensemble.) The new search algorithm makes use of the geometrical constructs known as Voronoi cells to derive the search in parameter space. These are nearest neighbour regions defined under a suitable distance norm. The algorithm is conceptually simple, requires just two ‘tuning parameters', and makes use of only the rank of a data fit criterion rather than the numerical value. In this way all difficulties associated with the scaling of a data misfit function are avoided, and any combination of data fit criteria can be used. It is also shown how Voronoi cells can be used to enhance any existing direct search algorithm, by intermittently replacing the forward modelling calculations with nearest neighbour calculations. The new direct search algorithm is illustrated with an application to a synthetic problem involving the inversion of receiver functions for crustal seismic structure. This is known to be a non-linear problem, where linearized inversion techniques suffer from a strong dependence on the starting solution. It is shown that the new algorithm produces a sophisticated type of ‘self-adaptive' search behaviour, which to our knowledge has not been demonstrated in any previous technique of this kind.},
author = {Sambridge, Malcolm},
doi = {10.1046/j.1365-246X.1999.00876.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}1999.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {inversion,numerical techniques,receiver functions,waveform inversion},
mendeley-tags = {inversion},
month = {aug},
number = {2},
pages = {479--494},
title = {{Geophysical inversion with a neighbourhood algorithm-I. Searching a parameter space}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246x.1999.00900.x https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.1999.00876.x},
volume = {138},
year = {1999}
}
@article{Cipolla2011,
abstract = {Thousands of hydraulic fracture treatments have been monitored in the past ten years using microseismic mapping, providing a wealth of measurements that show a surprising degree of diversity in event patterns. Interpreting the microseismic data to determine the geometry and complexity of hydraulic fractures continues to be focused on visualization of the event patterns and qualitative estimates of the “stimulated volume”, which has led to wide variations and inconsistencies in interpretations. Comparing the energy input during a hydraulic fracture treatment and resultant energy released by microseismic events demonstrates that the seismic deformation is a very small portion of the total deformation. The vast majority of the energy is consumed in aseismic deformation (tensile opening) and fluid friction (Maxwell et al. 2008). Proper interpretation of microseismic measurements should account for both seismic and aseismic deformation, coupling the geomechanics of fracture opening and propagation with the shear failures that generate microseisms. Interpretation of microseismic measurements begins with an evaluation of location uncertainty, using signal-to-noise ratios and error ellipsoids, along with event moment magnitude. In some cases, microseismic event location uncertainty is erroneously interpreted as fracture complexity. The next step is to normalize the data and correct for observation well bias, both distance and azimuth, including use of seismic radiation patterns. Without these corrections fracture behavior from well to well or stage to stage (especially in horizontal wells) can easily be misinterpreted. Advanced geophysical processing that describes the failure mechanisms in more detail may also aid in the interpretation. The final step in the interpretation is to include the geomechanics of the overall process, accounting for the fracture treatment volumes injected, the net pressure in the hydraulic fracture(s) and the shear failures that generated the microseisms. This final, critical step is often overlooked when interpreting microseismic measurements. The paper provides a comprehensive, yet practical guide to the interpretation of microseismic measurements.},
author = {Cipolla, C and Maxwell, S and Mack, M and Downie, R},
doi = {10.2118/144067},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE North American Unconventional Gas Conference and Exhibition/Cipolla et al.{\_}2011.pdf:pdf},
isbn = {9781618398109},
journal = {SPE North American Unconventional Gas Conference and Exhibition},
keywords = {SPE 144067,microseismic},
mendeley-tags = {microseismic},
number = {June},
pages = {1--28},
title = {{A Practical Guide to Interpreting Microseismic Measurements}},
year = {2011}
}
@article{Will2014,
abstract = {The Illinois Basin - Decatur Project (IBDP) is one of the US Department of Energy National Energy Technology Laboratory-funded carbon dioxide (CO2) sequestration projects that is approaching the goal of injecting one million tonnes of CO2 within three years. Since mid-November 2011, IBDP has maintained the target injection rate of approximately 1,000 tonnes of CO2 per day into the Mt. Simon Sandstone at a depth of approximately 7,000 ft (2134 m). Several measurement, monitoring, characterization, data integration, and modelling technologies have been implemented on this project, including real-time continuous microseismic monitoring which commenced several months prior to start of injection. Much of the extensive site characterization effort at IBDP has been motivated by the desire to understand the source mechanisms for observed microseismicity toward the ultimate goal of developing predictive capability. A rich dataset of microseismic observations has been acquired over nearly 4.5 years of monitoring to-date. These observations form semi-linear clusters in space, supporting the interpretation of a structural source mechanism. However, corresponding structural features are not observed in the existing 3D seismic data. This lack of direct observation of a structural source feature prompted a multi-disciplinary geoscience based approach to understanding the source mechanism for the observed microseismicity. Relationships between microseismic event occurrence and subsurface geology are observed in multiple domains and at multiple scales. Geomechanical characterization efforts show that the orientations of clusters are consistent with the in-situ tectonic stress regime. Microseismic event first motion analysis suggests focal mechanisms also consistent with the tectonic stresses. While no faults or other structural discontinuities may be unambiguously interpreted from the seismic data using conventional amplitude interpretation methods, investigation of specialized edge detection seismic attributes reveals a directional fabric in the rock mass that is consistent with the orientation of microseismic clusters. At a macro scale, some microseismic clusters appear to be associated with topographic features in the Precambrian basement interpreted from 3D seismic data. At the opposite end of the scale, the fractures observed in grains are suggestive of a tectonic stress regime consistent with both the geomechanical analysis and the microseismic cluster orientation. Through these multi-disciplinary, multi-scale studies, an understanding of the relationship between subsurface geology and observed microseismicity is evolving. While no single observation set supports unambiguous correlation, the consistency between multiple lines of investigation supports an interpretation of in-situ stresses, rock fabric anisotropy, grain scale failure, and relationships to basement structure, all consistent with realistic hypotheses for the microseismic source event mechanisms. One such source mechanism hypothesis has been tested using sophisticated numerical fluid flow and geomechanical modeling followed by seismological calculations, preliminary results of which are presented here.},
author = {Will, Robert and Smith, Valerie and Leetaru, Hannes E. and Freiburg, Jared T. and Lee, Donald W.},
doi = {10.1016/j.egypro.2014.11.478},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Will et al.{\_}2014.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {CO2,Carbon,Characterization,Geology,Geomechanics,Monitoring,Roseismic,Sequestration,decatur},
mendeley-tags = {decatur},
pages = {4424--4436},
publisher = {Elsevier B.V.},
title = {{Microseismic Monitoring, Event Occurrence, and the Relationship to Subsurface Geology}},
url = {http://dx.doi.org/10.1016/j.egypro.2014.11.478 https://linkinghub.elsevier.com/retrieve/pii/S1876610214022930},
volume = {63},
year = {2014}
}
@article{Rucker2017,
abstract = {Many tasks in applied geosciences cannot be solved by single measurements, but require the integration of geophysical, geotechnical and hydrological methods. Numerical simulation techniques are essential both for planning and interpretation, as well as for the process understanding of modern geophysical methods. These trends encourage open, simple, and modern software architectures aiming at a uniform interface for interdisciplinary and flexible modelling and inversion approaches. We present pyGIMLi (Python Library for Inversion and Modelling in Geophysics), an open-source framework that provides tools for modelling and inversion of various geophysical but also hydrological methods. The modelling component supplies discretization management and the numerical basis for finite-element and finite-volume solvers in 1D, 2D and 3D on arbitrarily structured meshes. The generalized inversion framework solves the minimization problem with a Gauss-Newton algorithm for any physical forward operator and provides opportunities for uncertainty and resolution analyses. More general requirements, such as flexible regularization strategies, time-lapse processing and different sorts of coupling individual methods are provided independently of the actual methods used. The usage of pyGIMLi is first demonstrated by solving the steady-state heat equation, followed by a demonstration of more complex capabilities for the combination of different geophysical data sets. A fully coupled hydrogeophysical inversion of electrical resistivity tomography (ERT) data of a simulated tracer experiment is presented that allows to directly reconstruct the underlying hydraulic conductivity distribution of the aquifer. Another example demonstrates the improvement of jointly inverting ERT and ultrasonic data with respect to saturation by a new approach that incorporates petrophysical relations in the inversion. Potential applications of the presented framework are manifold and include time-lapse, constrained, joint, and coupled inversions of various geophysical and hydrological data sets.},
author = {R{\"{u}}cker, Carsten and G{\"{u}}nther, Thomas and Wagner, Florian M.},
doi = {10.1016/j.cageo.2017.07.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/R{\"{u}}cker, G{\"{u}}nther, Wagner{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
month = {dec},
number = {August},
pages = {106--123},
publisher = {The Authors},
title = {{pyGIMLi: An open-source library for modelling and inversion in geophysics}},
url = {https://doi.org/10.1016/j.cageo.2017.07.011 http://linkinghub.elsevier.com/retrieve/pii/S0098300417300584},
volume = {109},
year = {2017}
}
@article{Murat1992,
abstract = {A back-propagation neural network is successfully applied to pick first arrivals (first breaks) in a background of noise. Network output is a decision whether each half-cycle on the trace is a first or not. 3D plots of the input attributes allow evaluation of the attributes for use in a neural network. Clustering and separation of first break from non-break data on the plots indicate that a neural network solution is possible, and therefore the attributes are suitable as network input. Application of the trained network to actual seismic data (Vibroseis and Poulter sources) demonstrates successful automated first-break selection for the following four attributes used as neural network input: (1) peak amplitude of a half-cycle; (2) amplitude difference between the peak value of the half-cycle and the previous (or following) half-cycle; (3) rms amplitude ratio for a data window (0.3 s) before and after the half-cycle; (4) rms amplitude ratio for a data window (0.06 s) on adjacent traces. The contribution of the attributes based on adjacent traces (4) was considered significant and future work will emphasize this aspect.},
author = {Murat, Michael E. and Rudman, Albert J.},
doi = {10.1111/j.1365-2478.1992.tb00543.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Murat, Rudman{\_}1992.pdf:pdf},
issn = {0016-8025},
journal = {Geophysical Prospecting},
month = {aug},
number = {6},
pages = {587--604},
title = {{Automated first arrival picking: a neural network approach}},
url = {http://doi.wiley.com/10.1111/j.1365-2478.1992.tb00543.x},
volume = {40},
year = {1992}
}
@article{Capon1967,
author = {Capon, J and Greenfield, R J and Kolker, R Jh},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
number = {2},
pages = {192--211},
publisher = {IEEE},
title = {{Multidimensional maximum-likelihood processing of a large aperture seismic array}},
volume = {55},
year = {1967}
}
@phdthesis{Frechet1985,
abstract = {La premi{\`{e}}re partie de la th{\`{e}}se analyse les relations entre s{\'{e}}ismes et contraintes. Apr{\`{e}}s un rappel sur la m{\'{e}}canique des milieux continus et son application aux contraintes dans le globe, on pr{\'{e}}sente le calcul des d{\'{e}}placements engendr{\'{e}}s par une faille rectangulaire de jeu et de pendage quelconques, dans un demi-espace {\'{e}}lastique homog{\`{e}}ne. La deuxi{\`{e}}me partie est consacr{\'{e}}e {\`{a}} l'{\'{e}}tude des doublets sismiques, c'est-{\`{a}}-dire des paires de s{\'{e}}ismes ayant des formes d'onde identiques. On distingue des doublets temporels, situ{\'{e}}s au m{\^{e}}me endroit mais {\`{a}} des dates diff{\'{e}}rentes, et des doublets spatiaux situ{\'{e}}s {\`{a}} quelque distance l'un de l'autre (au plus quelques centaines de m{\`{e}}tres), mais {\`{a}} des dates tr{\`{e}}s rapproch{\'{e}}es. L'{\'{e}}tude de ces donn{\'{e}}es est fond{\'{e}}e sur une analyse interspectrale par fen{\^{e}}tre mobile. Pour une fen{\^{e}}tre donn{\'{e}}e, on mesure le d{\'{e}}lai (d{\'{e}}calage en temps) entre les deux signaux en calculant la pente du d{\'{e}}phasage en fonction de la fr{\'{e}}quence. On calcule aussi la coh{\'{e}}rence et le rapport spectral pour chaque fr{\'{e}}quence. Le d{\'{e}}lai est mesur{\'{e}} avec une pr{\'{e}}cision atteignant 1 ms, et ceci en tout point du sismogramme. L'{\'{e}}tude porte sur 24 doublets et multiplets (67 s{\'{e}}ismes au total) situ{\'{e}}s en Californie centrale, {\`{a}} l'aide d'une soixantaine de stations du r{\'{e}}seau CALNET. La relocalisation relative des s{\'{e}}ismes d'un multiplet est faite avec une pr{\'{e}}cision atteignant 1 m{\`{e}}tre. On a pu ainsi d{\'{e}}montrer la simplicit{\'{e}} et l'unicit{\'{e}} locale des plans de faille en profondeur. Le d{\'{e}}lai mesur{\'{e}} dans la cauda permet de pr{\'{e}}ciser l'origine de celle-ci, profonde pour les stations {\`{a}} l'{\'{e}}cart des failles, superficielle pour les stations {\`{a}} l'aplomb des grandes failles. L'{\'{e}}tude des doublets temporels permet de mesurer les variations de vitesse des ondes dans la croate avec une grande pr{\'{e}}cision. Une variation r{\'{e}}gionale induit un " {\'{e}}tirement de sismogramme ". La mesure de ce dernier permet une r{\'{e}}solution de 0,01 {\%} de variation de vitesse des ondes S. Les variations ainsi mesur{\'{e}}es atteignent 0,4 {\%} dans la r{\'{e}}gion de Hollister et sont attribu{\'{e}}es au s{\'{e}}isme de Coyote Lake (M=5,6).},
author = {Fr{\'{e}}chet, J.},
booktitle = {Th{\`{e}}se d'Etat, Universit{\'{e}} Scientifique et M{\'{e}}dicale de Grenoble},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Th{\`{e}}se d'Etat, Universit{\'{e}} Scientifique et M{\'{e}}dicale de Grenoble/Fr{\'{e}}chet{\_}1985.pdf:pdf},
keywords = {location},
mendeley-tags = {location},
pages = {206},
title = {{Sismogen{\`{e}}se et doublets sismiques}},
year = {1985}
}
@article{MacKay1992,
abstract = {Three Bayesian ideas are presented for supervised adaptive classiers. First, it is argued that the output of a classier should be obtained by marginalising over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves {\`{a}}moderation' of the most probable classi-er's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in (MacKay, 1992a, 1992b) can also be applied to classication problems. This framework success-fully chooses the magnitude of weight decay terms, and ranks solutions found using dierent numbers of hidden units. Third, an information{\{}based data selection criterion is derived and demonstrated within this framework. 1 Introduction A quantitative Bayesian framework has been described for learning of mappings in feedfor-ward networks (MacKay, 1992a, 1992b). It was demonstrated that thievidence' framework could successfully choose the magnitude and type of weight decay terms, and could choose between solutions using dierent numbers of hidden units. The framework also gives quan-tied error bars expressing the uncertainty in the network's outputs and its parameters. In (MacKay, 1992c) information{\{}based objective functions for active learning were discussed within the same framework. These three papers concentrated on interpolation (regression) problems. Neural net-works can also be trained to perform classication tasks.}}},
author = {MacKay, David J. C.},
doi = {10.1162/neco.1992.4.5.720},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Neural Computation/MacKay{\_}1992.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {bayesian,neural network},
mendeley-tags = {bayesian,neural network},
month = {sep},
number = {5},
pages = {720--736},
title = {{The Evidence Framework Applied to Classification Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.5.720},
volume = {4},
year = {1992}
}
@article{Neal1992,
abstract = {It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the "Hybrid Monte Carlo" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of "free energy" differences, it should also be possible to compare the merits of different network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks.},
author = {Neal, Radford M.},
doi = {10.1.1.53.5868},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Dept. of Computer Science, University of Toronto, Tech. {\ldots}/Neal{\_}1992.pdf:pdf},
isbn = {CRG-TR-92-1},
journal = {Dept. of Computer Science, University of Toronto, Tech. {\ldots}},
keywords = {bayesian,mcmc,neural network},
mendeley-tags = {bayesian,mcmc,neural network},
pages = {1--21},
title = {{Bayesian training of backpropagation networks by the hybrid Monte Carlo method}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Bayesian+Training+of+Backpropagation+Networks+by+the+Hybrid+Monte+Carlo+Method{\#}0},
year = {1992}
}
@article{Pruess2012,
author = {Pruess, K and Oldenburg, C and Moridis, G},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Pruess, Oldenburg, Moridis{\_}2012.pdf:pdf},
keywords = {tough2},
mendeley-tags = {tough2},
number = {September},
title = {{TOUGH2 User's guide}},
url = {http://esd.lbl.gov/TOUGHPLUS/manuals/TOUGH2{\_}V2{\_}Users{\_}Guide.pdf},
year = {2012}
}
@inproceedings{Maxwell2002,
abstract = {Microseismic monitoring has been used to image hydraulic fracture growth in the Barnett Shale. The Barnett is a naturally fractured shale reservoir, which causes significant complexity in fracture growth during well stimulation operations. Several Barnett treatments have been successfully imaged between March 2000 and December 2001. In this paper, examples will be given to illustrate the complexity and variability which is developed during the treatment as the slurry interacts with the pre-existing fracture sets. The microseismic images explain why the stimulations occasionally grow at an angle to the assumed fracture orientation and into neighboring wells. Differences in production rates from various wells could also be related to the fracture geometry. The implications of the images to reservoir management highlight the benefit of imaging individual fracture networks to avoid overlapping and for targeting potential new well locations.},
author = {Maxwell, S.C. and Urbancic, T.I. and Steinsberger, N. and Zinno, R.},
booktitle = {SPE Annual Technical Conference and Exhibition},
doi = {10.2118/77440-MS},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Annual Technical Conference and Exhibition/Maxwell et al.{\_}2002.pdf:pdf},
isbn = {9781555631536},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Microseismic Imaging of Hydraulic Fracture Complexity in the Barnett Shale}},
url = {http://www.onepetro.org/doi/10.2118/77440-MS},
year = {2002}
}
@article{McCormack1993,
abstract = {Interactive seismic processing systems for editing noisy seismic traces and picking first-break refraction events have been developed using a neural network learning algorithm. We employ a backpropagation neural network (BNN) paradigm modified to improve the convergence rate of the BNN. The BNN is interactively “trained” to edit seismic data or pick first breaks by a human processor who judiciously selects and presents to the network examples of trace edits or refraction picks. The network then iteratively adjusts a set of internal weights until it can accurately duplicate the examples provided by the user. After the training session is completed, the BNN system can then process new data sets in a manner that mimics the human processor. Synthetic modeling studies indicate that the BNN uses many of the same subjective criteria that humans employ in editing and picking seismic data sets. Automated trace editing and first‐break picking based on the modified BNN paradigm achieve 90 to 98 percent agreement wit...},
author = {McCormack, Michael D. and Zaucha, David E and Dushek, Dennis W},
doi = {10.1190/1.1443352},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/McCormack, Zaucha, Dushek{\_}1993.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {machine learning,neural network,picking},
mendeley-tags = {machine learning,neural network,picking},
month = {jan},
number = {1},
pages = {67--78},
title = {{First-break refraction event picking and seismic data trace editing using neural networks}},
url = {http://library.seg.org/doi/abs/10.1190/1.1443352 http://library.seg.org/doi/10.1190/1.1443352},
volume = {58},
year = {1993}
}
@inproceedings{Clerc1999,
abstract = {A very simple particle swarm optimization iterative algorithm is presented, with just one equation and one social/confidence parameter. We define a “no-hope” convergence criterion and a “rehope” method so that, from time to time, the swarm re-initializes its position, according to some gradient estimations of the objective function and to the previous re-initialization (it means it has a kind of very rudimentary memory). We then study two different cases, a quite “easy” one (the Alpine function) and a “difficult” one (the Banana function), but both just in dimension two. The process is improved by taking into account the swarm gravity center (the “queen”) and the results are good enough so that it is certainly worthwhile trying the method on more complex problems},
author = {Clerc, Maurice},
booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},
doi = {10.1109/CEC.1999.785513},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)/Clerc{\_}1999.pdf:pdf},
isbn = {0-7803-5536-9},
keywords = {constriction,pso},
mendeley-tags = {constriction,pso},
pages = {1951--1957},
publisher = {IEEE},
title = {{The swarm and the queen: towards a deterministic and adaptive particle swarm optimization}},
url = {http://ieeexplore.ieee.org/document/785513/},
volume = {3},
year = {1999}
}
@article{Noble2014,
abstract = {Seismic traveltimes and their spatial derivatives are the basis of many imaging methods such as pre-stack depth migration and tomography. A common approach to compute these quantities is to solve the eikonal equation with a finite-difference scheme. If many recently published algorithms for resolving the eikonal equation do now yield fairly accurate traveltimes for most applications, the spatial derivatives of traveltimes remain very approximate. To address this accuracy issue, we develop a new hybrid eikonal solver that combines a spherical approximation when close to the source and a plane wave approximation when far away. This algorithm reproduces properly the spherical behaviour of wave fronts in the vicinity of the source. We implement a combination of 16 local operators that enables us to handle velocity models with sharp vertical and horizontal velocity contrasts. We associate to these local operators a global fast sweeping method to take into account all possible directions of wave propagation. Our formulation allows us to introduce a variable grid spacing in all three directions of space. We demonstrate the efficiency of this algorithm in terms of computational time and the gain in accuracy of the computed traveltimes and their derivatives on several numerical examples.},
author = {Noble, Mark and Gesret, Alexandrine and Belayouni, Nidhal},
doi = {10.1093/gji/ggu358},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Noble, Gesret, Belayouni{\_}2014.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Computational seismology,Numerical solutions,Wave propagation,eikonal},
mendeley-tags = {eikonal},
number = {3},
pages = {1572--1585},
title = {{Accurate 3-D finite difference computation of traveltimes in strongly heterogeneous media}},
volume = {199},
year = {2014}
}
@article{Zhang2004,
abstract = {The genetic algorithm method is combined with the finite-element method for the first time as an alternative method to invert gravity anomaly data for reconstructing the 3D density structure in the subsurface. The method provides a global search in the model space for all acceptable models.The computational efficiency is significantly improved by storing the coefficient matrix and using it in all forward calculations, then by dividing the region of interest into many subregions and applying parallel processing to the subregions. Central Taiwan, a geologically complex region, is used as an example to demonstrate the utility of the method. A crustal block 120{\pounds}150 km2 in area and 34 km in thickness is represented by a finiteelement model of 76 500 cubic elements, each 2{\pounds}2{\pounds}2 km3 in size. An initial density model is reconstructed from the regional 3D tomographic seismic velocity using an empirical relation between velocity and density. The difference between the calculated and the observed gravity anomaly (i.e., the residual anomaly) shows an elongated minimum of large magnitude that extends along the axis of the Taiwan mountain belt. Among the interpretive models tested, the best model shows a crustal root extending to depths of 50 to 60 km beneath the axis of theWestern Central and Eastern Central Ranges with a density contrast of 400 or 500 kg/m3 across the Moho. Both predictions appear to be supported by independent seismological and laboratory evidence},
author = {Zhang, Jian and Wang, Chi‐Yuen and Shi, Yaolin and Cai, Yongen and Chi, Wu‐Cheng and Dreger, Douglas and Cheng, Win‐Bin and Yuan, Yen‐Horng},
doi = {10.1190/1.1778235},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Zhang et al.{\_}2004.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {ga},
mendeley-tags = {ga},
month = {jul},
number = {4},
pages = {917--924},
title = {{Three‐dimensional crustal structure in central Taiwan from gravity inversion with a parallel genetic algorithm}},
url = {http://library.seg.org/doi/10.1190/1.1778235},
volume = {69},
year = {2004}
}
@article{Mohamed2010,
abstract = {History matching and uncertainty quantification are two important research topics in reservoir simulation currently. In the Bayesian approach, we start with prior information about a reservoir - for example from analogue outcrop data - and update our reservoir models with observations, for example from production data or time lapse seismic. The goal of this activity is often to generate multiple models that match the history and use the models to quantify uncertainties in predictions of reservoir performance. A critical aspect of generating multiple history matched models is the sampling algorithm used to generate the models. Algorithms that have been studied include gradient methods, genetic algorithms, the Ensemble Kalman Filter, and others. This paper investigates the efficiency of three stochastic sampling algorithms: Hamiltonian Monte Carlo (HMC) algorithm, Particle Swarm Optimization (PSO) algorithm and the Neighborhood Algorithm (NA). HMC is a Markov Chain Monte Carlo (MCMC) technique that uses Hamiltonian dynamics to achieve larger jumps than are possible with other MCMC techniques. PSO is a swarm intelligence algorithm that uses similar dynamics to HMC to guide the search, but incorporates acceleration and damping parameters to provide rapid convergence to possible multiple minima. The Neighbourhood Algorithm is a sampling technique that uses the properties of Voronoi cells in high dimensions to achieve multiple history matched models. The algorithms are compared by generating multiple history matched reservoir models, and comparing the plO - p. 50 - p. 90 uncertainty bounds produced by each algorithm. We show that all the algorithms are able to find equivalent match qualities for this example, but that some algorithms are able to find good fitting results quickly, whereas others are able to find a more diverse set of models in parameter space. The effects of the different sampling of model parameter space are compared in terms of the p. 10 - p. 50 - p. 90 uncertainty bounds in forecast oil rate. These results show that algorithms based on Hamiltonian dynamics and swarm intelligence concepts have the potential to be effective tools in uncertainty quantification in the oil industry. Copyright 2009, Society of Petroleum Engineers.},
author = {Mohamed, L. and Christie, M. and Demyanov, V.},
doi = {10.2118/119139-PA},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Journal/Mohamed, Christie, Demyanov{\_}2010.pdf:pdf},
isbn = {9781605607771},
issn = {1086055X},
journal = {SPE Journal},
keywords = {pso,uncertainty quantification},
mendeley-tags = {pso,uncertainty quantification},
number = {March},
pages = {31--38},
title = {{Comparison of Stochastic Sampling Algorithms for Uncertainty Quantification}},
volume = {15},
year = {2010}
}
@article{Pruess1999,
author = {Pruess, Karsten and Oldenburg, Curt and Moridis, George},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Pruess, Oldenburg, Moridis{\_}1999.pdf:pdf},
keywords = {tough2},
mendeley-tags = {tough2},
number = {November},
title = {{TOUGH2 User's guide}},
year = {1999}
}
@article{Blanco-Martin2017,
abstract = {The TOUGH-FLAC simulator for coupled thermal-hydraulic-mechanical processes modeling has been extended to the finite strain framework. In the approach selected, this extension has required modifications to the flow simulator (TOUGH2) and to the coupling scheme between the geomechanics and the flow sub-problems. In TOUGH2, the mass and energy balance equations have been extended to account for volume changes. Additionally, as large deformations are computed by FLAC3D, the geometry is updated in the flow sub-problem. The Voronoi partition needed in TOUGH2 is computed using an external open source library (Voro++) that uses the centroids of the deformed geomechanics mesh as generators of the Voronoi diagram. TOUGH-FLAC in infinitesimal and finite strain frameworks is verified against analytical solutions and other approaches to couple flow and geomechanics. Within the finite strain framework, TOUGH-FLAC is also successfully applied to a large-scale case. The extension of TOUGH-FLAC to the finite strain framework has little impact to the user as only one additional executable is needed (for Voro++), and the input files and the workflow of a simulation are the same as in standard TOUGH-FLAC. With this new provision for finite strains, TOUGH-FLAC can be used in the analysis of a wider range of engineering problems, and the areas of application of this simulator are therefore broadened.},
author = {Blanco-Mart{\'{i}}n, Laura and Rutqvist, Jonny and Birkholzer, Jens T.},
doi = {10.1016/j.cageo.2016.10.015},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Blanco-Mart{\'{i}}n, Rutqvist, Birkholzer{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Coupled THM processes,FLAC3D,Finite strains,Sequential modeling,TOUGH2,Voro++,flac,tough2},
mendeley-tags = {flac,tough2},
month = {nov},
number = {October 2016},
pages = {64--71},
publisher = {Elsevier Ltd},
title = {{Extension of TOUGH-FLAC to the finite strain framework}},
url = {http://dx.doi.org/10.1016/j.cageo.2016.10.015 https://linkinghub.elsevier.com/retrieve/pii/S009830041630601X},
volume = {108},
year = {2017}
}
@article{Sambridge1992,
abstract = {Recently a new class of methods, to solve non-linear optimization problems, has generated considerable interest in the field of Artificial Intelligence. These methods, known as genetic algorithms, are able to solve highly non-linear and non-local optimization problems and belong to the class of global optimization techniques, which includes Monte Carlo and Simulated Annealing methods. Unlike local techniques, such as damped least squares or conjugate gradients, genetic algorithms avoid all use of curvature information on the objective function. This means that they do not require any derivative information and therefore one can use any type of misfit function equally well. Most iterative methods work with a single model and find improvements by perturbing it in some fashion. Genetic algorithms, however, work with a group of models simultaneously and use stochastic processes to guide the search for an optimal solution. Both Simulated Annealing and genetic algorithms are modelled on natural optimization systems. Simulated Annealing uses an analogy with thermodynamics; genetic algorithms have an analogy with biological evolution. This evolution leads to an efficient exchange of information between all models encountered, and allows the algorithm to rapidly assimilate and exploit the information gained to find better data fitting models. To illustrate the power of genetic algorithms compared to Monte Carlo, we consider a simple multidimensional quadratic optimization problem and show that its relative efficiency increases dramatically as the number of unknowns is increased. As an example of their use in a geophysical problem with real data we consider the non-linear inversion of marine seismic refraction waveforms. The results show that genetic algorithms are inherently superior to random search techniques and can also perform better than iterative matrix inversion which requires a good starting model. This is primarily because genetic algorithms are able to combine both local and global search mechanisms into a single efficient method. Since many forward and inverse problems involve solving an optimization problem, we expect that the genetic approach will find applications in many other geophysical problems; these include seismic ray tracing, earthquake location, non-linear data fitting and, possibly seismic tomography.},
author = {Sambridge, Malcolm and Drijkoningen, Guy},
doi = {10.1111/j.1365-246X.1992.tb00100.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Sambridge, Drijkoningen{\_}1992.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Genetic algorithms,Global optimization,Waveform inversion,ga,inversion},
mendeley-tags = {ga,inversion},
month = {may},
number = {2},
pages = {323--342},
publisher = {Geophysical Journal International},
title = {{Genetic algorithms in seismic waveform inversion}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1992.tb00100.x},
volume = {109},
year = {1992}
}
@article{Sen2017,
abstract = {ABSTRACTPrestack or angle stack gathers are inverted to estimate pseudologs at every surface location for building reservoir models. Recently, several methods have been proposed to increase the resolution of the inverted models. All of these methods, however, require that the total number of model parameters be fixed a priori. We have investigated an alternate approach in which we allow the data themselves to choose model parameterization. In other words, in addition to the layer properties, the number of layers is also treated as a variable in our formulation. Such transdimensional inverse problems are generally solved by using the reversible jump Markov chain Monte Carlo (RJMCMC) approach, which is a tool for model exploration and uncertainty quantification. This method, however, has very low acceptance. We have developed a two-step method by combining RJMCMC with a fixed-dimensional MCMC called Hamiltonian Monte Carlo, which makes use of gradient information to take large steps. Acceptance probability ...},
author = {Sen, Mrinal K. and Biswas, Reetam},
doi = {10.1190/geo2016-0010.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Sen, Biswas{\_}2017.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
number = {3},
pages = {R119--R134},
title = {{Transdimensional seismic inversion using the reversible jump Hamiltonian Monte Carlo algorithm}},
url = {http://library.seg.org/doi/10.1190/geo2016-0010.1},
volume = {82},
year = {2017}
}
@article{Louie2001,
author = {Louie, John N},
issn = {1943-3573},
journal = {Bulletin of the Seismological Society of America},
number = {2},
pages = {347--364},
publisher = {Seismological Society of America},
title = {{Faster, better: shear-wave velocity to 100 meters depth from refraction microtremor arrays}},
volume = {91},
year = {2001}
}
@article{Eberhart2001,
abstract = {This paper focuses on the engineering and computer science aspects of developments, applications, and resources related to particle swarm optimization. Developments in the particle swarm algorithm since its origin in 1995 are reviewed. Included are brief discussions of constriction factors, inertia weights, and tracking dynamic systems. Applications, both those already developed, and promising future application areas, are reviewed. Finally, resources related to particle swarm optimization are listed, including books, Web sites, and software. A particle swarm optimization bibliography is at the end of the paper},
author = {Eberhart, R.C. and {Yuhui Shi}},
doi = {10.1109/CEC.2001.934374},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)/Eberhart, Yuhui Shi{\_}2001.pdf:pdf},
isbn = {0-7803-6657-3},
journal = {Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)},
keywords = {Acceleration,Application software,Bibliographies,Books,Computer science,Evolutionary computation,Particle swarm optimization,Particle tracking,Power system dynamics,Writing,constriction factors,dynamic systems,evolutionary computation,inertia weights,particle swarm optimization,pso,tracking},
mendeley-tags = {pso},
pages = {81--86},
title = {{Particle swarm optimization: developments, applications and resources}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=934374},
volume = {1},
year = {2001}
}
@article{Clerc2002,
abstract = {The particle swarm is an algorithm for finding optimal regions of complex search spaces through the interaction of individuals in a population of particles. This paper analyzes a particle's trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A five-dimensional depiction is developed, which describes the system completely. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system's convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the ability of the particle swarm to find optima of some well-studied test functions.},
author = {Clerc, M. and Kennedy, J.},
doi = {10.1109/4235.985692},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Clerc, Kennedy{\_}2002.pdf:pdf},
isbn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {pso},
mendeley-tags = {pso},
number = {1},
pages = {58--73},
pmid = {21738602},
title = {{The particle swarm - explosion, stability, and convergence in a multidimensional complex space}},
volume = {6},
year = {2002}
}
@inproceedings{Lagos2014,
abstract = {SUMMARY In this work we implement very fast simulated annealing (VFSA) and particle swarm optimization (PSO) to locate mi-croseismic events. The two methods are compared to a linear grid search (GS) algorithm in terms of their computational effi-ciency and accuracy. To this end, two different downhole sce-narios are studied, considering the cases where either one or two monitoring wells are available. First, we carry out a set of tests based on the exact arrival times obtained for each model. Then, we analyze the performance of the methods using arrival times automatically picked from realistic synthetically gener-ated 3C records. We conclude about the better performance of VFSA compared to PSO and GS.},
author = {Lagos, Soledad R. and Sabbione, Juan I. and Velis, Danilo R.},
booktitle = {SEG Technical Program Expanded Abstracts 2014},
doi = {10.1190/segam2014-1216.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2014/Lagos, Sabbione, Velis{\_}2014.pdf:pdf},
keywords = {microseismic,optimization,signal processing},
month = {aug},
pages = {2188--2192},
publisher = {Society of Exploration Geophysicists},
title = {{Very fast simulated annealing and particle swarm optimization for microseismic event location}},
url = {http://library.seg.org/doi/abs/10.1190/segam2014-1216.1},
year = {2014}
}
@article{Fonseca1998,
author = {Fonseca, Carlos M and Fleming, Peter J},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans},
number = {1},
pages = {26--37},
publisher = {IEEE},
title = {{Multiobjective optimization and multiple constraint handling with evolutionary algorithms. I. A unified formulation}},
volume = {28},
year = {1998}
}
@article{Finsterle2017,
abstract = {iTOUGH2 is a simulation-optimization framework for the TOUGH suite of nonisothermal multiphase flow models and related simulators of geophysical, geochemical, and geomechanical processes. After appropriate parameterization of subsurface structures and their properties, iTOUGH2 runs simulations for multiple parameter sets and analyzes the resulting output for parameter estimation through automatic model calibration, local and global sensitivity analyses, data-worth analyses, and uncertainty propagation analyses. Development of iTOUGH2 is driven by scientific challenges and user needs, with new capabilities continually added to both the forward simulator and the optimization framework. This review article provides a summary description of methods and features implemented in iTOUGH2, and discusses the usefulness and limitations of an integrated simulation-optimization workflow in support of the characterization and analysis of complex multiphysics subsurface systems.},
author = {Finsterle, S. and Commer, M. and Edmiston, J.K. and Jung, Y. and Kowalsky, M.B. and Pau, G.S.H. and Wainwright, H.M. and Zhang, Y.},
doi = {10.1016/j.cageo.2016.09.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Finsterle et al.{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Data-worth analysis,Inverse modeling,Numerical simulation,Parameter estimation,Sensitivity analysis,Uncertainty quantification,itough,tough2},
mendeley-tags = {itough,tough2},
month = {nov},
pages = {8--20},
publisher = {Elsevier Ltd},
title = {{iTOUGH2: A multiphysics simulation-optimization framework for analyzing subsurface systems}},
url = {http://dx.doi.org/10.1016/j.cageo.2016.09.005 https://linkinghub.elsevier.com/retrieve/pii/S0098300416303983},
volume = {108},
year = {2017}
}
@book{Tarantola2005,
abstract = {While the prediction of observations is a forward problem, the use of actual observations to infer the properties of a model is an inverse problem. Inverse problems are difficult because they may not have a unique solution. The description of uncertainties plays a central role in the theory, which is based on probability theory. This book proposes a general approach that is valid for linear as well as for nonlinear problems. The philosophy is essentially probabilistic and allows the reader to understand the basic difficulties appearing in the resolution of inverse problems. The book attempts to explain how a method of acquisition of information can be applied to actual real-world problems, and many of the arguments are heuristic. Prompted by recent developments in inverse theory, Inverse Problem Theory and Methods for Model Parameter Estimation is a completely rewritten version of a 1987 book by the same author. In this version there are lots of algorithmic details for Monte Carlo methods, least-squares discrete problems, and least-squares problems involving functions. In addition, some notions are clarified, the role of optimization techniques is underplayed, and Monte Carlo methods are taken much more seriously. The first part of the book deals exclusively with discrete inverse problems with a finite number of parameters while the second part of the book deals with general inverse problems.},
author = {Tarantola, Albert},
booktitle = {Society for Industrial and Applied Mathematics},
doi = {10.1137/1.9780898717921},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Society for Industrial and Applied Mathematics/Tarantola{\_}2005.pdf:pdf},
isbn = {978-0-89871-572-9},
issn = {0001-4966},
keywords = {Inverse problems,inverse methods,inversion,least-squares,mcmc,probability,uncertainties},
mendeley-tags = {inversion,mcmc},
month = {jan},
pages = {1816--24},
pmid = {17069280},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Inverse Problem Theory and Methods for Model Parameter Estimation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17069280 http://epubs.siam.org/doi/book/10.1137/1.9780898717921},
volume = {120},
year = {2005}
}
@incollection{Alvarez-Benitez2005a,
abstract = {In extending the Particle Swarm Optimisation methodology tomulti-objective problems it is unclear how global guidesfor particles should be selected. Previous work hasrelied on metric information in objective space, althoughthis is at variance with the notion of dominance which isused to assess the quality of solutions. Here we proposemethods based exclusively on dominance for selectingguides from a non-dominated archive. The methods are evaluated on standard test problems and we find thatprobabilistic selection favouring archival particles thatdominate few particles provides good convergence towards$\backslash$nand coverage of the Pareto front. We demonstrate that thescheme is robust to changes in objective scaling. Wepropose and evaluate methods for confining particles to the feasible region, and find that allowing particles toexplore regions close to the constraint boundaries isimportant to ensure convergence to the Pareto front.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Alvarez-Benitez, Julio E and Everson, Richard M and Fieldsend, Jonathan E},
booktitle = {Evolutionary Multi-Criterion Optimization},
doi = {10.1007/978-3-540-31880-4_32},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Evolutionary Multi-Criterion Optimization/Alvarez-Benitez, Everson, Fieldsend{\_}2005.pdf:pdf},
isbn = {978-3-540-24983-2},
issn = {03029743},
keywords = {pso},
mendeley-tags = {pso},
pages = {459--473},
pmid = {25246403},
title = {{A MOPSO Algorithm Based Exclusively on Pareto Dominance Concepts}},
url = {http://dx.doi.org/10.1007/978-3-540-31880-4{\_}32 http://link.springer.com/10.1007/978-3-540-31880-4{\_}32},
volume = {3410},
year = {2005}
}
@inproceedings{Shi1998,
abstract = {Evolutionary computation techniques, genetic algorithms, evolutionary strategies and genetic programming are motivated by the evolution of nature. A population of individuals, which encode the problem solutions are manipulated according to the rule of survival of the fittest through {\&}ldquo;genetic{\&}rdquo; operations, such as mutation, crossover and reproduction. A best solution is evolved through the generations. In contrast to evolutionary computation techniques, Eberhart and Kennedy developed a different algorithm through simulating social behavior (R.C. Eberhart et al., 1996; R.C. Eberhart and J. Kennedy, 1996; J. Kennedy and R.C. Eberhart, 1995; J. Kennedy, 1997). As in other algorithms, a population of individuals exists. This algorithm is called particle swarm optimization (PSO) since it resembles a school of flying birds. In a particle swarm optimizer, instead of using genetic operators, these individuals are {\&}ldquo;evolved{\&}rdquo; by cooperation and competition among the individuals themselves through generations. Each particle adjusts its flying according to its own flying experience and its companions' flying experience. We introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the significant and effective impact of this new parameter on the particle swarm optimizer},
author = {Shi, Yuhui and Eberhart, R C},
booktitle = {1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)},
doi = {10.1109/ICEC.1998.699146},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat.98TH8360)/Shi, Eberhart{\_}1998.pdf:pdf},
isbn = {0-7803-4869-9},
issn = {1549-9596},
keywords = {competition,cooperation,evolutionary computation techniques,evolutionary strategies,flying birds,flying experience,genetic algorithms,genetic programming,inertia weight,iterative methods,modified particle swarm optimizer,particle swarm optimization,pso,search problems,social behavior simulation,survival of the fittest},
mendeley-tags = {pso},
pages = {69--73},
pmid = {19366194},
publisher = {IEEE},
title = {{A modified particle swarm optimizer}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=699146 http://ieeexplore.ieee.org/document/699146/},
year = {1998}
}
@book{Nazarian1983,
author = {Nazarian, Soheil and Stokoe, I I and Kenneth, H and Hudson, W R},
isbn = {0309036208},
number = {930},
title = {{Use of spectral analysis of surface waves method for determination of moduli and thicknesses of pavement systems}},
year = {1983}
}
@article{Palagi1994,
author = {Palagi, C.L. and Aziz, Khalid},
doi = {10.2118/22889-PA},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Advanced Technology Series/Palagi, Aziz{\_}1994.pdf:pdf},
issn = {1076-0148},
journal = {SPE Advanced Technology Series},
keywords = {voronoi},
mendeley-tags = {voronoi},
month = {apr},
number = {02},
pages = {69--77},
title = {{Use of Voronoi Grid in Reservoir Simulation}},
url = {http://www.onepetro.org/doi/10.2118/22889-PA},
volume = {2},
year = {1994}
}
@incollection{Maxwell2019a,
author = {Maxwell, Shawn},
booktitle = {Geophysics and Geosequestration},
doi = {10.1017/9781316480724.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics and Geosequestration/Maxwell{\_}2019.pdf:pdf},
isbn = {9781316480724},
keywords = {decatur},
mendeley-tags = {decatur},
month = {apr},
number = {May},
pages = {168--180},
publisher = {Cambridge University Press},
title = {{Microseismic Imaging of CO2 Injection}},
url = {https://www.cambridge.org/core/product/identifier/9781316480724{\%}23CN-bp-10/type/book{\_}part},
year = {2019}
}
@article{Hirata1987,
abstract = {A new algorithm is applied to inverting arrival time data for hypocenter location. The algorithm incorporates both observed and prior data from a Bayesian point of view. We define marginal probability density function (pdf) to eliminate the origin time from the location problem; the posterior pdf of hypocenter parameters is integrated over the whole range of the origin time. The best estimate of the hypocenter is defined as a set of spatial coordinates which maximizes the marginal pdf. Assuming Gaussian errors in both observed and prior data, we obtain a simple algorithm. Estimation errors of parameters are evaluated by an asymptotic covariance matrix, with which an asymptotic posterior pdf is computed. The algorithm is applied to observed data and is tested. An example of analysis is given for aftershocks of the 1969 Gifuken-chubu earthquake (M = 6.6) reported by the Japan Meteorological Agency (JMA). The spatial distribution of the aftershocks is supposed to be Gaussian with standard deviation of 15 km. A center of the aftershock distribution, which gives the prior estimates of hypocenters, is also estimated from observed data. Results of the nonlinear inversion of arrival time data are examined in terms of the asymptotic posterior pdf. We found that relocated hypocenters of the aftershocks are concentrated in a narrow region of 2-3 km in width, while the hypocenters previously reported by JMA have a wide distribution of 5-7 km. ?? 1987.},
author = {Hirata, Naoshi and Matsu'ura, Mitsuhiro},
doi = {10.1016/0031-9201(87)90066-5},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Physics of the Earth and Planetary Interiors/Hirata, Matsu'ura{\_}1987.pdf:pdf},
isbn = {0031-9201},
issn = {00319201},
journal = {Physics of the Earth and Planetary Interiors},
keywords = {location},
mendeley-tags = {location},
number = {C},
pages = {50--61},
title = {{Maximum-likelihood estimation of hypocenter with origin time eliminated using nonlinear inversion technique}},
volume = {47},
year = {1987}
}
@techreport{Zehner2011,
abstract = {Finite element based simulation is an important tool to assess processes in the subsurface, such as groundwater flow or CO2 storage in reservoirs. The simulation requires the generation of suit- able input models which are usually generated using specialized software packages for geometri- cal modelling such as GOCAD. Generally this software supports two types of 3D grids: struc- tured grids that consist of hexahedra and unstructured grids that use tetrahedra. To represent complicated geological structures, tetrahedral grids would be preferable but unfortunately their construction is less well supported. In order to generate tetrahedral grids with sufficient quality, the quality of the triangular meshes that define the boundary representation of the model must be higher than is normally required for illustration and communication purposes and they must fulfil certain constraints. This contribution will explain which constraints these are and how the con- struction of complicated geological structures, such as faulted systems, can be done in order to meet these requirements. The software GOCAD can be extended using a plugin mechanism and so we have written several plugins that support simple interaction or modelling functionality that are not yet present in GOCAD and which in addition provide an interface from the modelling software to external meshing tools that are free for non-commercial use. Further plugins have been implemented that provide interfaces with the finite element simulation software OpenGeo- Sys that is developed at the UFZ and to the software Paraview for the scientific visualization of the results. While we use GOCAD in particular, the problems described and the solutions and workflows suggested are general and should be of interest to users of other software packages as well. 1},
author = {Zehner, Bj{\"{o}}rn},
booktitle = {IAMG Congress},
doi = {10.5242/iamg.2011.0069},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IAMG Congress/Zehner{\_}2011.pdf:pdf},
keywords = {mesh},
mendeley-tags = {mesh},
month = {sep},
pages = {1--13},
title = {{Constructing Geometric Models of the Subsurface for Finite Element Simulation}},
url = {http://www.ufz.eu/export/data/1/20123{\_}Zehner{\_}IAMG{\_}2011{\_}Salzburg.pdf http://www.cogeo.at/publications/iamg2011/IAMG2011{\_}Salzburg{\_}069.pdf},
year = {2011}
}
@article{Ulrych2001,
author = {Ulrych, Tadeusz J. and Sacchi, Mauricio D. and Woodbury, Alan},
doi = {10.1190/1.1444923},
issn = {0016-8033},
journal = {Geophysics},
month = {jan},
number = {1},
pages = {55--69},
title = {{A Bayes tour of inversion: A tutorial}},
url = {https://library.seg.org/doi/10.1190/1.1444923},
volume = {66},
year = {2001}
}
@article{Nicolas2016,
author = {Nicolas, Aur{\'{e}}lien and Fortin, J{\'{e}}r{\^{o}}me and Regnet, Jean-Baptiste and Dimanov, Alexandre and G{\'{u}}eguen, Yves},
doi = {10.1093/gji/ggw154},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Nicolas et al.{\_}2016.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Elasticity and anelasticity,Fracture and flow,Microstructures,Plasticity, diffusion, and creep,and creep,diffusion},
number = {1},
pages = {438--456},
title = {{Brittle and semi-brittle behaviours of a carbonate rock: Influence of water and temperature}},
volume = {206},
year = {2016}
}
@inproceedings{Bhat2006,
abstract = {The training of neural networks can be viewed as a problem of inference, which can be addressed from a Bayesian viewpoint. This perspective leads to a method, new to the field of particle physics, called Bayesian neural networks (BNN). After a brief overview of the method we illustrate how it can be usefully deployed in particle physics research.},
author = {Bhat, Pushpalatha C. and Prosper, Harrison B.},
booktitle = {Statistical Problems in Particle Physics, Astrophysics and Cosmology},
doi = {10.1142/9781860948985_0032},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Statistical Problems in Particle Physics, Astrophysics and Cosmology/Bhat, Prosper{\_}2006.pdf:pdf},
isbn = {978-1-86094-649-3},
keywords = {bayesian,bayesian techniques,feedforward networks,machine learning,neural network,statistical pattern recognition},
mendeley-tags = {bayesian,machine learning,neural network},
month = {may},
number = {1},
pages = {151--154},
publisher = {PUBLISHED BY IMPERIAL COLLEGE PRESS AND DISTRIBUTED BY WORLD SCIENTIFIC PUBLISHING CO.},
title = {{Bayesian Neural Networks}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9781860948985{\_}0032},
volume = {4},
year = {2006}
}
@article{DalMoro2007,
abstract = {Surface wave dispersion curve inversion is a challenging problem for linear inversion procedures due to its highly non-linear nature and to the large numbers of local minima and maxima of the objective function (multi-modality). In order to improve the reliability of the inversion results, we implemented and tested a two-step inversion scheme based on Genetic Algorithms (GAs). The proposed scheme performs several preliminary "parallel" runs (first step) and a final global run using the previously-determined fittest models as starting population. In this work we focus on the inversion of shear-wave velocity and layer thickness while fixing compressional-wave velocity and density according to user-defined Poisson's ratios and velocity-density relationship respectively. The procedure can nonetheless perform the inversion under different degrees of regularization, depending on the a priori information and the desired degree of freedom of the system. Thanks to the large number of considered models, in addition to the fittest model, a mean model and its accuracy are evaluated by means of a statistical approach based on the estimation of the Marginal Posterior Probability Density (MPPD). We tested the proposed GA-based inversion scheme on three synthetic models reproducing a complex structure with low-to-moderate velocity cover (also including a low-velocity channel) lying over hard bedrock. For all the considered cases the bedrock velocity and depth were properly identified, and velocity inversion was reconstructed with minor uncertainties. The performed tests also investigate the influence of the first higher mode, the reduction of the frequency range of the considered dispersion curve as well as the use of different number of strata. While a limited frequency range of the dispersion curve (maximum frequency reduced from 80 to 40 Hz) does not seem to significantly limit the accuracy of the retrieved model, the adoption of the correct number of strata and the addition of the first higher mode help better focus the final solution. In conclusion, the proposed approach represents an improvement of a purely GA-based optimization scheme and the MPPD-based mean model typically offers a more significant and precise solution than the fittest one. Results of the inversion performed on a field data set were validated by borehole stratigraphy. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {{Dal Moro}, Giancarlo and Pipan, Michele and Gabrielli, Paolo},
doi = {10.1016/j.jappgeo.2006.04.002},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Dal Moro, Pipan, Gabrielli{\_}2007.pdf:pdf},
isbn = {09269851},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Dispersion curve inversion,Genetic algorithms (GAs),Geophysical data inversion,Posterior probability density (PPD),Rayleigh waves,Surface waves,surface wave},
mendeley-tags = {surface wave},
month = {jan},
number = {1},
pages = {39--55},
title = {{Rayleigh wave dispersion curve inversion via genetic algorithms and Marginal Posterior Probability Density estimation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0926985106000541},
volume = {61},
year = {2007}
}
@article{VanDenBergh2006,
abstract = {Particle swarm optimization (PSO) has shown to be an efficient, robust and simple optimization algorithm. Most of the PSO studies are empirical, with only a few theoretical analyses that concentrate on understanding particle trajectories. These theoretical studies concentrate mainly on simplified PSO systems. This paper overviews current theoretical studies, and extend these studies to investigate particle trajectories for general swarms to include the influence of the inertia term. The paper also provides a formal proof that each particle converges to a stable point. An empirical analysis of multi-dimensional stochastic particles is also presented. Experimental results are provided to support the conclusions drawn from the theoretical findings. ?? 2005 Elsevier Inc. All rights reserved.},
author = {{Van Den Bergh}, F. and Engelbrecht, A. P.},
doi = {10.1016/j.ins.2005.02.003},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Information Sciences/Van Den Bergh, Engelbrecht{\_}2006.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Convergence,Equilibrium,Particle swarm optimization,Particle trajectories,pso},
mendeley-tags = {pso},
number = {8},
pages = {937--971},
title = {{A study of particle swarm optimization particle trajectories}},
volume = {176},
year = {2006}
}
@article{Conti2017,
abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
archivePrefix = {arXiv},
arxivId = {1712.06560},
author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1712.06560},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Conti et al.{\_}2017.pdf:pdf},
month = {dec},
title = {{Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents}},
url = {http://arxiv.org/abs/1712.06560},
year = {2017}
}
@article{Noble2010,
author = {Noble, Mark and Thierry, Philippe and Taillandier, Cedric and Calandra, Henri},
doi = {10.1190/1.3284057},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/The Leading Edge/Noble et al.{\_}2010.pdf:pdf},
issn = {1070-485X},
journal = {The Leading Edge},
month = {jan},
number = {1},
pages = {86--93},
title = {{High-performance 3D first-arrival traveltime tomography}},
url = {http://library.seg.org/doi/10.1190/1.3284057},
volume = {29},
year = {2010}
}
@article{Zhang2015,
abstract = {Particle swarm optimization (PSO) is a heuristic global optimization method, proposed originally by Kennedy and Eberhart in 1995. It is now one of the most commonly used optimization techniques. This survey presented a comprehensive investigation of PSO. On one hand, we provided advances with PSO, including its modifications (including quantum-behaved PSO, bare-bones PSO, chaotic PSO, and fuzzy PSO), population topology (as fully connected, von Neumann, ring, star, random, etc.), hybridization (with genetic algorithm, simulated annealing, Tabu search, artificial immune system, ant colony algorithm, artificial bee colony, differential evolution, harmonic search, and biogeography-based optimization), extensions (to multiobjective, constrained, discrete, and binary optimization), theoretical analysis (parameter selection and tuning, and convergence analysis), and parallel implementation (in multicore, multiprocessor, GPU, and cloud computing forms). On the other hand, we offered a survey on applications of PSO to the following eight fields: electrical and electronic engineering, automation control systems, communication theory, operations research, mechanical engineering, fuel and energy, medicine, chemistry, and biology. It is hoped that this survey would be beneficial for the researchers studying PSO algorithms.},
archivePrefix = {arXiv},
arxivId = {931256},
author = {Zhang, Yudong and Wang, Shuihua and Ji, Genlin},
doi = {10.1155/2015/931256},
eprint = {931256},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Mathematical Problems in Engineering/Zhang, Wang, Ji{\_}2015.pdf:pdf},
isbn = {0300-5410},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
pages = {1--38},
title = {{A Comprehensive Survey on Particle Swarm Optimization Algorithm and Its Applications}},
url = {http://www.hindawi.com/journals/mpe/2015/931256/},
volume = {2015},
year = {2015}
}
@article{Das2011,
abstract = {Differential evolution (DE) is arguably one of the most powerful stochastic real-parameter optimization algorithms in current use. DE operates through similar computational steps as employed by a standard evolutionary algorithm (EA). However, unlike traditional EAs, the DE-variants perturb the current-generation population members with the scaled differences of randomly selected and distinct population members. Therefore, no separate probability distribution has to be used for generating the offspring. Since its inception in 1995, DE has drawn the attention of many researchers all over the world resulting in a lot of variants of the basic algorithm with improved performance. This paper presents a detailed review of the basic concepts of DE and a survey of its major variants, its application to multiobjective, constrained, large scale, and uncertain optimization problems, and the theoretical studies conducted on DE so far. Also, it provides an overview of the significant engineering applications that have benefited from the powerful nature of DE.},
author = {Das, Swagatam and Suganthan, Ponnuthurai Nagaratnam},
doi = {10.1109/TEVC.2010.2059031},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Evolutionary Computation/Das, Suganthan{\_}2011.pdf:pdf},
isbn = {1089-778X VO - 15},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Derivative-free optimization,differential evolution (DE),direct search,evolutionary algorithms (EAs),genetic algorithms (GAs),metaheuristics,particle swarm optimization (PSO)},
month = {feb},
number = {1},
pages = {4--31},
title = {{Differential Evolution: A Survey of the State-of-the-Art}},
url = {http://ieeexplore.ieee.org/document/5601760/},
volume = {15},
year = {2011}
}
@article{Baillard2014,
abstract = {We present an automatic P- and S-wave onset-picking algorithm, using kurtosis-derived characteristic functions (CF) and eigenvalue decompositions on three-component seismic data. We modified the kurtosis CF (Saragiotis et al., 2002) to improve pick precision by computing the CF over several frequency bandwidths, window sizes, and smoothing parameters. Once phases are picked, our algorithm determines the onset type (P or S) using polarization parameters, removes bad picks using a clustering procedure and the signal-to-noise ratio (SNR) and assigns a pick quality index based on the SNR. We tested our algorithm on data from two different networks: (1) a 30-station, 100x100 km array of mostly onland wideband seismometers in a subduction context and (2) a four-station, 7x4 km array of ocean-bottom seismometers over a midocean ridge volcano. We compared picks from the automatic algorithm with manual and short-term average/long-term average (STA/LTA)-based automatic picks on subsets of each dataset. For the larger array, the automatic algorithm resulted in more locations than manual picking (133 versus 93 locations out of 163 total events detected), picking as many P onsets and twice as many S onsets as with manual picking or the STA/LTA algorithm. The difference between manual and automatic pick times for P-wave onsets was 0.01{\{}+/-{\}}0.08 s overall, compared with -0.18{\{}+/-{\}}0.19 s using the STA/LTA picker. For S-wave onsets, the difference was -0.09{\{}+/-{\}}0.23 s, which is comparable to the STA/LTA picker, but our picker provided nearly twice as many picks. The pick accuracy was constant over the range of event magnitudes (0.7-3.7 Ml). For the smaller array, the time difference between our algorithm and manual picks is 0.04{\{}+/-{\}}0.17 s for P waves and 0.07{\{}+/-{\}}0.08 s for S waves. Misfit between the automatic and manual picks is significantly lower using our procedure than using the STA/LTA algorithm.},
author = {Baillard, Christian and Crawford, Wayne C. and Ballu, Val{\'{e}}rie and Hibert, Cl{\'{e}}ment and Mangeney, Anne},
doi = {10.1785/0120120347},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Baillard et al.{\_}2014.pdf:pdf},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
keywords = {picking},
mendeley-tags = {picking},
month = {feb},
number = {1},
pages = {394--409},
title = {{An Automatic Kurtosis-Based P- and S-Phase Picker Designed for Local Seismic Networks}},
url = {http://www.bssaonline.org/cgi/doi/10.1785/0120120347},
volume = {104},
year = {2014}
}
@article{Monteiller2005,
abstract = {Improving our understanding of crustal processes requires a better knowledge of the geometry and the position of geological bodies. In this study we have designed a method based upon double-difference relocation and tomography to image, as accurately as possible, a heterogeneous medium containing seismogenic objects. Our approach consisted not only of incorporating double difference in tomography but also partly in revisiting tomographic schemes for choosing accurate and stable numerical strategies, adapted to the use of cross-spectral time delays. We used a finite difference solution to the eikonal equation for travel time computation and a Tarantola-Valette approach for both the classical and double-difference three-dimensional tomographic inversion to find accurate earthquake locations and seismic velocity estimates. We estimated efficiently the square root of the inverse model's covariance matrix in the case of a Gaussian correlation function. It allows the use of correlation length and a priori model variance criteria to determine the optimal solution. Double-difference relocation of similar earthquakes is performed in the optimal velocity model, making absolute and relative locations less biased by the velocity model. Double-difference tomography is achieved by using high-accuracy time delay measurements. These algorithms have been applied to earthquake data recorded in the vicinity of Kilauea and Mauna Loa volcanoes for imaging the volcanic structures. Stable and detailed velocity models are obtained: the regional tomography unambiguously highlights the structure of the island of Hawaii and the double-difference tomography shows a detailed image of the southern Kilauea caldera-upper east rift zone magmatic complex.},
author = {Monteiller, Vadim and Got, Jean Luc and Virieux, Jean and Okubo, Paul},
doi = {10.1029/2004JB003466},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Monteiller et al.{\_}2005.pdf:pdf},
isbn = {0148-0227},
issn = {21699356},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {location},
mendeley-tags = {location},
number = {12},
pages = {1--22},
title = {{An efficient algorithm for double-difference tomography and location in heterogeneous media, with an application to the Kilauea volcano}},
volume = {110},
year = {2005}
}
@article{Wolfe2002,
abstract = {This article examines the properties of difference operators that are used to relocate earthquakes and remove path anomaly biases. There are presently three established algorithms based on such techniques: (1) the method of Jordan and Sverdrup (1981), (2) the double-difference method of Got et al. (1994), and (3) the modified double-difference method of Waldhauser and Ellsworth (2001). We show that the underlying mathematics of these three methods are similar, although there are distinct contrasts in how each is adapted. Our results provide insight into the performance of individual methods. Both the Jordan and Sverdrup (1981) and double difference methods (Got et al., 1994; Waldhauser and Ellsworth, 2001) remove the average path anomaly bias in a set of events, but the equation weighting is more ideal in the first method. Distance dependent weighting in the Waldhauser and Ellsworth (2001) method does not reduce earthquake location-dependent path anomaly bias unless damping is applied, but damping causes the locations between earthquakes spaced far apart to be less well resolved. Alternatively, the results using Jordan and Sverdrup (1981) and Got et al. (1994) only remove a constant bias across a model subregion and cannot resolve the relative locations between subregions. The results of this study indicate that differencing operators contain the fundamental limitation that when the path anomalies from velocity heterogeneity change stongly with earthquake position, the bias effects can be reduced in the relative locations between closely spaced earthquakes, but the effects cannot be reduced in the relative locations between earthquakes spaced far apart.},
author = {Wolfe, Cecily J.},
doi = {10.1785/0120010189},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Wolfe{\_}2002.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {location},
mendeley-tags = {location},
number = {8},
pages = {2879--2892},
title = {{On the mathematics of using difference operators to relocate earthquakes}},
volume = {92},
year = {2002}
}
@article{Belhadj2018a,
author = {Belhadj, Jihane and Romary, Thomas and Gesret, Alexandrine and Noble, Mark and Figliuzzi, Bruno},
doi = {10.1088/1361-6420/aabce7},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Inverse Problems/Belhadj et al.{\_}2018.pdf:pdf},
issn = {0266-5611},
journal = {Inverse Problems},
month = {jun},
number = {6},
pages = {065007},
title = {{New parameterizations for Bayesian seismic tomography}},
url = {http://stacks.iop.org/0266-5611/34/i=6/a=065007?key=crossref.c41a7424f0d5ea28675e3b9248845416},
volume = {34},
year = {2018}
}
@article{Ruzek2001,
abstract = {A novel global optimizing algorithm -- Differential Evolution (DE) -- has appeared recently. This method is easy and advantageous when used for kinematic location of the earthquake hypocenter. The motivation for implementing a robust (i.e., global and nonlinear) optimizing algorithm for the location problem is to obtain better results than those from the classical (i.e., linearized) approach (such as the FASTHYPO, HYPOELLIPSE, HYPOCENTER solutions, among others). Better solutions have lower {\{}$\backslash$textregistered{\}}nal mis{\{}$\backslash$textregistered{\}}ts expressed as the common L2 norm. The features of the DE algorithm are studied on a set of synthetic location problems. The DE procedure is controlled by 3 internal parameters, which are easy to adjust, and the convergence properties are very good. Location results using DE are compared with the HYPO71 solutions for real earthquake data from the Gulf of Corinth region, Greece. The DE results are signi{\{}$\backslash$textregistered{\}}cantly better. The DE optimizing algorithm seems to be very promising both for the location problem as well as for other problems in geophysics.},
author = {Rů{\v{z}}ek, B. and Kvasni{\v{c}}ka, M},
doi = {10.1007/PL00001199},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Pure and Applied Geophysics/Rů{\v{z}}ek, Kvasni{\v{c}}ka{\_}2001.pdf:pdf},
issn = {0033-4533},
journal = {Pure and Applied Geophysics},
keywords = {de,evolution strategy,geophysical inversion,global optimization,hypocenter location,nonlinear optimization,stochastic optimization},
mendeley-tags = {de},
month = {apr},
number = {4},
pages = {667--693},
title = {{Differential Evolution Algorithm in the Earthquake Hypocenter Location}},
url = {http://link.springer.com/10.1007/PL00001199},
volume = {158},
year = {2001}
}
@article{Mikki2008,
author = {Mikki, Said M and Kishk, Ahmed A},
doi = {10.2200/S00110ED1V01Y200804CEM020},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Synthesis Lectures on Computational Electromagnetics/Mikki, Kishk{\_}2008.pdf:pdf},
issn = {1932-1252},
journal = {Synthesis Lectures on Computational Electromagnetics},
month = {jan},
number = {1},
pages = {1--103},
title = {{Particle Swarm Optimization: A Physics-Based Approach}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00110ED1V01Y200804CEM020},
volume = {3},
year = {2008}
}
@article{Schaff2004,
abstract = {Earthquake location using relative arrival time measurements can lead to dramatically reduced location errors and a view of fault-zone processes with unprecedented detail. There are two principal reasons why this approach reduces location errors. The first is that the use of differenced arrival times to solve for the vector separation of earthquakes removes from the earthquake location problem much of the error due to unmodeled velocity structure. The second reason, on which we focus in this article, is that waveform cross correlation can substantially reduce measurement error. While cross correlation has long been used to determine relative arrival times with subsample precision, we extend correlation measurements to less similar waveforms, and we introduce a general quantitative means to assess when correlation data provide an improvement over catalog phase picks. We apply the technique to local earthquake data from the Calaveras Fault in northern California. Tests for an example streak of 243 earthquakes demonstrate that relative arrival times with normalized cross correlation coefficients as low as similar to70{\%}, interevent separation distances as large as to 2 km, and magnitudes up to 3.5 as recorded on the Northern California Seismic Network are more precise than relative arrival times determined from catalog phase data. Also discussed are improvements made to the correlation technique itself. We find that for large time offsets, our implementation of time-domain cross correlation is often more robust and that it recovers more observations than the cross spectral approach. Longer time windows give better results than shorter ones. Finally, we explain how thresholds and empirical weighting functions may be derived to optimize the location procedure for any given region of interest, taking advantage of the respective strengths of diverse correlation and catalog phase data on different length scales.},
author = {Schaff, David P. and Bokelmann, G{\"{o}}tz H R and Ellsworth, William L. and Zanzerkia, Eva and Waldhauser, Felix and Beroza, Gregory C.},
doi = {10.1785/0120020238},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Schaff et al.{\_}2004.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
number = {2},
pages = {705--721},
title = {{Optimizing correlation techniques for improved earthquake location}},
volume = {94},
year = {2004}
}
@article{Templeton2017,
abstract = {To improve the detection of microearthquakes down to very small magnitude levels, we apply the empirical Matched Field Processing (MFP) method to 14.5 months of continuous borehole seismic data from the USGS seismic network at Decatur, Illinois and identify 95{\%} more events than were detected in the original catalog. Improved microseismic event detection is one component of a new Microseismic Toolkit that aims to reduce specific technical difficulties that make seismic hazard assessments for CO2 injection and storage projects difficult. These tools can also allow for an improved understanding of the subsurface reservoir at the Illinois Basin - Decatur Project (IDBP) site as observed through changes in the dynamic microearthquake activity and in the more slowly-varying subsurface medium changes associated with CO2 plume growth.},
author = {Templeton, Dennise and Matzel, Eric and Morency, Christina and White, Joshua},
doi = {10.1016/j.egypro.2017.03.1544},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Energy Procedia/Templeton et al.{\_}2017.pdf:pdf},
issn = {18766102},
journal = {Energy Procedia},
keywords = {Decatur,decatur,fluid injection,induced seismicity,source location},
mendeley-tags = {decatur},
month = {jul},
number = {November 2016},
pages = {4040--4046},
publisher = {The Author(s)},
title = {{Seismic Characterization of the Decatur, Illinois (USA) Carbon Capture and Storage Site}},
url = {http://dx.doi.org/10.1016/j.egypro.2017.03.1544 https://linkinghub.elsevier.com/retrieve/pii/S1876610217317381},
volume = {114},
year = {2017}
}
@article{Bodin2012a,
abstract = {Interpolation of spatial data is a widely used technique across the Earth sciences. For example, the thickness of the crust can be estimated by different active and passive seismic source surveys, and seismologists reconstruct the topography of the Moho by interpolating these different estimates. Although much research has been done on improving the quantity and quality of observations, the interpolation algorithms utilized often remain standard linear regression schemes, with three main weaknesses: (1) the level of structure in the surface, or smoothness, has to be predefined by the user; (2) different classes of measurements with varying and often poorly constrained uncertainties are used together, and hence it is difficult to give appropriate weight to different data types with standard algorithms; (3) there is typically no simple way to propagate uncertainties in the data to uncertainty in the estimated surface. Hence the situation can be expressed by Mackenzie (2004): “We use fantastic telescopes, the best physical models, and the best computers. The weak link in this chain is interpreting our data using 100 year old mathematics”. Here we use recent developments made in Bayesian statistics and apply them to the problem of surface reconstruction. We show how the reversible jump Markov chain Monte Carlo (rj-McMC) algorithm can be used to let the degree of structure in the surface be directly determined by the data. The solution is described in probabilistic terms, allowing uncertainties to be fully accounted for. The method is illustrated with an application to Moho depth reconstruction in Australia.},
author = {Bodin, T. and Salmon, M. and Kennett, B. L N and Sambridge, M.},
doi = {10.1029/2012JB009547},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Bodin et al.{\_}2012(2).pdf:pdf},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {Australia,Bayesian inference,Moho discontinuity,Monte Carlo inversion,spatial data analysis,surface reconstruction},
number = {10},
pages = {1--13},
title = {{Probabilistic surface reconstruction from multiple data sets: An example for the Australian Moho}},
volume = {117},
year = {2012}
}
@incollection{SCHWAB1972,
author = {Schwab, F.A. and Knopoff, L.},
booktitle = {METHODS IN COMPUTATIONAL PHYSICS: Advances in Research and Applications: Volume 11: Seismology: Surface Waves and Earth Oscillations},
doi = {10.1016/B978-0-12-460811-5.50008-8},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/METHODS IN COMPUTATIONAL PHYSICS Advances in Research and Applications Volume 11 Seismology Surface Waves and Earth Oscillations/Schwab, Knopoff{\_}1972.pdf:pdf},
issn = {0076-6860},
number = {1960},
pages = {87--180},
publisher = {ACADEMIC PRESS, INC.},
title = {{Fast Surface Wave and Free Mode Computations}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780124608115500088},
volume = {11},
year = {1972}
}
@article{Ba2014,
abstract = {We present a math-heuristic algorithm for the lot sizing problem with carryover. The proposed algorithm uses mathematical$\backslash$n programming techniques in a metaheuristic fashion to iteratively solve smaller portions of the original problem. More specifically,$\backslash$n we draw ideas from the corridor method to design and impose exogenous constraints on the original problem and, subsequently,$\backslash$n we solve to optimality the constrained problem using a MIP solver. The algorithm iteratively builds new corridors around the$\backslash$n best solution found within each corridor and, therefore, explores adjacent portions of the search space. In the attempt of$\backslash$n fostering diversification while exploring the original search space, we generate a pool of incumbent solutions for the corridor$\backslash$n method and, therefore, we reapply the corridor method using different starting points. The algorithm has been tested on instances$\backslash$n of a standard benchmark library and the reported results show the robustness and effectiveness of the proposed scheme.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Ba, Raul and Ortega, Julio},
doi = {10.1007/978-3-662-45523-4},
eprint = {9780201398298},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Ba, Ortega{\_}2014.pdf:pdf},
isbn = {978-3-662-45522-7},
issn = {03029743},
keywords = {hybrid mpi,mpi,multi-core processors,openmp,parallel evolutionary algorithms,vehicle routing problems},
pages = {653--664},
pmid = {4520227},
title = {{Applications of Evolutionary Computation}},
url = {http://link.springer.com/10.1007/978-3-662-45523-4},
volume = {8602},
year = {2014}
}
@article{Sen1996,
abstract = {The posterior probability density function (PPD), $\sigma$(m|dobs), of earth model m, where dobs are the measured data, describes the solution of a geophysical inverse problem, when a Bayesian inference model is used to describe the problem. In many applications, the PPD is neither analytically tractable nor easily approximated and simple analytic expressions for the mean and variance of the PPD are not available. Since the complete description of the PPD is impossible in the highly multi-dimensional model space of many geophysical applications, several measures such as the highest posterior density regions, marginal PPD and several orders of moments are often used to describe the solutions. Calculation of such quantities requires evaluation of multidimensional integrals. A faster alternative to enumeration and blind Monte-Carlo integration is importance sampling which may be useful in several applications. Thus how to draw samples of m from the PPD becomes an important aspect of geophysical inversion such that importance sampling can be used in the evaluation of these multi-dimensional integrals. Importance sampling can be carried out most efficiently by a Gibbs' sampler (GS). We also introduce a method which we called parallel Gibbs' sampler (PGS) based on genetic algorithms (GA) and show numerically that the results from the two samplers are nearly identical. We first investigate the performance of enumeration and several sampling based techniques such as a GS, PGS and several multiple maximum a posteriori (MAP) algorithms for a simple geophysical problem of inversion of resistivity sounding data. Several non-linear optimization methods based on simulated annealing (SA), GA and some of their variants can be devised which can be made to reach very close to the maximum of the PPD. Such MAP estimation algorithms also sample different points in the model space. By repeating these MAP inversions several times, it is possible to sample adequately the most significant portion(s) of the PPD and all these models can be used to construct the marginal PPD, mean) covariance, etc. We observe that the GS and PGS results are identical and indistinguishable from the enumeration scheme. Multiple MAP algorithms slightly underestimate the posterior variances although the correlation values obtained by all the methods agree very well. Multiple MAP estimation required 0.3{\%} of the computational effort of enumeration and 40{\%} of the effort of a GS or PGS for this problem. Next, we apply GS to the inversion of a marine seismic data set to quantify uncertainties in the derived model, given the prior distribution determined from several common midpoint gathers.},
author = {Sen, Mrinal K. and Stoffa, Paul L.},
doi = {10.1111/j.1365-2478.1996.tb00152.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Sen, Stoffa{\_}1996.pdf:pdf},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {uncertainty quantification},
mendeley-tags = {uncertainty quantification},
number = {2},
pages = {313--350},
title = {{Bayesian inference, Gibbs' sampler and uncertainty estimation in geophysical inversion}},
volume = {44},
year = {1996}
}
@article{Berry2014,
abstract = {TOUGH2 is an integral finite differences numerical simulator for non-isothermal multiphase flow in fractured porous media, which can manage complex spatial discretizations. Numerical simulation accuracy is affected, among other things, by grid resolution. Increasing the grid resolution requires computational and operating costs depending on the number of nodes and variables processed. The complexity of the management of the model increases when unstructured grids and local refinement are used. In order to improve the management and optimize the activities to update the model, an open source pre-processor has been developed using the open source codes GRASS GIS, SQLite and AMESH. Operations such as domain discretization, rock type assignment and mesh file generation have been automatized. Graphical interfaces allow for a user-friendly utilization. Operating errors and time required by pre-processing activities to generate and update locally refined unstructured grids have been reduced. Productivity in numerical modeling has been substantially increased.},
author = {Berry, P. and Bondu{\'{a}}, S. and Bortolotti, V. and Cormio, C. and Vasini, E.M.},
doi = {10.1016/j.envsoft.2014.08.011},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Environmental Modelling {\&} Software/Berry et al.{\_}2014.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling {\&} Software},
keywords = {GIS,Numerical modeling,Open source,Pre-processing,TOUGH2,Unstructured grid,Voronoi grid,tough2},
mendeley-tags = {tough2},
month = {dec},
pages = {52--64},
title = {{A GIS-based open source pre-processor for georesources numerical modeling}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815214002357},
volume = {62},
year = {2014}
}
@article{Sambridge2014,
abstract = {Non-linear inverse problems in the geosciences often involve probabilistic sampling of multimodal density functions or global optimization and sometimes both. Efficient algorithmic tools for carrying out sampling or optimization in challenging cases are of major interest. Here results are presented of some numerical experiments with a technique, known as Parallel Tempering, which originated in the field of computational statistics but is finding increasing numbers of applications in fields ranging from Chemical Physics to Astronomy. To date, experience in use of Parallel Tempering within earth sciences problems is very limited. In this paper, we describe Parallel Tempering and compare it to related methods of Simulated Annealing and Simulated Tempering for optimization and sampling, respectively. A key feature of Parallel Tempering is that it satisfies the detailed balance condition required for convergence of Markov chain Monte Carlo (McMC) algorithms while improving the efficiency of probabilistic sampling. Numerical results are presented on use of Parallel Tempering for trans-dimensional inversion of synthetic seismic receiver functions and also the simultaneous fitting of multiple receiver functions using global optimization. These suggest that its use can significantly accelerate sampling algorithms and improve exploration of parameter space in optimization. Parallel Tempering is a meta-algorithm which may be used together with many existing McMC sampling and direct search optimization techniques. It's generality and demonstrated performance suggests that there is significant},
author = {Sambridge, Malcolm},
doi = {10.1093/gji/ggt342},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Sambridge{\_}2014.pdf:pdf},
issn = {1365-246X},
journal = {Geophysical Journal International},
keywords = {Inverse theory,Numerical solutions,sa},
mendeley-tags = {sa},
month = {jan},
number = {1},
pages = {357--374},
title = {{A Parallel Tempering algorithm for probabilistic sampling and multimodal optimization}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggt342 http://academic.oup.com/gji/article/196/1/357/585739/A-Parallel-Tempering-algorithm-for-probabilistic},
volume = {196},
year = {2014}
}
@book{Menke2012,
abstract = {MATLAB edition; third edition. Since 1984, Geophysical Data Analysis has filled the need for a short, concise reference on inverse theory for individuals who have an intermediate background in science and mathematics. The new edition maintains the accessible and succinct manner for which it is known, with the addition of: MATLAB examples and problem setsAdvanced color graphicsCoverage of new topics, including Adjoint Methods; Inversion by Steepest Descent, Monte Carlo and Simulated Annealing methods; and Bootstrap algorithm for determining empirical confidence intervalsOnline data sets and MATLAB scripts that can be used as an inverse theory tutorial. Additional material on probability, including Bayesian influence, probability density function, and metropolis algorithmDetailed discussion of application of inverse theory to tectonic, gravitational and geomagnetic studiesNumerous examples and end-of-chapter homework problems help you explore and further understand the ideas presentedUse as classroom text facilitated by a complete set of exemplary lectures in Microsoft PowerPoint format and homework problem solutions for instructors.},
author = {Menke, William},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Menke{\_}2012.pdf:pdf},
isbn = {978-0-12-397160-9},
month = {jan},
pages = {293},
publisher = {Elsevier/Academic Press},
title = {{Geophysical data analysis: Discrete inverse theory}},
url = {http://www.sciencedirect.com/science/book/9780123971609},
year = {2012}
}
@book{Nielsen2015a,
author = {Nielsen, Michael A},
publisher = {Determination press USA},
title = {{Neural networks and deep learning}},
volume = {25},
year = {2015}
}
@article{Grandis1999,
author = {Grandis, H. and Menvielle, Michel and Roussignol, Michel},
doi = {10.1046/j.1365-246x.1999.00904.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Grandis, Menvielle, Roussignol{\_}1999.pdf:pdf},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {mcmc},
mendeley-tags = {mcmc},
month = {sep},
number = {3},
pages = {757--768},
title = {{Bayesian inversion with Markov chains-I. The magnetotelluricone-dimensional case}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246x.1999.00904.x},
volume = {138},
year = {1999}
}
@article{Um1987,
abstract = {A new approximate algorithm for two-point ray tracing is proposed and tested in a variety of laterally heterogeneous velocity models. An initial path estimate is perturbed using a geometric interpretation of the ray equations, and the travel time along the path is minimized in a piecewise fashion. This perturbation is iteratively performed until the travel time converges within a specified limit. Test results show that this algorithm successfully finds the correct travel time within typical observational error much faster than existing three-dimensional ray tracing programs. The method finds an accurate ray path in a fully three-dimensional form even where lateral variations in velocity are severe. Because our algorithm utilizes direct minimization of the travel time instead of solving the ray equations, a simple linear interpolation scheme can be employed to compute velocity as a function of position, providing an added computational advantage.},
author = {Um, Junho and Thurber, Clifford},
issn = {0037-1106},
journal = {Bulletin of the Seismological Society of America},
month = {jun},
number = {3},
pages = {972--986},
title = {{A fast algorithm for two-point seismic ray tracing}},
url = {http://dx.doi.org/},
volume = {77},
year = {1987}
}
@article{Koren1991,
abstract = {The complete solution to an inverse problem, including information on accuracy and resolution, is given by the a posteriori probability density in the model space. By running a modified simulated annealing, samples from the model space can be drawn in such a way that their frequencies of occurrence approximate their a posteriori likelihoods. Using this method, maximum likelihood estimation and uncertainty analysis of seismic background velocity models are performed on multioffset seismic data. The misfit between observed and synthetic waveforms within the time windows along computed multioffset travel times, is used as an objective function for the simulated annealing approach. The real medium is modeled as a series of layers separated by curved interfaces. Lateral velocity variations within the layers are determined by interpolation from specified values at a number of sampling points. The input data consists of multioffset seismic data. Additionally, zero-offset times are used to migrate the reflectors in time to the depth domain. The multioffset times are calculated by an efficient ray-tracing algorithm which allows inversion of a large number of seismograms. The a posteriori probability density for this problem is highly multidimensional and highly multimodal. Therefore, the information contained in this distribution cannot be adequately represented by standard deviations and covariances. However, by sequentially displaying a large number of images, computed from the a posteriori background velocity samples and the data, it is possible to convey to the spectator a better understanding of what information we really have on the subsurface.},
author = {Koren, Zvi and Mosegaard, Klaus and Landa, Evgeny and Thore, Pierre and Tarantola, Albert},
doi = {10.1029/91JB02278},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Koren et al.{\_}1991.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {doi:10.1029/91JB02278,http://dx.doi.org/10.1029/91JB02278},
month = {nov},
number = {B12},
pages = {20289--20299},
title = {{Monte Carlo estimation and resolution analysis of seismic background velocities}},
url = {http://doi.wiley.com/10.1029/91JB02278},
volume = {96},
year = {1991}
}
@article{Panagiotakis2008,
abstract = {In this paper, we propose a method for the automatic identification of P-phase arrival based on the distribution of local maxima (LM) in earthquake seismograms. The method efficiently combines energy and frequency characteristics of the LM distribution (LMD). The detection is mainly based on the energy of a seismic event in the case the earthquake has higher amplitude than seismic background noise. Otherwise, it is based on the frequency of LM. Thus, the method provides robust detection of P-phase arrival in any quality type of seismic data. Moreover, it uses two sequential sliding signal windows yielding very high accuracy on the P-phase estimation. A hierarchical P-phase detection algorithm dramatically reduces the computational cost, making possible a real-time implementation. Experimental results from a large database of more than 80 low, medium, and high signal-to-noise ratio seismic events and comparison with existing methods in the literature indicate the reliable performance of the proposed scheme.},
author = {Panagiotakis, Costas and Kokinou, Eleni and Vallianatos, Filippos},
doi = {10.1109/TGRS.2008.917272},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/IEEE Transactions on Geoscience and Remote Sensing/Panagiotakis, Kokinou, Vallianatos{\_}2008.pdf:pdf},
isbn = {0196-2892},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Automatic picking,P-phase arrival identification,Seismic-signal analysis,Signal segmentation,picking},
mendeley-tags = {picking},
month = {aug},
number = {8},
pages = {2280--2287},
title = {{Automatic P-Phase Picking Based on Local-Maxima Distribution}},
url = {http://ieeexplore.ieee.org/document/4554252/},
volume = {46},
year = {2008}
}
@inproceedings{Li2016,
abstract = {Fluid injection into reservoirs is a method generally employed in geothermal systems or in non-conventional hydrocarbon to enhance permeability. However, such method could lead to induced seismicity, which can be due to fracture propagation or fracture sliding. Our goal is to investigate the effect of fluid injection on the mechanical behavior of rocks at laboratory scale as well as the induced seismicity –using the monitoring of acoustic emissions-. This experimental work should give insight of i) how the reservoir property changes during fluid injection and ii) how induced seismicity and large magnitude events are related with the fluid flow. At laboratory scale, fluid pressure variation in porous rocks leads to change in strain and could eventually lead to inelastic strain (Stanchits et al. 2011, Schubnel et al. 2007). Inelastic deformation could be associated with a change in the permeability as well as induced acoustic emission events.},
author = {Li, Zhi and Luu, Keurfon and Nicolas, Aurelien and Fortin, Jerome and Gueguen, Yves},
booktitle = {4th International Workshop on Rock Physics},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/4th International Workshop on Rock Physics/Li et al.{\_}2016.pdf:pdf},
keywords = {microseismic},
mendeley-tags = {microseismic},
pages = {1--3},
title = {{Fluid-induced rupture on heat-treated andesite}},
year = {2016}
}
@article{Tronicke2012,
abstract = {Particle swarm optimization (PSO) is a relatively new global optimization approach inspired by the social behavior of bird flocking and fish schooling. Although this approach has proven to provide excellent convergence rates in different optimization problems, it has seldom been applied to inverse geophysical problems. Until today, published geophysical applications mainly focus on finding an optimum solution for simple, 1D inverse problems. We have applied PSO-based optimization strategies to reconstruct 2D P-wave velocity fields from crosshole traveltime data sets. Our inversion strategy also includes generating and analyzing a representative ensemble of acceptable models, which allows us to appraise uncertainty and nonuniqueness issues. The potential of our strategy was tested on field data collected at a well-constrained test site in Horstwalde, Germany. At this field site, the shallow subsurface mainly consists of sand- and gravel-dominated glaciofluvial sediments, which, as known from several boreholes and other geophysical experiments, exhibit some well-defined layering at the scale of our crosshole seismic data. Thus, we have implemented a flexible, layer-based model parameterization, which, compared with standard cell-based parameterizations, allows for significantly reducing the number of unknown model parameters and for efficiently implementing a priori model constraints. Comparing the 2D velocity fields resulting from our PSO strategy to independent borehole and direct-push data illustrated the benefits of choosing an efficient global optimization approach. These include a straightforward and understandable appraisal of nonuniqueness issues as well as the possibility of an improved and also more objective interpretation.},
author = {Tronicke, Jens and Paasche, Hendrik and B{\"{o}}niger, Urs},
doi = {10.1190/geo2010-0411.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Tronicke, Paasche, B{\"{o}}niger{\_}2012.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {pso},
mendeley-tags = {pso},
month = {jan},
number = {1},
pages = {R19--R32},
title = {{Crosshole traveltime tomography using particle swarm optimization: A near-surface field example}},
url = {http://library.seg.org/doi/abs/10.1190/geo2010-0411.1 http://library.seg.org/doi/10.1190/geo2010-0411.1},
volume = {77},
year = {2012}
}
@article{Auger2005,
abstract = {In this paper we introduce a restart-CMA- evolution strategy, where the population size is increased for each restart (IPOP). By increasing the population size the search characterisic becomes more global after each restart. The IPOP-CMA-ES is evaluated on the test suit of 25 functions designed for the special session on real-parameter optimization of CEC 2005. Its performance is compared to a local restart strategy with constant small population size. On unimodal functions the performance is similar. On multi-modal functions the local restart strategy significantly outperforms IPOP in 4 test cases whereas IPOP performs significantly better in 29 out of 60 tested cases.},
author = {Auger, A. and Hansen, N.},
doi = {10.1109/CEC.2005.1554902},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2005 IEEE Congress on Evolutionary Computation/Auger, Hansen{\_}2005.pdf:pdf},
isbn = {0-7803-9363-5},
issn = {1089-778X},
journal = {2005 IEEE Congress on Evolutionary Computation},
pages = {1769--1776},
title = {{A Restart CMA Evolution Strategy With Increasing Population Size}},
url = {http://ieeexplore.ieee.org/document/1554902/},
volume = {2},
year = {2005}
}
@article{Curtis2004,
abstract = {Most general experimental design algorithms are either: (i) stochastic and hence give different designs each time they are run with finite computing power, or (ii) deterministic but converge to results that depend on an initial or reference design, taking little or no account of the range of all other possible designs. In this paper we introduce an approximation to standard measures of experimental design quality that enables a new algorithm to be used. The algorithm is simple, deterministic and the resulting experimental design is influenced by the full range of possible designs, thus addressing problems (i) and (ii) above. Although the designs produced are not guaranteed to be globally optimal, they significantly increase the magnitude of small eigenvalues in the model- data relationship (without requiring that these eigenvalues be calculated). This reduces the model uncertainties expected post- experiment. We illustrate the method on simple tomographic and microseismic location examples with varying degrees of seismic attenuation.},
author = {Curtis, Andrew and Michelini, Alberto and Leslie, David and Lomax, Anthony},
doi = {10.1111/j.1365-246X.2004.02114.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Curtis et al.{\_}2004.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Inversion,Microseismicity,Tomography,location},
mendeley-tags = {location},
number = {2},
pages = {595--606},
title = {{A deterministic algorithm for experimental design applied to tomographic and microseismic monitoring surveys}},
volume = {157},
year = {2004}
}
@book{Koza1992,
author = {Koza, John R},
publisher = {MIT Press, Cambridge, MA},
title = {{Genetic Programming II, Automatic Discovery of Reusable Subprograms}},
year = {1992}
}
@article{Plessix2006,
abstract = {Estimating the model parameters from measured data generally consists of minimizing an error functional. A classic technique to solve a minimization problem is to successively determine the minimum of a series of linearized problems. This formulation requires the Frchet derivatives (the Jacobian matrix), which can be expensive to compute. If the minimization is viewed as a non-linear optimization problem, only the gradient of the error functional is needed. This gradient can be computed without the Frchet derivatives. In the 1970s, the adjoint-state method was developed to efficiently compute the gradient. It is now a well-known method in the numerical community for computing the gradient of a functional with respect to the model parameters when this functional depends on those model parameters through state variables, which are solutions of the forward problem. However, this method is less well understood in the geophysical community. The goal of this paper is to review the adjoint-state method. The idea is to define some adjoint-state variables that are solutions of a linear system. The adjoint-state variables are independent of the model parameter perturbations and in a way gather the perturbations with respect to the state variables. The adjoint-state method is efficient because only one extra linear system needs to be solved. Several applications are presented. When applied to the computation of the derivatives of the ray trajectories, the link with the propagator of the perturbed ray equation is established.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Plessix, R.-E.},
doi = {10.1111/j.1365-246X.2006.02978.x},
eprint = {0402594v3},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Plessix{\_}2006.pdf:pdf},
isbn = {0956540x},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Adjoint state,Gradient,Migration,Tomography,adjoint state},
mendeley-tags = {adjoint state},
month = {nov},
number = {2},
pages = {495--503},
pmid = {10653792},
primaryClass = {arXiv:cond-mat},
title = {{A review of the adjoint-state method for computing the gradient of a functional with geophysical applications}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.2006.02978.x},
volume = {167},
year = {2006}
}
@incollection{Hart2003,
author = {Hart, Douglas I.},
doi = {10.1007/978-94-017-0271-3_2},
pages = {13--30},
title = {{Automated Picking of Seismic First-Arrivals with Neural Networks}},
url = {http://link.springer.com/10.1007/978-94-017-0271-3{\_}2},
year = {2003}
}
@article{Rinaldi2014,
abstract = {The importance of geomechanics-including the potential for faults to reactivate during large-scale geologic carbon sequestration operations-has recently become more widely recognized. However, notwithstanding the potential for triggering notable (felt) seismic events, the potential for buoyancy-driven CO2 to reach potable groundwater and the ground surface is actually more important from public safety and storage-efficiency perspectives. In this context, this work extends the previous studies on the geomechanical modeling of fault responses during underground carbon dioxide injection, focusing on the short-term integrity of the sealing caprock, and hence on the potential for leakage of either brine or CO2 to reach the shallow groundwater aquifers during active injection. We consider stress/strain-dependent permeability and study the leakage through the fault zone as its permeability changes during a reactivation, also causing seismicity. We analyze several scenarios related to the volume of CO2 injected (and hence as a function of the overpressure), involving both minor and major faults, and analyze the profile risks of leakage for different stress/strain-permeability coupling functions. We conclude that whereas it is very difficult to predict how much fault permeability could change upon reactivation, this process can have a significant impact on the leakage rate. Moreover, our analysis shows that induced seismicity associated with fault reactivation may not necessarily open up a new flow path for leakage. Results show a poor correlation between magnitude and amount of fluid leakage, meaning that a single event is generally not enough to substantially change the permeability along the entire fault length. Consequently, even if some changes in permeability occur, this does not mean that the CO2 will migrate up along the entire fault, breaking through the caprock to enter the overlying aquifer.},
author = {Rinaldi, Antonio P. and Rutqvist, Jonny and Cappa, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1016/j.ijggc.2013.11.001},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Rinaldi, Rutqvist, Cappa{\_}2014.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Carbon sequestration,Geomechanics,Induced seismicity,Leakage,Stress dependent permeability,flac,tough2},
mendeley-tags = {flac,tough2},
month = {jan},
pages = {117--131},
publisher = {Elsevier Ltd},
title = {{Geomechanical effects on CO2 leakage through fault zones during large-scale underground injection}},
url = {http://dx.doi.org/10.1016/j.ijggc.2013.11.001 https://linkinghub.elsevier.com/retrieve/pii/S1750583613003848},
volume = {20},
year = {2014}
}
@article{Luu2018a,
abstract = {Seismic traveltime tomography is an optimization problem that requires large computational efforts. Therefore, linearized techniques are commonly used for their low computational cost. These local optimization methods are likely to get trapped in a local minimum as they critically depend on the initial model. On the other hand, global optimization methods based on MCMC are insensitive to the initial model but turn out to be computationally expensive. Particle Swarm Optimization (PSO) is a rather new global optimization approach with few tuning parameters that has shown excellent convergence rates and is straightforwardly parallelizable, allowing a good distribution of the workload. However, while it can traverse several local minima of the evaluated misfit function, classical implementation of PSO can get trapped in local minima at later iterations as particles inertia dim. We propose a Competitive PSO (CPSO) to help particles to escape from local minima with a simple implementation that improves swarm's diversity. The model space can be sampled by running the optimizer multiple times and by keeping all the models explored by the swarms in the different runs. A traveltime tomography algorithm based on CPSO is successfully applied on a real 3D data set in the context of induced seismicity.},
author = {Luu, Keurfon and Noble, Mark and Gesret, Alexandrine and Belayouni, Nidhal and Roux, Pierre-Fran{\c{c}}ois},
doi = {10.1016/j.cageo.2018.01.016},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Luu et al.{\_}2018.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {high performance computing,hpc,microseismic,particle swarm optimization,pso,traveltime tomography,uncertainty quantification},
mendeley-tags = {hpc,pso},
month = {apr},
number = {August 2017},
pages = {81--93},
publisher = {Elsevier Ltd},
title = {{A parallel competitive Particle Swarm Optimization for non-linear first arrival traveltime tomography and uncertainty quantification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0098300417312335},
volume = {113},
year = {2018}
}
@article{Poormirzaee2015,
author = {Poormirzaee, Rashed and Moghadam, Rasoul Hamidzadeh and Zarean, Ahmad},
doi = {10.1007/s12517-014-1662-x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Arabian Journal of Geosciences/Poormirzaee, Moghadam, Zarean{\_}2015.pdf:pdf},
issn = {1866-7511},
journal = {Arabian Journal of Geosciences},
keywords = {Inversion,P-wave velocity,PSO,Seismic refraction data,pso},
mendeley-tags = {pso},
month = {aug},
number = {8},
pages = {5981--5989},
title = {{Inversion seismic refraction data using particle swarm optimization: a case study of Tabriz, Iran}},
url = {http://link.springer.com/10.1007/s12517-014-1662-x},
volume = {8},
year = {2015}
}
@inproceedings{Fogel1999,
author = {Fogel, David B},
booktitle = {Evolutionary Algorithms},
pages = {89--109},
publisher = {Springer},
title = {{An overview of evolutionary programming}},
year = {1999}
}
@misc{Will2013,
author = {Will, Robert},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Will{\_}2013.pdf:pdf},
keywords = {decatur},
mendeley-tags = {decatur},
number = {October},
title = {{Geomechanical and Flow Modeling at the Illinois Basin – Decatur Project}},
year = {2013}
}
@inproceedings{Engelbrecht2012,
abstract = {Since its birth in 1995, particle swarm optimization (PSO) has been well studied and successfully applied. While a better understanding of PSO and particle behaviors have been obtained through theoretical and empirical analysis, some issues about the beavior of particles remain unanswered. One such issue is how velocities should be initialized. Though zero initial velocities have been advocated, a popular initialization strategy is to set initial weights to random values within the domain of the optimization problem. This article first illustrates that particles tend to leave the boundaries of the search space irrespective of the initialization approach, resulting in wasted search effort. It is also shown that random initialization increases the number of roaming particles, and that this has a negative impact on convergence time. It is also shown that enforcing a boundary constraint on personal best positions does not help much to address this problem. The main objective of the article is to show that the best approach is to initialize particles to zero, or random values close to zero, without imposing a personal best bound.},
author = {Engelbrecht, Andries},
booktitle = {2012 IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2012.6256112},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/2012 IEEE Congress on Evolutionary Computation/Engelbrecht{\_}2012.pdf:pdf},
isbn = {978-1-4673-1509-8},
keywords = {pso},
mendeley-tags = {pso},
month = {jun},
pages = {1--8},
publisher = {IEEE},
title = {{Particle swarm optimization: Velocity initialization}},
url = {http://ieeexplore.ieee.org/document/6256112/},
year = {2012}
}
@article{Hansen2001,
abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
author = {Hansen, Nikolaus and Ostermeier, Andreas},
doi = {10.1162/106365601750190398},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Evolutionary Computation/Hansen, Ostermeier{\_}2001.pdf:pdf},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {cmaes,covariance matrix adaptation,cumulation,cumulative path length control,de-,derandomized self-adaptation,evolu-,evolution strategy,randomization,self-adaptation,step size control,strategy parameter control,tion path},
mendeley-tags = {cmaes},
number = {2},
pages = {159--195},
pmid = {11382355},
title = {{Completely Derandomized Self-Adaptation in Evolution Strategies}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/106365601750190398},
volume = {9},
year = {2001}
}
@article{Tibshirani2001,
abstract = {We propose a method (the ‘gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
doi = {10.1111/1467-9868.00293},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Tibshirani, Walther, Hastie{\_}2001.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {cluster analysis},
mendeley-tags = {cluster analysis},
month = {may},
number = {2},
pages = {411--423},
pmid = {306526},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract http://doi.wiley.com/10.1111/1467-9868.00293},
volume = {63},
year = {2001}
}
@article{Poormirzaee2016,
abstract = {The refraction microtremor method has been increasingly used as an appealing tool for investigating near surface S-wave structure. However, inversion, as a main stage in processing refraction microtremor data, is challenging for most local search methods due to its high nonlinearity. With the development of data optimization approaches, fast and easier techniques can be employed for processing geophysical data. Recently, particle swarm optimization algorithm has been used in many fields of studies. Use of particle swarm optimization in geophysical inverse problems is a relatively recent development which offers many advantages in dealing with the nonlinearity inherent in such applications. In this study, the reliability and efficiency of particle swarm optimization algorithm in the inversion of refraction microtremor data were investigated. A new framework was also proposed for the inversion of refraction microtremor Rayleigh wave dispersion curves. First, particle swarm optimization code in MATLAB was developed; then, in order to evaluate the efficiency and stability of proposed algorithm, two noise-free and two noise-corrupted synthetic datasets were inverted. Finally, particle swarm optimization inversion algorithm in refraction microtremor data was applied for geotechnical assessment in a case study in the area in city of Tabriz in northwest of Iran. The S-wave structure in the study area successfully delineated. Then, for evaluation, the estimated Vs profile was compared with downhole data available around of the considered area. It could be concluded that particle swarm optimization inversion algorithm is a suitable technique for inverting microtremor waves.},
author = {Poormirzaee, Rashed},
doi = {10.1007/s12517-016-2701-6},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Arabian Journal of Geosciences/Poormirzaee{\_}2016.pdf:pdf},
issn = {1866-7511},
journal = {Arabian Journal of Geosciences},
keywords = {Inversion,Particle swarm optimization,Refraction microtremor,Shear wave,pso},
mendeley-tags = {pso},
month = {oct},
number = {16},
pages = {673},
publisher = {Arabian Journal of Geosciences},
title = {{S-wave velocity profiling from refraction microtremor Rayleigh wave dispersion curves via PSO inversion algorithm}},
url = {http://dx.doi.org/10.1007/s12517-016-2701-6 http://link.springer.com/10.1007/s12517-016-2701-6},
volume = {9},
year = {2016}
}
@article{FernandezMartinez2012a,
abstract = {In this paper, we show how to design a powerful set of particle swarm optimizers to be applied in inverse modelling. The design is based on the interpretation of the swarm dynamics as a stochastic damped mass-spring system, the so-called particle swarm optimization (PSO) continuous model. Based on this idea we derived a family of PSO optimizers (GPSO, CC-PSO and CP-PSO) having different exploitation and exploration capabilities. Their convergence is related to the stability of their first (mean trajectories)- and second-order moments (variance and temporal covariance). Good parameter sets are located inside their first stability regions close to the upper border of their respective second stability regions where the attraction from the particles oscillation centre is very weak. In this region of weak attraction, both convergence to the global minimum and exploration of the search space are possible. Based on this idea, we have designed a particle–cloud algorithm where each particle in the swarm has different inertia (damping) and acceleration (rigidity) constants. We explored the performance of these algorithms for different PSO members using different benchmark functions, showing that the cloud algorithms have a very good balance between exploration and exploitation. Also, the cloud design helps to avoid two main drawbacks of the PSO algorithm: the tuning of the PSO parameters and the clamping of the particles velocities. We also present the lime and sand algorithm that changes the time step with iterations. This feature helps to avoid entrapment in local minima when the time step is increased, and enables exploration around the global best when the time step is decreased. All these designs are based on the theoretical analysis of the PSO dynamics. We explain how to use this knowledge to the solution and appraisal of inverse problems. Finally, we briefly introduce the combined use of PSO and model reduction techniques to allow posterior sampling in high dimensional spaces.},
author = {{Fern{\'{a}}ndez Mart{\'{i}}nez}, Juan Luis and {Garc{\'{i}}a Gonzalo}, Esperanza and {Fern{\'{a}}ndez Mu{\~{n}}iz}, Zulima and Mukerji, Tapan},
doi = {10.1177/0142331211402900},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Transactions of the Institute of Measurement and Control/Fern{\'{a}}ndez Mart{\'{i}}nez et al.{\_}2012.pdf:pdf},
issn = {0142-3312},
journal = {Transactions of the Institute of Measurement and Control},
keywords = {cloud particle swarm,exploitation,global optimization,pso,sampling},
mendeley-tags = {pso},
number = {6},
pages = {705--719},
title = {{How to design a powerful family of particle swarm optimizers for inverse modelling}},
url = {http://journals.sagepub.com/doi/10.1177/0142331211402900},
volume = {34},
year = {2012}
}
@inproceedings{Schulze-Riegert2001,
author = {Schulze-Riegert, R.W. and Axmann, J.K. and Haase, O. and Rian, D.T. and You, Y.-L.},
booktitle = {SPE Reservoir Simulation Symposium},
doi = {10.2118/66393-MS},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Optimization Methods for History Matching of Complex Reservoirs}},
url = {http://www.onepetro.org/doi/10.2118/66393-MS},
year = {2001}
}
@article{Loshchilov2015,
abstract = {The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is often considered to be the method of choice for continuous optimization when first- and/or second- order information is available. However, the use of L-BFGS can be complicated in a black-box scenario where gradient information is not available and therefore should be numerically estimated. The accuracy of this estimation, obtained by finite difference methods, is often problem-dependent that may lead to premature convergence of the algorithm. In this paper, we demonstrate an alternative to L-BFGS, the limited memory Covariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov (2014). The LM-CMA is a stochastic derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems. Inspired by the L-BFGS, the LM-CMA samples candidate solutions according to a covariance matrix reproduced from {\$}m{\$} direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the memory complexity to {\$}O(mn){\$}, where {\$}n{\$} is the number of decision variables. The time complexity of sampling one candidate solution is also {\$}O(mn){\$}, but scales as only about 25 scalar-vector multiplications in practice. The algorithm has an important property of invariance w.r.t. strictly increasing transformations of the objective function, such transformations do not compromise its ability to approach the optimum. The LM-CMA outperforms the original CMA-ES and its large scale versions on non-separable ill-conditioned problems with a factor increasing with problem dimension. Invariance properties of the algorithm do not prevent it from demonstrating a comparable performance to L-BFGS on non-trivial large scale smooth and nonsmooth optimization problems.},
archivePrefix = {arXiv},
arxivId = {1511.00221},
author = {Loshchilov, Ilya},
eprint = {1511.00221},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Loshchilov{\_}2015.pdf:pdf},
keywords = {cmaes},
mendeley-tags = {cmaes},
month = {nov},
title = {{LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization}},
url = {http://arxiv.org/abs/1511.00221},
year = {2015}
}
@article{Shao1993,
abstract = {We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the Cp, and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n {\^{a}}†' {\^{a}}ˆ{\v{z}}. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n{\^{I}}½-out cross-validation with n{\^{I}}½, the number of observations reserved for validation, satisfying n{\^{I}}½/n {\^{a}}†' 1 as n {\^{a}}†' {\^{a}}ˆ{\v{z}}. This is a somewhat shocking discovery, because n{\^{I}}½/n {\^{a}}†' 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n{\^{I}}½-out cross-validation method are provided, and results from a simulation study are presented.},
author = {Shao, Jun},
doi = {10.2307/2290328},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of the American Statistical Association/Shao{\_}1993.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Balanced incomplete,Consistency,Data splitting,Model assessment,Monte Carlo,Prediction,machine learning},
mendeley-tags = {machine learning},
month = {jun},
number = {422},
pages = {486},
title = {{Linear Model Selection by Cross-Validation}},
url = {http://www.jstor.org/stable/2290328 http://about.jstor.org/terms http://www.jstor.org/stable/2290328?origin=crossref},
volume = {88},
year = {1993}
}
@article{Rutqvist2010,
abstract = {In Salah Gas Project in Algeria has been injecting 0.5-1 million tonnes CO2 per year over the past 5 years into a water-filled strata at a depth of about 1800-1900 m. Unlike most CO2 storage sites, the permeability of the storage formation is relatively low and comparatively thin with a thickness of about 20 m. To ensure adequate CO2 flow-rates across the low-permeability sand-face, the In Salah Gas Project decided to use long-reach (about 1-1.5 km) horizontal injection wells. In an ongoing research project we use field data and coupled reservoir-geomechanical numerical modeling to assess the effectiveness of this approach and to investigate monitoring techniques to evaluate the performance of a CO2 injection operation in relatively low-permeability formations. Among the field data used are ground surface deformations evaluated from recently acquired satellite-based inferrometry (InSAR). The InSAR data shows a surface uplift on the order of 5 mm per year above active CO2 injection wells and the uplift pattern extends several km from the injection wells. In this paper we use the observed surface uplift to constrain our coupled reservoir-geomechanical model and conduct sensitivity studies to investigate potential causes and mechanisms of the observed uplift. The results of our analysis indicate that most of the observed uplift magnitude can be explained by pressure-induced, poro-elastic expansion of the 20-m-thick injection zone, but there could also be a significant contribution from pressure-induced deformations within a 100-m-thick zone of shaly sands immediately above the injection zone.},
author = {Rutqvist, Jonny and Vasco, Donald W. and Myer, Larry},
doi = {10.1016/j.ijggc.2009.10.017},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Rutqvist, Vasco, Myer{\_}2010.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {Geological CO2 sequestration,Geomechanics,Ground surface deformations,In Salah,InSAR,Modeling,flac,tough2},
mendeley-tags = {flac,tough2},
month = {mar},
number = {2},
pages = {225--230},
publisher = {Elsevier},
title = {{Coupled reservoir-geomechanical analysis of CO2 injection and ground deformations at In Salah, Algeria}},
url = {http://dx.doi.org/10.1016/j.egypro.2009.01.241 https://linkinghub.elsevier.com/retrieve/pii/S1750583609001388},
volume = {4},
year = {2010}
}
@article{Trelea2003,
abstract = {The particle swarm optimization algorithm is analyzed using standard results from the dynamic system theory. Graphical parameter selection guidelines are derived. The exploration-exploitation tradeoff is discussed and illustrated. Examples of performance on benchmark functions superior to previously published results are given. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Trelea, Ioan Cristian},
doi = {10.1016/S0020-0190(02)00447-7},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Information Processing Letters/Trelea{\_}2003.pdf:pdf},
isbn = {0020-0190},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {Analysis of algorithms,Parallel algorithms,Particle swarm optimization,Stochastic optimization,pso},
mendeley-tags = {pso},
number = {6},
pages = {317--325},
title = {{The particle swarm optimization algorithm: Convergence analysis and parameter selection}},
volume = {85},
year = {2003}
}
@article{Ekinci2016,
abstract = {An efficient approach to estimate model parameters from residual gravity data based on differential evolution (DE), a stochastic vector-based metaheuristic algorithm, has been presented. We have showed the applicability and effectiveness of this algorithm on both synthetic and field anomalies. According to our knowledge, this is a first attempt of applying DE for the parameter estimations of residual gravity anomalies due to isolated causative sources embedded in the subsurface. The model parameters dealt with here are the amplitude coefficient (A), the depth and exact origin of causative source (zo and xo, respectively) and the shape factors (q and ƞ). The error energy maps generated for some parameter pairs have successfully revealed the nature of the parameter estimation problem under consideration. Noise-free and noisy synthetic single gravity anomalies have been evaluated with success via DE/best/1/bin, which is a widely used strategy in DE. Additionally some complicated gravity anomalies caused by multiple source bodies have been considered, and the results obtained have showed the efficiency of the algorithm. Then using the strategy applied in synthetic examples some field anomalies observed for various mineral explorations such as a chromite deposit (Camaguey district, Cuba), a manganese deposit (Nagpur, India) and a base metal sulphide deposit (Quebec, Canada) have been considered to estimate the model parameters of the ore bodies. Applications have exhibited that the obtained results such as the depths and shapes of the ore bodies are quite consistent with those published in the literature. Uncertainty in the solutions obtained from DE algorithm has been also investigated by Metropolis-Hastings (M-H) sampling algorithm based on simulated annealing without cooling schedule. Based on the resulting histogram reconstructions of both synthetic and field data examples the algorithm has provided reliable parameter estimations being within the sampling limits of M-H sampler. Although it is not a common inversion technique in geophysics, it can be stated that DE algorithm is worth to get more interest for parameter estimations from potential field data in geophysics considering its good accuracy, less computational cost (in the present problem) and the fact that a well-constructed initial guess is not required to reach the global minimum.},
author = {Ekinci, Yunus Levent and Balkaya, {\c{C}}ağlayan and G{\"{o}}kt{\"{u}}rkler, G{\"{o}}khan and Turan, Se{\c{c}}il},
doi = {10.1016/j.jappgeo.2016.03.040},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Applied Geophysics/Ekinci et al.{\_}2016.pdf:pdf},
issn = {09269851},
journal = {Journal of Applied Geophysics},
keywords = {Differential Evolution,Gravity anomalies,Metaheuristics,Metropolis-Hastings sampling,Parameter estimations,Simple-shaped gravity sources,de},
mendeley-tags = {de},
month = {jun},
pages = {133--147},
title = {{Model parameter estimations from residual gravity anomalies due to simple-shaped sources using Differential Evolution Algorithm}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0926985116300891},
volume = {129},
year = {2016}
}
@article{Torczon1997,
author = {Torczon, Virginia},
doi = {10.1137/S1052623493250780},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
month = {feb},
number = {1},
pages = {1--25},
title = {{On the Convergence of Pattern Search Algorithms}},
url = {http://epubs.siam.org/doi/10.1137/S1052623493250780},
volume = {7},
year = {1997}
}
@article{Lan2014,
abstract = {In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create wormholes connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a residual energy function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.},
archivePrefix = {arXiv},
arxivId = {1306.0063},
author = {Lan, Shiwei and Streets, Jeffrey and Shahbaba, Babak},
eprint = {1306.0063},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/AAAI/Lan, Streets, Shahbaba{\_}2014.pdf:pdf},
isbn = {9781577356790},
journal = {AAAI},
keywords = {machine learning,mcmc},
mendeley-tags = {machine learning,mcmc},
month = {may},
pages = {1--18},
title = {{Wormhole Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1306.0063},
year = {2014}
}
@article{Saka2016,
abstract = {Metaheuristic algorithms have provided efficient tools to engineering designers by which it became possible to determine the optimum solutions of engineering design optimization problems encountered in every day practice. Generally metaheuristics are based on metaphors that are taken from nature or some other processes. Because of their success of providing solutions to complex engineering design optimization problems the recent literature has flourished with a large number of new metaheuristics based on a variety of metaphors. Despite the fact that most of these techniques have numerically proven themselves as reliable and strong tools for solutions of design optimization problems in many different disciplines, some argue against these methods on account of not having mathematical background and making use of irrelevant and odd metaphors. However, so long as these efforts bring about computationally efficient and robust optimum structural tools for designers what type of metaphors they are based on becomes insignificant. After a brief historical review of structural optimization this article opens this issue up for discussion of the readers and attempts to answer some of the criticisms asserted in some recent publications related with the novelty of metaheuristics.},
author = {Saka, M.P. and Hasan{\c{c}}ebi, O. and Geem, Z.W.},
doi = {10.1016/j.swevo.2016.01.005},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Swarm and Evolutionary Computation/Saka, Hasan{\c{c}}ebi, Geem{\_}2016.pdf:pdf},
isbn = {2210-6502},
issn = {22106502},
journal = {Swarm and Evolutionary Computation},
keywords = {Evolution strategies,Harmony search,Metaheuristics,Phenomenon mimicking algorithm,Structural optimization},
month = {jun},
pages = {88--97},
publisher = {Elsevier},
title = {{Metaheuristics in structural optimization and discussions on harmony search algorithm}},
url = {http://dx.doi.org/10.1016/j.swevo.2016.01.005 http://linkinghub.elsevier.com/retrieve/pii/S2210650216000158},
volume = {28},
year = {2016}
}
@article{Chu2011,
abstract = {Despite the fact that the popular particle swarm optimizer (PSO) is currently being extensively applied to many real-world problems that often have high-dimensional and complex fitness landscapes, the effects of boundary constraints on PSO have not attracted adequate attention in the literature. However, in accordance with the theoretical analysis in [11], our numerical experiments show that particles tend to fly outside of the boundary in the first few iterations at a very high probability in high-dimensional search spaces. Consequently, the method used to handle boundary violations is critical to the performance of PSO. In this study, we reveal that the widely used random and absorbing bound-handling schemes may paralyze PSO for high-dimensional and complex problems. We also explore in detail the distinct mechanisms responsible for the failures of these two bound-handling schemes. Finally, we suggest that using high-dimensional and complex benchmark functions, such as the composition functions in [19], is a prerequisite to identifying the potential problems in applying PSO to many real-world applications because certain properties of standard benchmark functions make problems inexplicit. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
author = {Chu, Wei and Gao, Xiaogang and Sorooshian, Soroosh},
doi = {10.1016/j.ins.2010.11.030},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Information Sciences/Chu, Gao, Sorooshian{\_}2011.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Bound-handling strategy,Complex benchmark functions,High-dimensional optimization,Particle swarm optimization,Real-world applications,pso},
mendeley-tags = {pso},
month = {oct},
number = {20},
pages = {4569--4581},
publisher = {Elsevier Inc.},
title = {{Handling boundary constraints for particle swarm optimization in high-dimensional search space}},
url = {http://dx.doi.org/10.1016/j.ins.2010.11.030 http://linkinghub.elsevier.com/retrieve/pii/S0020025510005839},
volume = {181},
year = {2011}
}
@article{Prieux2013,
abstract = {Building an accurate initial velocity model for full waveform inversion$\backslash$n(FWI) is a key issue to guarantee convergence of full waveform inversion$\backslash$ntowards the global minimum of a misfit function. In this study, we$\backslash$nassess joint refraction and reflection stereotomography as a tool to$\backslash$nbuild a reliable starting model for frequency-domain full waveform$\backslash$ninversion from long-offset (i.e., wide-aperture) data. Stereotomography$\backslash$nis a slope tomographic method that is based on the inversion of$\backslash$ntraveltimes and slopes of locally-coherent events in a data cube. One$\backslash$nadvantage of stereotomography compared to conventional traveltime$\backslash$nreflection tomography is the semi-automatic picking procedure of$\backslash$nlocally-coherent events, which is easier than the picking of continuous$\backslash$nevents, and can lead to a higher density of picks. While conventional$\backslash$napplications of stereotomography only consider short-offset reflected$\backslash$nwaves, we assess the benefits provided by the joint inversion of$\backslash$nreflected and refracted arrivals. Introduction of the refracted waves$\backslash$nallows the construction of a starting model that kinematically fits the$\backslash$nfirst arrivals, a necessary requirement for full waveform inversion. In$\backslash$na similar way to frequency-domain full waveform inversion, we design a$\backslash$nmultiscale approach of stereotomography, which proceeds hierarchically$\backslash$nfrom the wide-aperture to the short-aperture components of the data, to$\backslash$nreduce the non-linearity of the stereotomographic inversion of$\backslash$nlong-offset data. This workflow which combines stereotomography and full$\backslash$nwaveform inversion, is applied to synthetic and real data case studies$\backslash$nfor the Valhall oil-field target. The synthetic results show that the$\backslash$njoint refraction and reflection stereotomography for a 24-km maximum$\backslash$noffset data set provides a more reliable initial model for full waveform$\backslash$ninversion than reflection stereotomography performed for a 4-km maximum$\backslash$noffset data set, in particular in low-velocity gas layers and in the$\backslash$ndeep part of a structure below a reservoir. Application of joint$\backslash$nstereotomography, full waveform inversion and reverse-time migration to$\backslash$nreal data reveals that the FWI models and the reverse-time migration$\backslash$nimages computed from the stereotomography model shares several features$\backslash$nwith FWI velocity models and migrated images computed from an$\backslash$nanisotropic reflection-traveltime tomography model, although$\backslash$nstereotomography was performed in the isotropic approximation.$\backslash$nImplementation of anisotropy in joint refraction and reflection$\backslash$nstereotomography of long-offset data is a key issue to further improve$\backslash$nthe accuracy of the method.},
author = {Prieux, Vincent and Lambar{\'{e}}, Gilles and Operto, St{\'{e}}phane and Virieux, Jean},
doi = {10.1111/j.1365-2478.2012.01099.x},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Prospecting/Prieux et al.{\_}2013.pdf:pdf},
isbn = {0016-8025},
issn = {00168025},
journal = {Geophysical Prospecting},
keywords = {Full waveform inversion,Stereotomograpgy},
month = {jun},
number = {SUPPL.1},
pages = {109--137},
title = {{Building starting models for full waveform inversion from wide-aperture data by stereotomography}},
url = {http://doi.wiley.com/10.1111/j.1365-2478.2012.01099.x},
volume = {61},
year = {2013}
}
@article{Zhu1994,
abstract = {A master station (MS) method is presented in this paper to rapidly determine hypocenters in three-dimensional (3-D) heterogeneous velocities. An equal differential time (EDT) surface is defined as the collection of all spatial points that satisfy the time difference between two arrivals, which can be two picks at two stations or two different phase picks at one station. The EDT surface is independent of the origin time and will contain the hypocenter. For an event with J arrivals, there are (J-1) independent EDT surfaces. The MS method determines the hypocenter that satisfies two types of constraints: to be traversed by most EDT surfaces and to yield minimum travel time residual statistics. The statistics include both the residual variance and the amplitude of the origin time error. The combined use of the EDT surfaces and residual statistics allows for a unique determination of the hypocenter and origin time using different types of phase arrivals. In principle, only three arrivals are minimally required to constrain a unique solution if three different stations are used. For a 3-D velocity model, the EDT surfaces and the residual statistics can be computed efficiently using a reference file created by Moser's (1991) ray tracing method. An illustration of the MS method is given for 27 small events that occurred in southern California, using a P wave velocity model modified from that of Magistrale et al. (1992). The average misfit between the bulletin hypocenters and the new solutions is 3.8 km. If a 3-D velocity model is accurate, the MS method can be a viable means of hypocenter determination.},
author = {Zhu, Hua-Wei},
doi = {10.1029/94JB00934},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research/Zhu{\_}1994.pdf:pdf},
journal = {Journal of Geophysical Research},
keywords = {location},
mendeley-tags = {location},
pages = {439--455},
title = {{Rapid three-dimensional hypocentral determination using a master station method}},
volume = {99},
year = {1994}
}
@article{Hewlett1966,
abstract = {EXPERIENCE OF ARTIFICIAL FERTILIZATION, such as is effected with ornamental plants in order to obtain new variations in color, has led to the experiments which will here be discussed. The striking regularity with which the same hybrid forms always reappeared whenever fertilization took place between the same species induced further experiments to be undertaken, the object of which was to follow up the developments of the hybrids in their progeny. To this object numerous careful observers, such as K{\"{o}}lreuter, G{\"{a}}rtner, Herbert, Lecoq, Wichura and others, have devoted a part of their lives with inexhaustible perseverance. G{\"{a}}rtner especially in his work Die Bastarderzeugung im Pflanzenreiche [The Production of Hybrids in the Vegetable Kingdom], has recorded very valuable observations; and quite recently Wichura published the results of some profound investigations into the hybrids of the Willow. That, so far, no generally applicable law governing the formation and development of hybrids has been successfully formulated can hardly be wondered at by anyone who is acquainted with the extent of the task, and can appreciate the difficulties with which experiments of this class have to contend. A final decision can only be arrived at when we shall have before us the results of detailed experiments made on plants belonging to the most diverse orders.Those who survey the work done in this department will arrive at the conviction that among all the numerous experiments made, not one has been carried out to such an extent and in such a way as to make it possible to determine the number of different forms under which the offspring of the hybrids appear, or to arrange these forms with certainty according to their separate generations, or definitely to ascertain their statistical relations. It requires indeed some courage to undertake a labor of such far– reaching extent; this appears, however, to be the only right way by which we can finally reach the solution of a question the importance of which cannot be overestimated in connection with the history of the evolution of organic forms. The paper now presented records the results of such a detailed experiment. This experiment was practically confined to a small plant group, and is now, after eight years' pursuit, concluded in all essentials. Whether the plan upon which the separate experiments were conducted and carried out was the best suited to attain the desired end is left to the friendly decision of the reader.},
author = {Hewlett, P. S. and Mendel, Gregor},
doi = {10.2307/2528198},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Biometrics/Hewlett, Mendel{\_}1966.pdf:pdf},
isbn = {0674278003},
issn = {0006341X},
journal = {Biometrics},
month = {sep},
number = {3},
pages = {636},
pmid = {1000044616},
title = {{Experiments in Plant Hybridisation}},
url = {http://www.jstor.org/stable/2528198?origin=crossref https://www.jstor.org/stable/2528198?origin=crossref},
volume = {22},
year = {1966}
}
@article{Langheinrich2018,
abstract = {This paper aims to evaluate the applicability of automatically generated meshes produced within the gmsh meshing framework in preparation for high resolution wind speed and atmospheric pressure simulations for detailed urban environments. The process of creating a skeleton geometry for the meshing process based on level of detail (LOD) 2 CityGML data is described. Gmsh itself offers several approaches for the 2D and 3D meshing process respectively. The different algorithms are shortly introduced and preliminary rated in regard to the mesh quality that is to be expected, as they differ inversely in terms of robustness and resulting mesh element quality. A test area was chosen to simulate the turbulent flow of wind around an urban environment to evaluate the different mesh incarnations by means of a relative comparison of the residuals resulting from the finite-element computational fluid dynamics (CFD) calculations. The applied mesh cases are assessed regarding their convergence evolution as well as final residual values, showing that gmsh 2D and 3D algorithm combinations utilizing the Frontal meshing approach are the preferable choice for the kind of underlying geometry as used in the on hand experiments.},
author = {Langheinrich, M.},
doi = {10.5194/isprs-archives-XLII-2-559-2018},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences/Langheinrich{\_}2018.pdf:pdf},
issn = {2194-9034},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {3D,CFD,Computational fluid dynamics,Evaluation,Meshing,Spatial data,Wind speed,mesh},
mendeley-tags = {mesh},
month = {may},
number = {2},
pages = {559--564},
title = {{Evaluation of Gmsh meshing algorithms in preparation of high-resolution wind speed simulations in urban areas}},
url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/559/2018/},
volume = {XLII-2},
year = {2018}
}
@article{Goldstein1987,
author = {Goldstein, Peter and Archuleta, Ralph J},
issn = {1944-8007},
journal = {Geophysical Research Letters},
number = {1},
pages = {13--16},
publisher = {Wiley Online Library},
title = {{Array analysis of seismic signals}},
volume = {14},
year = {1987}
}
@article{Pasquet2017,
abstract = {The simultaneous estimation of 2D pressure (P-) and S-wave velocities (VP and VS, respectively) is a promising approach for imaging subsurface mechanical properties. It can be performed with a single acquisition setup by combining P-wave refraction and surface-wave (SW) analysis. Although SW methods are commonly applied for the 1D estimation of VS, 2D profiling requires the implementation of specific processing and inversion tools not yet widely available in the community. We have developed an open-source MATLAB-based package that performs SW inversion and profiling (SWIP) so as to retrieve 1D to 2D variations of VS from any kind of linear active-source near-surface seismic data. Each step of the workflow involves up-to-date processing and inversion techniques and provides ready-to-use outputs with quality control tools. First, windowing and stacking techniques are implemented to enhance the signal-to-noise ratio and extract local dispersion images along the line. Then, dispersion curves are picke...},
author = {Pasquet, Sylvain and Bodet, Ludovic},
doi = {10.1190/geo2016-0625.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Pasquet, Bodet{\_}2017.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
number = {6},
pages = {WB47--WB61},
title = {{SWIP: An integrated workflow for surface-wave dispersion inversion and profiling}},
url = {http://library.seg.org/doi/10.1190/geo2016-0625.1},
volume = {82},
year = {2017}
}
@article{McKinnon1998,
author = {McKinnon, K. I. M.},
doi = {10.1137/S1052623496303482},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
month = {jan},
number = {1},
pages = {148--158},
title = {{Convergence of the Nelder--Mead Simplex Method to a Nonstationary Point}},
url = {http://epubs.siam.org/doi/10.1137/S1052623496303482},
volume = {9},
year = {1998}
}
@article{Thomson1950,
author = {Thomson, William T},
issn = {0021-8979},
journal = {Journal of applied Physics},
number = {2},
pages = {89--93},
publisher = {AIP},
title = {{Transmission of elastic waves through a stratified solid medium}},
volume = {21},
year = {1950}
}
@article{Bondua2017,
abstract = {As is known, a full three-dimensional (3D) unstructured grid permits a great degree of flexibility when performing accurate numerical reservoir simulations. However, when the Integral Finite Difference Method (IFDM) is used for spatial discretization, constraints (arising from the required orthogonality between the segment connecting the blocks nodes and the interface area between blocks) pose difficulties in the creation of grids with irregular shaped blocks. The full 3D Voronoi approach guarantees the respect of IFDM constraints and allows generation of grids conforming to geological formations and structural objects and at the same time higher grid resolution in volumes of interest. In this work, we present dedicated pre- and post-processing gridding software tools for the TOUGH family of numerical reservoir simulators, developed by the Geothermal Research Group of the DICAM Department, University of Bologna. VORO2MESH is a new software coded in C++, based on the voro++ library, allowing computation of the 3D Voronoi tessellation for a given domain and the creation of a ready to use TOUGH2 MESH file. If a set of geological surfaces is available, the software can directly generate the set of Voronoi seed points used for tessellation. In order to reduce the number of connections and so to decrease computation time, VORO2MESH can produce a mixed grid with regular blocks (orthogonal prisms) and irregular blocks (polyhedron Voronoi blocks) at the point of contact between different geological formations. In order to visualize 3D Voronoi grids together with the results of numerical simulations, the functionality of the TOUGH2Viewer post-processor has been extended. We describe an application of VORO2MESH and TOUGH2Viewer to validate the two tools. The case study deals with the simulation of the migration of gases in deep layered sedimentary formations at basin scale using TOUGH2-TMGAS. A comparison between the simulation performances of unstructured and structured grids is presented.},
author = {Bondu{\`{a}}, Stefano and Battistelli, Alfredo and Berry, Paolo and Bortolotti, Villiam and Consonni, Alberto and Cormio, Carlo and Geloni, Claudio and Vasini, Ester Maria},
doi = {10.1016/j.cageo.2017.03.008},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Bondu{\`{a}} et al.{\_}2017.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {3D Voronoi,Gas migration,TOUGH2,Unstructured grids,tough2},
mendeley-tags = {tough2},
month = {nov},
number = {May 2016},
pages = {50--55},
publisher = {Elsevier Ltd},
title = {{3D Voronoi grid dedicated software for modeling gas migration in deep layered sedimentary formations with TOUGH2-TMGAS}},
url = {http://dx.doi.org/10.1016/j.cageo.2017.03.008 https://linkinghub.elsevier.com/retrieve/pii/S0098300417302819},
volume = {108},
year = {2017}
}
@article{Mittal2015,
abstract = {Evolutionary Algorithms (EAs) are being routinely applied for a variety of optimization tasks, and real-parameter optimization in the presence of constraints is one such important area. During constrained optimization EAs often create solutions that fall outside the feasible region; hence a viable constraint- handling strategy is needed. This paper focuses on the class of constraint-handling strategies that repair infeasible solutions by bringing them back into the search space and explicitly preserve feasibility of the solutions. Several existing constraint-handling strategies are studied, and two new single parameter constraint-handling methodologies based on parent-centric and inverse parabolic probability (IP) distribution are proposed. The existing and newly proposed constraint-handling methods are first studied with PSO, DE, GAs, and simulation results on four scalable test-problems under different location settings of the optimum are presented. The newly proposed constraint-handling methods exhibit robustness in terms of performance and also succeed on search spaces comprising up-to 500 variables while locating the optimum within an error of 10{\$}{\^{}}{\{}-10{\}}{\$}. The working principle of the IP based methods is also demonstrated on (i) some generic constrained optimization problems, and (ii) a classic `Weld' problem from structural design and mechanics. The successful performance of the proposed methods clearly exhibits their efficacy as a generic constrained-handling strategy for a wide range of applications.},
archivePrefix = {arXiv},
arxivId = {1504.04421},
author = {Padhye, Nikhil and Mittal, Pulkit and Deb, Kalyanmoy},
eprint = {1504.04421},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Padhye, Mittal, Deb{\_}2015.pdf:pdf},
keywords = {pso},
mendeley-tags = {pso},
month = {apr},
title = {{Feasibility Preserving Constraint-Handling Strategies for Real Parameter Evolutionary Optimization}},
url = {http://arxiv.org/abs/1504.04421},
year = {2015}
}
@article{Schaff2005,
abstract = {We processed the complete digital seismogram database for northern California to measure accurate differential travel times for correlated earthquakes observed at common stations. Correlated earthquakes are earthquakes that occur within a few kilometers of one another and have similar focal mechanisms, thus generating similar waveforms, allowing measurements to be made via cross-corre- lation analysis. The waveform database was obtained from the Northern California Earthquake Data Center and includes about 15 million seismograms from 225,000 local earthquakes between 1984 and 2003. A total of 26 billion cross-correlation measurements were performed on a 32-node (64 processor) Linux cluster, using improved analysis tools. All event pairs with separation distances of 5 km or less were processed at all stations that recorded the pair. We computed a total of about 1.7 billion P-wave differential times from pairs of waveforms that had cross- correlation coefficients (CC) of 0.6 or larger. The P-wave differential times are often on the order of a factor of ten to a hundred times more accurate than those obtained from routinely picked phase onsets. 1.2 billion S-wave differential times were mea- sured with CC?0.6, a phase not routinely picked at the Northern California Seismic Network because of the noise level of remaining P coda. We found that approxi- mately 95{\%} of the seismicity includes events that have cross-correlation coefficients of CC ? 0.7 with at least one other event recorded at four or more stations. At some stations more than 40{\%} of the recorded events are similar at the CC ? 0.9 level, indicating the potential existence of large numbers of repeating earthquakes. Large numbers of correlated events occur in different tectonic regions, including the San Andreas Fault, Long Valley caldera, Geysers geothermal field and Mendocino triple junction. Future research using these data may substantially improve earthquake lo- cations and add insight into the velocity structure in the crust.},
author = {Schaff, David P. and Waldhauser, Felix},
doi = {10.1785/0120040221},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Schaff, Waldhauser{\_}2005.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
number = {6},
pages = {2446--2461},
title = {{Waveform cross-correlation-based differential travel-time measurements at the northern California seismic network}},
volume = {95},
year = {2005}
}
@article{Krishnanand2006,
author = {Krishnanand, K N and Ghose, Debasish},
issn = {1574-1702},
journal = {Multiagent and Grid Systems},
number = {3},
pages = {209--222},
publisher = {IOS press},
title = {{Glowworm swarm based optimization algorithm for multimodal functions with collective robotics applications}},
volume = {2},
year = {2006}
}
@article{Mendel1866,
author = {Mendel, Gregor},
journal = {Verhandlungen des naturforschenden Vereines in Brunn 4: 3},
title = {{Versuche {\"{u}}ber Pflanzenhybriden}},
volume = {44},
year = {1866}
}
@inproceedings{Poliannikov2016,
abstract = {We study the problem of the moment tensor inversion of a double-couple microseismic source from observed S/P amplitude ratios. The emphasis of this work is on uncertainty quantification that includes the effect of the uncertain event location. We use a Bayesian approach to quantify the uncertainty of the fault plane solution. The posterior distribution is effectively calculated by sampling from the posterior distribution of the event location, and performing a moment-tensor inversion using individual samples. The uncertainty in the reconstructed moment tensor depends on the receiver geometry, signal noise, and the true moment tensor. After a suitable transformation of the input data, the problem can be reduced to a classical least-squares estimation problem.},
author = {Poliannikov, Oleg and Fehler, Michael and Rodi, William},
booktitle = {SEG Technical Program Expanded Abstracts 2016},
doi = {10.1190/segam2016-13875371.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SEG Technical Program Expanded Abstracts 2016/Poliannikov, Fehler, Rodi{\_}2016.pdf:pdf},
keywords = {3D,earthquake,induced seismicity,microseismic,uncertainty quantification},
mendeley-tags = {uncertainty quantification},
month = {sep},
pages = {2545--2549},
publisher = {Society of Exploration Geophysicists},
title = {{Quantifying the uncertainty in fault plane solutions inferred from S/P amplitude ratios}},
url = {http://library.seg.org/doi/10.1190/segam2016-13875371.1},
year = {2016}
}
@article{Scales2001,
author = {Scales, John A. and Tenorio, Luis},
doi = {10.1190/1.1444930},
issn = {0016-8033},
journal = {Geophysics},
month = {mar},
number = {2},
pages = {389--397},
title = {{Prior information and uncertainty in inverse problems}},
url = {https://library.seg.org/doi/10.1190/1.1444930},
volume = {66},
year = {2001}
}
@article{Leys2013,
abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software. ?? 2013 Elsevier Inc.},
author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
doi = {10.1016/j.jesp.2013.03.013},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Experimental Social Psychology/Leys et al.{\_}2013.pdf:pdf},
isbn = {0022-1031},
issn = {00221031},
journal = {Journal of Experimental Social Psychology},
keywords = {MAD,Median absolute deviation,Outlier,outliers},
mendeley-tags = {outliers},
number = {4},
pages = {764--766},
pmid = {20228874},
publisher = {Elsevier Inc.},
title = {{Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median}},
url = {http://dx.doi.org/10.1016/j.jesp.2013.03.013},
volume = {49},
year = {2013}
}
@inproceedings{Shenfield2017,
author = {Shenfield, Alex and Rostami, Shahin},
booktitle = {2017 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},
doi = {10.1109/CIBCB.2017.8058553},
isbn = {978-1-4673-8988-4},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{Multi-objective evolution of artificial neural networks in multi-class medical diagnosis problems with class imbalance}},
url = {http://ieeexplore.ieee.org/document/8058553/},
year = {2017}
}
@article{Liu1989,
abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
author = {Liu, Dong C and Nocedal, Jorge},
doi = {10.1007/BF01589116},
issn = {1436-4646},
journal = {Mathematical Programming},
keywords = {quasi-newton},
mendeley-tags = {quasi-newton},
number = {1},
pages = {503--528},
title = {{On the limited memory BFGS method for large scale optimization}},
url = {https://doi.org/10.1007/BF01589116},
volume = {45},
year = {1989}
}
@incollection{Pham2006,
author = {Pham, Duc Truong and Ghanbarzadeh, Afshin and Ko{\c{c}}, Ebubekir and Otri, Sameh and Rahim, S and Zaidi, Muhamad},
booktitle = {Intelligent Production Machines and Systems},
pages = {454--459},
publisher = {Elsevier},
title = {{-The Bees Algorithm—A Novel Tool for Complex Optimisation Problems}},
year = {2006}
}
@article{Torczon2000,
author = {Lewis, Robert Michael and Torczon, Virginia and Trosset, Michael W},
doi = {10.1016/S0377-0427(00)00423-4},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Computational and Applied Mathematics/Lewis, Torczon, Trosset{\_}2000.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {derivative-free optimization,direct search methods,pattern search methods},
month = {dec},
number = {1-2},
pages = {191--207},
title = {{Direct search methods: then and now}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0377042700004234},
volume = {124},
year = {2000}
}
@article{Barros2015,
abstract = {The common-reflection surface (CRS) method is a sophisticated alternative to the traditional common-midpoint stacking because its traveltime approximation allows for the use of more traces than the normal moveout. This in turn requires more parameters for the moveout description, thus increasing the computational burden of the parameter estimation. In the literature, a suboptimal strategy is often used, which decreases the complexity but, as we found in this work, compromises the accuracy of the parameters in some cases. To cope with this problem, in this work, we have devised detailed information for efficient estimation of the CRS parameters using the differential evolution (DE) global optimization algorithm. Because we used data sets with low fold and low signal-to-noise ratio, from which no reliable velocity analysis could be easily performed, we applied this algorithm in a fully automatic global search, i.e., without any velocity guide. The results for a 2D real data set from Brazil indicated...},
author = {Barros, Tiago and Ferrari, Rafael and Krummenauer, Rafael and Lopes, Renato},
doi = {10.1190/geo2015-0032.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Barros et al.{\_}2015.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
keywords = {common-midpoint (CMP),de,multiparameter,normal moveout (NMO),signal processing,statistics.},
mendeley-tags = {de},
number = {6},
pages = {WD189--WD200},
title = {{Differential evolution-based optimization procedure for automatic estimation of the common-reflection surface traveltime parameters}},
url = {http://library.seg.org/doi/abs/10.1190/geo2015-0032.1},
volume = {80},
year = {2015}
}
@article{Zhang2003,
abstract = {We have developed an automatic P-wave arrival detection and picking algorithm based on the wavelet transform and Akaike information criteria (AIC) picker. Wavelet coefficients at high resolutions show the fine structure of the time series, and those at low resolutions characterize its coarse features. Primary features such as the P-wave arrival are retained over several resolution scales, whereas sec-ondary features such as scattered arrivals decay quickly at lower resolutions. We apply the discrete wavelet transform to single-component seismograms through a series of sliding time windows. In each window the AIC autopicker is applied to the thresholded absolute wavelet coefficients at different scales, and we compare the consistency of those picks to determine whether a P-wave arrival has been detected in the given time window. The arrival time is then determined using the AIC picker on the time window chosen by the wavelet transform. We test our method on regional earthquake data from the Dead Sea Rift region and local earthquake data from the Parkfield, California region. We find that 81{\%} of picks are within 0.2-sec of the corresponding analyst pick for the Dead Sea dataset, and 93{\%} of picks are within 0.1 sec of the analyst pick for the Parkfield dataset. We attribute the lower percentage of agreement for the Dead Sea dataset to the substantially lower signal-to-noise ratio of those data, and the likelihood that some percentage of the analyst picks are in error.},
author = {Zhang, Haijiang and Thurber, Clifford and Rowe, Charlotte},
doi = {10.1785/0120020241},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Bulletin of the Seismological Society of America/Zhang, Thurber, Rowe{\_}2003.pdf:pdf},
isbn = {0037-1106},
issn = {00371106},
journal = {Bulletin of the Seismological Society of America},
keywords = {automatic picking,microseismic},
mendeley-tags = {automatic picking,microseismic},
number = {5},
pages = {1904--1912},
title = {{Automatic P-wave arrival detection and picking with multiscale wavelet analysis for single-component recordings}},
volume = {93},
year = {2003}
}
@article{Baer1987,
author = {Baer, M. and Kradolfer, U.},
journal = {Bulletin of the Seismological Society of America},
number = {4},
pages = {1437--1445},
title = {{An automatic phase picker for local and teleseismic events}},
volume = {77},
year = {1987}
}
@article{Chmiel2016,
abstract = {We have investigated the use of ambient-noise data to extract phase and group velocities from surface-noise sources in a microseismic monitoring context. The data were continuously recorded on 44 patch arrays with an interpatch distance on the order of 1 km. Typically, a patch-array design consists of a few tens of patches, each containing 48 strings of 12 single-vertical-component geophones densely distributed within the patch area. The specificity of the patch-array design allows seismic analysis at two different scales. Within each patch, highly coherent signals at small distances provide phase information at high frequency (up to 10 Hz), from which surface-wave phase velocities can be extracted. Between the pairs of patches, surface-wave group-velocity maps can be built using correctly identified and localized surface-noise sources. The technique can be generalized to every patch pair using different noise sources identified at the surface. We note that the incoherent but localized noise sources accelerate the convergence of the noise-correlation functions. This opens the route to passive seismic monitoring of the near surface from repetitive inversion of phase- and group-velocity maps.},
author = {Chmiel, Malgorzata and Roux, Philippe and Bardainne, Thomas},
doi = {10.1190/geo2016-0027.1},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysics/Chmiel, Roux, Bardainne{\_}2016.pdf:pdf},
issn = {0016-8033},
journal = {Geophysics},
month = {nov},
number = {6},
pages = {KS231--KS240},
title = {{Extraction of phase and group velocities from ambient surface noise in a patch-array configuration}},
url = {http://library.seg.org/doi/10.1190/geo2016-0027.1},
volume = {81},
year = {2016}
}
@phdthesis{Auger2016,
author = {Auger, Anne},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Unknown/Auger{\_}2016.pdf:pdf},
keywords = {cmaes},
mendeley-tags = {cmaes},
pages = {1--85},
school = {Universit{\'{e}} Paris-Sud},
title = {{Analysis of Comparison-based Stochastic Continuous Black-Box Optimization Algorithms}},
year = {2016}
}
@article{Warpinski2004,
abstract = {This paper presents an analysis of the stress and pressure changes caused by hydraulic fractures and evaluates the likelihood and causes of microseismic activity in the vicinity of the fracture. Along with the formation stresses, pressure, and properties, the analysis predicts where microseisms should occur in relation to the fracture and makes possible accurate interpretation of the significance of the microseismic events. The most important factor controlling the seismically active zone is the coupling of the fracturing pressure into the formation. Thus, liquid-saturated reservoirs experience much more widespread activity than do gas reservoirs. The analysis also shows that the fracture tip induces large shear stresses that result in a local zone of instability. Such a zone is the primary reason that microseisms accurately map out the length and height of the fracture, because considerable microseismic activity occurs around the tip as it propagates.},
author = {Warpinski, N.R. and Wolhart, S.L. and Wright, C.a.},
doi = {10.2118/87673-PA},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SPE Journal/Warpinski, Wolhart, Wright{\_}2004.pdf:pdf},
isbn = {9781555631543},
issn = {1086-055X},
journal = {SPE Journal},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {mar},
number = {01},
pages = {24--33},
title = {{Analysis and Prediction of Microseismicity Induced by Hydraulic Fracturing}},
url = {http://www.onepetro.org/doi/10.2118/87673-PA},
volume = {9},
year = {2004}
}
@article{Fletcher1963,
author = {Fletcher, R. and Powell, M. J. D.},
doi = {10.1093/comjnl/6.2.163},
issn = {0010-4620},
journal = {The Computer Journal},
month = {aug},
number = {2},
pages = {163--168},
title = {{A Rapidly Convergent Descent Method for Minimization}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/6.2.163},
volume = {6},
year = {1963}
}
@article{Ramirez2005,
abstract = {We describe a stochastic inversion method for mapping subsurface regions where the electrical resistivity is changing. The technique combines prior information, electrical resistance data, and forward models to produce subsurface resistivity models that are most consistent with all available data. Bayesian inference and a Metropolis simulation algorithm form the basis for this approach. Attractive features include its ability (1) to provide quantitative measures of the uncertainty of a generated estimate and (2) to allow alternative model estimates to be identified, compared, and ranked. Methods that monitor convergence and summarize important trends of the posterior distribution are introduced. Results from a physical model test and a field experiment were used to assess performance. The presented stochastic inversions provide useful estimates of the most probable location, shape, and volume of the changing region and the most likely resistivity change. The proposed method is computationally expensive, requiring the use of extensive computational resources to make its application practical.},
author = {Ramirez, A. L. and Nitao, J. J. and Hanley, W. G. and Aines, R. and Glaser, R. E. and Sengupta, S. K. and Dyer, K. M. and Hickling, T. L. and Daily, W. D.},
doi = {10.1029/2004JB003449},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Ramirez et al.{\_}2005.pdf:pdf},
isbn = {0148-0227},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {doi:10.1029/2004JB003449,electrical resistivity,http://dx.doi.org/10.1029/2004JB003449,mcmc,stochastic inversion},
mendeley-tags = {mcmc},
month = {feb},
number = {B2},
pages = {1--18},
title = {{Stochastic inversion of electrical resistivity changes using a Markov Chain Monte Carlo approach}},
url = {http://doi.wiley.com/10.1029/2004JB003449},
volume = {110},
year = {2005}
}
@article{Bishop1995,
abstract = {Bayesian techniques have been developed over many years in a range of different fields, but have only recently been applied to the problem of learning in neural networks. As well as providing a consistent framework for statistical pattern recognition, the Bayesian approach offers a number of practical advantages including a potential solution to the problem of over-fitting. This chapter aims to provide an introductory overview of the application of Bayesian methods to neural networks. It assumes the reader is familiar with standard feed-forward network models and how to train them using conventional techniques.},
author = {Bishop, Christopher M},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Machine Learning/Bishop{\_}1995.pdf:pdf},
journal = {Machine Learning},
keywords = {neural network},
mendeley-tags = {neural network},
number = {1},
pages = {1--11},
title = {{Bayesian methods for neural networks}},
url = {http://eprints.aston.ac.uk/1131/},
volume = {7},
year = {1995}
}
@article{Rutqvist2014a,
abstract = {In this paper, we present model simulations of ground motions caused by CO2-injection-induced fault reactivation and analyze the results in terms of the potential for damage to ground surface structures and nuisance to the local human population. It is an integrated analysis from cause to consequence, including the whole chain of processes starting from earthquake inception in the subsurface, wave propagation toward the ground surface, and assessment of the consequences of ground vibration. For a small magnitude (Mw=3) event at a hypocenter depth of about 1000m, we first used the simulated ground-motion wave train in an inverse analysis to estimate source parameters (moment magnitude, rupture dimensions and stress drop), achieving good agreement and thereby verifying the modeling of the chain of processes from earthquake inception to ground vibration. We then analyzed the ground vibration results in terms of peak ground acceleration (PGA), peak ground velocity (PGV) and frequency content, with comparison to U.S. Geological Survey's instrumental intensity scales for earthquakes and the U.S. Bureau of Mines' vibration criteria for cosmetic damage to buildings, as well as human-perception vibration limits. Our results confirm the appropriateness of using PGV (rather than PGA) and frequency for the evaluation of potential ground-vibration effects on structures and humans from shallow injection-induced seismic events. For the considered synthetic Mw=3 event, our analysis showed that the short duration, high frequency ground motion may not cause any significant damage to surface structures, but would certainly be felt by the local population. {\textcopyright} 2014.},
author = {Rutqvist, Jonny and Cappa, Fr{\'{e}}d{\'{e}}ric and Rinaldi, Antonio P. and Godano, Maxime},
doi = {10.1016/j.ijggc.2014.02.017},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/International Journal of Greenhouse Gas Control/Rutqvist et al.{\_}2014.pdf:pdf},
issn = {17505836},
journal = {International Journal of Greenhouse Gas Control},
keywords = {CO2 injection,Fault rupture,Ground acceleration,Induced seismicity,flac,tough2},
mendeley-tags = {flac,tough2},
month = {may},
pages = {64--77},
publisher = {Elsevier Ltd},
title = {{Modeling of induced seismicity and ground vibrations associated with geologic CO2 storage, and assessing their effects on surface structures and human perception}},
url = {http://dx.doi.org/10.1016/j.ijggc.2014.02.017 https://linkinghub.elsevier.com/retrieve/pii/S1750583614000553},
volume = {24},
year = {2014}
}
@article{Bondua2012,
abstract = {Within the MAC-GEO research project, funded by Regione Toscana and addressed to the exploitation of high enthalpy geothermal systems, the authors worked on the utilization and customization of the open source numerical simulator TOUGH2, as implemented in the code dedicated to model calibration iTOUGH2.TOUGH2 is one of the most used numerical simulation software for non-isothermal flow of multicomponent, multiphase fluids in one, two and three-dimensional porous and fractured media.Lacking an official Graphical User Interface tool for post-processing operations, several commercial and academic software have been developed to manage and display TOUGH2 input and output data files. Almost all of these tools seem to have limits to visualizing parameter values of the numerical model, work only with a predefined binary version of TOUGH2, and only a few of them can manage locally refined unstructured grids (i.e. Voronoi grids). To overcome these limitations, the authors have developed and tested a dedicated software application (called TOUGH2Viewer) for reading and managing TOUGH2 output files, written in Java and able to provide an interactive 3D view of the numerical model. Several functionalities have been implemented for block query and searching, contour mapping and 3D surface mapping of TOUGH2 primary variables (i.e. pressure, temperature, etc.).TOUGH2Viewer is also able to display 2D and 3D views of mass and heat flow between blocks, for each time step in which the simulation proceeds.The application described in this paper is under development to improve its functionalities; nevertheless the current software release is a valid support tool for post-processing that significantly improves the possibility to inspect the simulated data coming from TOUGH2. {\textcopyright} 2012 Elsevier Ltd.},
author = {Bondu{\'{a}}, S. and Berry, P. and Bortolotti, V. and Cormio, C.},
doi = {10.1016/j.cageo.2012.04.008},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Computers {\&} Geosciences/Bondu{\'{a}} et al.{\_}2012.pdf:pdf},
issn = {00983004},
journal = {Computers {\&} Geosciences},
keywords = {Computer software,Graphical user interface,Numerical modeling,TOUGH2 geothermal simulation,Voronoi,tough2},
mendeley-tags = {tough2},
month = {sep},
pages = {107--118},
title = {{TOUGH2Viewer: A post-processing tool for interactive 3D visualization of locally refined unstructured grids for TOUGH2}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300412001318},
volume = {46},
year = {2012}
}
@article{Jones2008,
abstract = {Acoustic emission (AE) monitoring is a non-invasive method of monitoring fracturing both in situ, and in experimental rock deformation studies. Until recently, the major impediment for imaging brittle failure within a rock mass is the accuracy at which the hypocenters may be located. However, recent advances in the location of regional scale earthquakes have successfully reduced hypocentral uncertainties by an order of magnitude. The least-squares Geiger, master event relocation, and double difference methods have been considered in a series of synthetic experiments which investigate their ability to resolve AE hypocentral locations. The effect of AE hypocenter location accuracy due to seismic velocity perturbations, uncertainty in the first arrival pick, array geometry and the inversion of a seismically anisotropic structure with an isotropic velocity model were tested. Hypocenters determined using the Geiger procedure for a homogeneous, isotropic sample with a known velocity model gave a RMS error for the hypocenter locations of 2.6 mm; in contrast the double difference method is capable of reducing the location error of these hypocenters by an order of magnitude. We test uncertainties in velocity model of up to +/- 10{\%} and show that the double difference method can attain the same RMS error as using the standard Geiger procedure with a known velocity model. The double difference method is also capable of precise locations even in a 40{\%} anisotropic velocity structure using an isotropic model for location and attains a RMS mislocation error of 2.6 mm that is comparable to a RMS mislocation error produced with an isotropic known velocity model using the Geiger approach. We test the effect of sensor geometry on location accuracy and find that, even when sensors are missing, the double difference method is capable of a 1.43 mm total RMS mislocation compared to 4.58 mm for the Geiger method. The accuracy of automatic picking algorithms used for AE studies is +/- 0.5 mu s (1 time sample when the sampling rate is 0.2 mu s). We investigate how AE locations are effected by the accuracy of first arrival picking by randomly delaying the actual first arrival by up to 5 time samples. We find that even when noise levels are set to 5 time samples the double difference method successfully relocates the synthetic AE.},
author = {Jones, G. A. and Nippress, S. E J and Rietbrock, A. and Reyes-Montes, J. M.},
doi = {10.1007/s00024-008-0303-2},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Pure and Applied Geophysics/Jones et al.{\_}2008.pdf:pdf},
isbn = {0033-4553$\backslash$r1420-9136},
issn = {00334553},
journal = {Pure and Applied Geophysics},
keywords = {Acoustic emission,Earthquake,Relocation,Seismic anisotropy,location},
mendeley-tags = {location},
number = {2},
pages = {235--254},
pmid = {22311540},
title = {{Accurate location of synthetic acoustic emissions and location sensitivity to relocation methods, velocity perturbations, and seismic anisotropy}},
volume = {165},
year = {2008}
}
@article{Podvin1991,
author = {Podvin, Pascal and Lecomte, Isabelle},
doi = {10.1111/j.1365-246X.1991.tb03461.x},
issn = {0956540X},
journal = {Geophysical Journal International},
month = {apr},
number = {1},
pages = {271--284},
title = {{Finite difference computation of traveltimes in very contrasted velocity models: a massively parallel approach and its associated tools}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1111/j.1365-246X.1991.tb03461.x},
volume = {105},
year = {1991}
}
@incollection{Cerveny1987,
address = {Dordrecht},
author = {{\v{C}}erven{\'{y}}, V.},
booktitle = {Seismic Tomography},
doi = {10.1007/978-94-009-3899-1_5},
pages = {99--133},
publisher = {Springer Netherlands},
title = {{Ray tracing algorithms in three-dimensional laterally varying layered structures}},
url = {http://www.springerlink.com/index/10.1007/978-94-009-3899-1{\_}5},
year = {1987}
}
@article{Grayver2016,
abstract = {This paper presents a methodology to sample equivalence domain (ED) in nonlinear partial differential equation (PDE)-constrained inverse problems. For this purpose, we first applied state-of-the-art stochastic optimization algorithm called Covariance Matrix Adaptation Evolution Strategy (CMAES) to identify low-misfit regions of the model space. These regions were then randomly sampled to create an ensemble of equivalent models and quantify uncertainty. CMAES is aimed at exploring model space globally and is robust on very ill-conditioned problems. We show that the number of iterations required to converge grows at a moderate rate with respect to number of unknowns and the algorithm is embarrassingly parallel. We formulated the problem by using the generalized Gaussian distribution. This enabled us to seamlessly use arbitrary norms for residual and regularization terms. We show that various regularization norms facilitate studying different classes of equivalent solutions. We further show how performance of the standard Metropolis-Hastings Markov chain Monte Carlo algorithm can be substantially improved by using information CMAES provides. This methodology was tested by using individual and joint inversions of magneotelluric, controlled-source electromagnetic (EM) and global EM induction data.},
author = {Grayver, Alexander V. and Kuvshinov, Alexey V.},
doi = {10.1093/gji/ggw063},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Geophysical Journal International/Grayver, Kuvshinov{\_}2016.pdf:pdf},
issn = {1365246X},
journal = {Geophysical Journal International},
keywords = {Geomagnetic induction,Inverse theory,Magnetotellurics,Marine electromagnetics,Numerical solutions,cmaes},
mendeley-tags = {cmaes},
number = {2},
pages = {971--987},
title = {{Exploring equivalence domain in nonlinear inverse problems using Covariance Matrix Adaption Evolution Strategy (CMAES) and random sampling}},
volume = {205},
year = {2016}
}
@article{Geem2001,
abstract = {Many optimization problems in various fields have been solved using diverse optimization algorithms. Traditional optimization techniques such as linear programming (LP), non-linear programming (NLP), and dynamic program ming (DP) have had major roles in solving these problems. However, their drawbacks generate demand for other types of algorithms, such as heuristic optimization approaches (simulated annealing, tabu search, and evolutionary algorithms). However, there are still some possibili ties of devising new heuristic algorithms based on analogies with natural or artificial phenom ena. A new heuristic algorithm, mimicking the improvisation of music players, has been devel oped and named Harmony Search (HS). The performance of the algorithm is illustrated with a traveling salesman problem (TSP), a specific academic optimization problem, and a least-cost pipe network design problem.},
author = {{Zong Woo Geem} and {Joong Hoon Kim} and Loganathan, G.V.},
doi = {10.1177/003754970107600201},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/SIMULATION/Zong Woo Geem, Joong Hoon Kim, Loganathan{\_}2001.pdf:pdf},
isbn = {9781424418237},
issn = {0037-5497},
journal = {SIMULATION},
keywords = {Harmony search,combinatorial optimi zation,heuristic algorithm,music,optimization},
month = {feb},
number = {2},
pages = {60--68},
pmid = {12674748},
title = {{A New Heuristic Optimization Algorithm: Harmony Search}},
url = {http://journals.sagepub.com/doi/10.1177/003754970107600201},
volume = {76},
year = {2001}
}
@inproceedings{Daniels2007,
author = {Daniels, John Leonard and Waters, George A. and {Le Calvez}, Joel Herve and Bentley, Doug and Lassek, John T.},
booktitle = {SPE Annual Technical Conference and Exhibition},
doi = {10.2118/110562-MS},
keywords = {microseismic monitoring},
mendeley-tags = {microseismic monitoring},
month = {apr},
publisher = {Society of Petroleum Engineers},
title = {{Contacting More of the Barnett Shale Through an Integration of Real-Time Microseismic Monitoring, Petrophysics, and Hydraulic Fracture Design}},
url = {http://www.onepetro.org/doi/10.2118/110562-MS},
year = {2007}
}
@article{Mirjalili2014,
author = {Mirjalili, Seyedali and Mirjalili, Seyed Mohammad and Lewis, Andrew},
doi = {10.1016/j.advengsoft.2013.12.007},
issn = {09659978},
journal = {Advances in Engineering Software},
month = {mar},
pages = {46--61},
title = {{Grey Wolf Optimizer}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0965997813001853},
volume = {69},
year = {2014}
}
@article{Bodin2012,
abstract = {We present a novel method for joint inversion of receiver functions and surface wave dispersion data, using a transdimensional Bayesian formulation. This class of algorithm treats the number of model parameters (e.g. number of layers) as an unknown in the problem. The dimension of the model space is variable and a Markov chain Monte Carlo (McMC) scheme is used to provide a parsimonious solution that fully quantifies the degree of knowledge one has about seismic structure (i.e constraints on the model, resolution, and trade-offs). The level of data noise (i.e. the covariance matrix of data errors) effectively controls the information recoverable from the data and here it naturally determines the complexity of the model (i.e. the number of model parameters). However, it is often difficult to quantify the data noise appropriately, particularly in the case of seismic waveform inversion where data errors are correlated. Here we address the issue of noise estimation using an extended Hierarchical Bayesian formulation, which allows both the variance and covariance of data noise to be treated as unknowns in the inversion. In this way it is possible to let the data infer the appropriate level of data fit. In the context of joint inversions, assessment of uncertainty for different data types becomes crucial in the evaluation of the misfit function. We show that the Hierarchical Bayes procedure is a powerful tool in this situation, because it is able to evaluate the level of information brought by different data types in the misfit, thus removing the arbitrary choice of weighting factors. After illustrating the method with synthetic tests, a real data application is shown where teleseismic receiver functions and ambient noise surface wave dispersion measurements from the WOMBAT array (South-East Australia) are jointly inverted to provide a probabilistic 1D model of shear-wave velocity beneath a given station.},
author = {Bodin, T. and Sambridge, M. and Tkal{\v{c}}i{\'{c}}, H. and Arroucau, P. and Gallagher, K. and Rawlinson, N.},
doi = {10.1029/2011JB008560},
file = {:C$\backslash$:/Users/keurf/OneDrive/Articles/Journal of Geophysical Research Solid Earth/Bodin et al.{\_}2012.pdf:pdf},
isbn = {2156-2202},
issn = {01480227},
journal = {Journal of Geophysical Research: Solid Earth},
month = {feb},
number = {B2},
title = {{Transdimensional inversion of receiver functions and surface wave dispersion}},
url = {http://doi.wiley.com/10.1029/2011JB008560},
volume = {117},
year = {2012}
}
